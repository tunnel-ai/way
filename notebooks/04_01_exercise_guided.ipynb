{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d56270",
   "metadata": {},
   "source": [
    "# Module 4 — 04_01 Guided Exercise (Unsupervised Learning)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tunnel-ai/way/blob/main/notebooks/04_01_exercise_guided.ipynb)\n",
    "\n",
    "This guided exercise is designed to be completed **top-to-bottom**.\n",
    "\n",
    "**Rules of the exercise**\n",
    "- Use the **canonical dataset** generated by `generate_transaction_risk_dataset(seed=1955)`.\n",
    "- Do **not** use `is_fraud` or `transaction_loss_amount` as model inputs.\n",
    "- You may *inspect* those outcomes only in the **final validation section**.\n",
    "\n",
    "**What you will produce**\n",
    "1) A K-Means clustering with short cluster profiles  \n",
    "2) A PCA visualization colored by your cluster assignments  \n",
    "3) A DBSCAN run with a defensible `eps` (via k-distance plot)  \n",
    "4) An Isolation Forest anomaly score + a “top anomalies” table  \n",
    "5) A short validation: Are anomalies enriched for fraud or high loss?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Colab) First-time setup: clone repo + add src/ to Python path\n",
    "# If you're running locally, you likely don't need this cell.\n",
    "\n",
    "# !git clone https://github.com/tunnel-ai/way.git\n",
    "# import sys\n",
    "# sys.path.insert(0, \"/content/way/src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.generators.transaction_risk_dgp import generate_transaction_risk_dataset\n",
    "\n",
    "df = generate_transaction_risk_dataset(seed=1955)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68861c9e",
   "metadata": {},
   "source": [
    "## 0) Feature set selection (numeric behavioral features)\n",
    "\n",
    "We will start with a numeric behavioral feature set. This keeps the geometry interpretable.\n",
    "\n",
    "**TODO 0A**: Confirm the selected columns exist (defensive check).  \n",
    "**TODO 0B**: Create `X_num` as a DataFrame with only these columns.\n",
    "\n",
    "> Keep outcomes (`is_fraud`, `transaction_loss_amount`) out of `X_num`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e238f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 0A: choose the feature set\n",
    "candidate_numeric = [\n",
    "    \"transaction_amount\",\n",
    "    \"transaction_hour\",\n",
    "    \"transaction_day\",\n",
    "    \"account_age_days\",\n",
    "    \"customer_risk_score\",\n",
    "    \"prior_transaction_count\",\n",
    "    \"prior_fraud_count\",\n",
    "]\n",
    "\n",
    "# TODO 0A: keep only columns that exist\n",
    "NUM_FEATURES = [c for c in candidate_numeric if c in df.columns]\n",
    "\n",
    "# TODO 0B: build X_num\n",
    "X_num = df[NUM_FEATURES].copy()\n",
    "\n",
    "NUM_FEATURES, X_num.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb8a94b",
   "metadata": {},
   "source": [
    "## 1) Scaling (required)\n",
    "\n",
    "Distance-based methods are extremely sensitive to scale.\n",
    "\n",
    "**TODO 1**: Standardize `X_num` into `X_scaled`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_num)\n",
    "\n",
    "pd.DataFrame(X_scaled, columns=NUM_FEATURES).agg([\"mean\", \"std\"]).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d283180",
   "metadata": {},
   "source": [
    "## 2) K-Means clustering (structured partition)\n",
    "\n",
    "**TODO 2A**: Fit K-Means with `k=4` (use `random_state=42`, `n_init=10`).  \n",
    "**TODO 2B**: Add cluster labels to `df_km`.  \n",
    "**TODO 2C**: Compute and report:\n",
    "- cluster counts\n",
    "- cluster mean profiles for the selected features\n",
    "- silhouette score\n",
    "\n",
    "Interpretation prompt:\n",
    "- Which cluster looks most “high risk” based on behavior alone?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2A\n",
    "k = 4\n",
    "kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "cluster_km = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# TODO 2B\n",
    "df_km = df.copy()\n",
    "df_km[\"cluster_kmeans\"] = cluster_km\n",
    "\n",
    "# TODO 2C\n",
    "counts = df_km[\"cluster_kmeans\"].value_counts().sort_index()\n",
    "profiles = df_km.groupby(\"cluster_kmeans\")[NUM_FEATURES].mean()\n",
    "sil = silhouette_score(X_scaled, cluster_km)\n",
    "\n",
    "counts, profiles, sil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef1dc3",
   "metadata": {},
   "source": [
    "### Quick visualization (two-feature view)\n",
    "\n",
    "**TODO 2D**: Make a scatter plot using:\n",
    "- x-axis: `transaction_amount`\n",
    "- y-axis: `customer_risk_score`\n",
    "colored by K-Means cluster.\n",
    "\n",
    "If one of those features is missing, pick a reasonable substitute from `NUM_FEATURES`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17498c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2D\n",
    "x_feat = \"transaction_amount\" if \"transaction_amount\" in NUM_FEATURES else NUM_FEATURES[0]\n",
    "y_feat = \"customer_risk_score\" if \"customer_risk_score\" in NUM_FEATURES else NUM_FEATURES[1]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(df_km[x_feat], df_km[y_feat], c=df_km[\"cluster_kmeans\"], s=10, alpha=0.5)\n",
    "plt.xlabel(x_feat)\n",
    "plt.ylabel(y_feat)\n",
    "plt.title(\"K-Means clusters (two-feature view)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e120ef6",
   "metadata": {},
   "source": [
    "## 3) PCA for visualization and sense-making\n",
    "\n",
    "**TODO 3A**: Fit PCA on `X_scaled` with up to 10 components (or fewer if needed).  \n",
    "**TODO 3B**: Plot cumulative explained variance.  \n",
    "**TODO 3C**: Create a 2D PCA scatterplot colored by K-Means cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3A\n",
    "pca = PCA(n_components=min(10, len(NUM_FEATURES)), random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "explained = pca.explained_variance_ratio_\n",
    "\n",
    "# TODO 3B: explained variance plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(np.cumsum(explained), marker=\"o\")\n",
    "plt.ylim(0, 1.01)\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA explained variance (cumulative)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "explained[:5], explained.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7cb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3C: PCA 2D scatter colored by K-Means cluster\n",
    "pc1, pc2 = X_pca[:, 0], X_pca[:, 1]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(pc1, pc2, c=cluster_km, s=10, alpha=0.5)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA space colored by K-Means clusters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6990d137",
   "metadata": {},
   "source": [
    "## 4) DBSCAN (density-based clustering + noise)\n",
    "\n",
    "DBSCAN requires choosing `eps`. A common practical approach is the **k-distance plot**.\n",
    "\n",
    "**TODO 4A**: Build a k-distance plot using `k_nn=10`.  \n",
    "**TODO 4B**: Choose an `eps` near the “elbow” (start with the 95th percentile).  \n",
    "**TODO 4C**: Fit DBSCAN with `min_samples=10`.  \n",
    "**TODO 4D**: Report:\n",
    "- cluster label counts (including `-1` noise)\n",
    "- noise rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4A: k-distance plot\n",
    "k_nn = 10\n",
    "nn = NearestNeighbors(n_neighbors=k_nn)\n",
    "nn.fit(X_scaled)\n",
    "distances, _ = nn.kneighbors(X_scaled)\n",
    "\n",
    "k_dist = np.sort(distances[:, -1])\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(k_dist)\n",
    "plt.title(f\"k-distance plot (k={k_nn})\")\n",
    "plt.xlabel(\"Points sorted by distance\")\n",
    "plt.ylabel(f\"Distance to {k_nn}th nearest neighbor\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4454d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4B / 4C: DBSCAN run\n",
    "eps = float(np.percentile(k_dist, 95))  # adjust once if needed\n",
    "min_samples = 10\n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "cluster_db = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "df_db = df.copy()\n",
    "df_db[\"cluster_dbscan\"] = cluster_db\n",
    "\n",
    "counts_db = df_db[\"cluster_dbscan\"].value_counts().head(10)\n",
    "noise_rate = (df_db[\"cluster_dbscan\"] == -1).mean()\n",
    "\n",
    "counts_db, noise_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89730b87",
   "metadata": {},
   "source": [
    "### DBSCAN visualization (PCA space)\n",
    "\n",
    "**TODO 4E**: Plot PCA (PC1 vs PC2) colored by DBSCAN labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4E\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(pc1, pc2, c=cluster_db, s=10, alpha=0.5)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA space colored by DBSCAN (+ noise = -1)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7623d",
   "metadata": {},
   "source": [
    "## 5) Isolation Forest anomaly detection\n",
    "\n",
    "**TODO 5A**: Fit Isolation Forest (use `random_state=42`).  \n",
    "**TODO 5B**: Create `anomaly_score` where higher = more unusual.  \n",
    "**TODO 5C**: Plot the anomaly score distribution.  \n",
    "**TODO 5D**: Create a table of the top 25 most anomalous points with:\n",
    "- anomaly_score\n",
    "- transaction_amount\n",
    "- customer_risk_score\n",
    "- transaction_hour\n",
    "- merchant_category (if it exists)\n",
    "- channel (if it exists)\n",
    "\n",
    "Note: we still have not used labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5A / 5B\n",
    "iso = IsolationForest(n_estimators=300, contamination=0.02, random_state=42)\n",
    "iso.fit(X_scaled)\n",
    "\n",
    "score_normal = iso.decision_function(X_scaled)\n",
    "anomaly_score = -score_normal\n",
    "\n",
    "df_anom = df.copy()\n",
    "df_anom[\"anomaly_score\"] = anomaly_score\n",
    "\n",
    "df_anom[\"anomaly_score\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5C\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(df_anom[\"anomaly_score\"], bins=50, alpha=0.8)\n",
    "plt.xlabel(\"Anomaly score (higher = more unusual)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Isolation Forest anomaly score distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718bce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5D\n",
    "show_cols = [\"anomaly_score\", \"transaction_amount\", \"customer_risk_score\", \"transaction_hour\"]\n",
    "for c in [\"merchant_category\", \"channel\"]:\n",
    "    if c in df_anom.columns:\n",
    "        show_cols.append(c)\n",
    "\n",
    "top25 = df_anom.sort_values(\"anomaly_score\", ascending=False).head(25)[show_cols]\n",
    "top25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacd179",
   "metadata": {},
   "source": [
    "## 6) Validation (labels revealed as diagnostics)\n",
    "\n",
    "Now—and only now—we use outcomes to validate whether:\n",
    "- clusters differ in fraud prevalence\n",
    "- top anomalies are enriched for fraud\n",
    "- top anomalies have higher loss\n",
    "\n",
    "**TODO 6A**: Fraud rate by K-Means cluster  \n",
    "**TODO 6B**: Fraud rate among top 2% anomaly scores  \n",
    "**TODO 6C**: Compare mean and 95th percentile loss overall vs top anomalies\n",
    "\n",
    "Interpretation prompt:\n",
    "- Do anomalies align with fraud? With high loss? With neither? What might that imply?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6A\n",
    "y_fraud = df[\"is_fraud\"].astype(int)\n",
    "fraud_by_cluster = pd.DataFrame({\"cluster\": cluster_km, \"is_fraud\": y_fraud}).groupby(\"cluster\")[\"is_fraud\"].mean()\n",
    "fraud_by_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6B\n",
    "q = 0.98\n",
    "thr = np.quantile(df_anom[\"anomaly_score\"], q)\n",
    "mask_top = df_anom[\"anomaly_score\"] >= thr\n",
    "\n",
    "fraud_overall = y_fraud.mean()\n",
    "fraud_top = y_fraud[mask_top].mean()\n",
    "\n",
    "fraud_overall, fraud_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6C\n",
    "y_loss = df[\"transaction_loss_amount\"]\n",
    "loss_top = y_loss[mask_top]\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"group\": [\"overall\", f\"top_anomaly_q{q}\"],\n",
    "    \"mean_loss\": [y_loss.mean(), loss_top.mean()],\n",
    "    \"p95_loss\": [y_loss.quantile(0.95), loss_top.quantile(0.95)],\n",
    "    \"median_loss\": [y_loss.median(), loss_top.median()],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e397b",
   "metadata": {},
   "source": [
    "## Reflection (short)\n",
    "\n",
    "Answer in 3–6 sentences:\n",
    "\n",
    "1) Which method (K-Means, DBSCAN, Isolation Forest) produced the most **actionable** view of the data? Why?  \n",
    "2) In this dataset, did anomaly detection seem to track **fraud**, **loss**, both, or neither?  \n",
    "3) If you had to propose one next step for a real fraud team, what would it be?\n",
    "\n",
    "(There is no single correct answer. You are graded on evidence-based reasoning.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
