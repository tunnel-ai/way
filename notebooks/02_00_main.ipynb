{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc74b03",
   "metadata": {},
   "source": [
    "# Module 2 — Linear, Polynomial & Regularized Regression\n",
    "\n",
    "**Sections:**  \n",
    "2.1 Example (Business & Science) • 2.2 OLS (baseline) • 2.3 Polynomial Regression •  \n",
    "2.4 Regularization (Ridge/Lasso/ElasticNet) • 2.5 Error Metrics • 2.6 Model Evaluation (CV & Learning Curves)\n",
    "\n",
    "> **Learning goals:**  \n",
    "• Understand linear, polynomial, and regularized regression and when to use them  \n",
    "• Implement end-to-end pipelines (clean → model → evaluate)  \n",
    "• Interpret coefficients, residuals, and key metrics (RMSE/MAE/R²/MAPE)  \n",
    "• Use cross-validation and learning curves for robust evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb9392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>age_years</th>\n",
       "      <th>lot_size</th>\n",
       "      <th>dist_to_center_km</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2148.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.537</td>\n",
       "      <td>3.66</td>\n",
       "      <td>454013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1908.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.65</td>\n",
       "      <td>379302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2368.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>0.513</td>\n",
       "      <td>12.37</td>\n",
       "      <td>395900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2916.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.694</td>\n",
       "      <td>12.66</td>\n",
       "      <td>565279.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1208.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.281</td>\n",
       "      <td>4.96</td>\n",
       "      <td>281811.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sqft  bedrooms  bathrooms  age_years  lot_size  dist_to_center_km  \\\n",
       "0  2148.0         4          3         13     0.537               3.66   \n",
       "1  1908.0         2          1         23       NaN               3.65   \n",
       "2  2368.0         2          2         77     0.513              12.37   \n",
       "3  2916.0         4          4          6     0.694              12.66   \n",
       "4  1208.0         1          1          8     0.281               4.96   \n",
       "\n",
       "      price  \n",
       "0  454013.0  \n",
       "1  379302.0  \n",
       "2  395900.0  \n",
       "3  565279.0  \n",
       "4  281811.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Import dataset ===\n",
    "# This cell will load the dataset you need to work on. It will include messiness and a more realistic set than we use in Module 1.\n",
    "from datasets_module2 import make_housing_realistic #<----- chnage make_housing_realistic to make_auto_mpg_realistic to load the Auto MPG dataset.\n",
    "\n",
    "# Quick test\n",
    "# We pass n and seed explicitly (even though defaults exist)\n",
    "# to make the code self-documenting and reproducible. You can change the size (n) of the samples generated \n",
    "# as well as the seed if you would like to see other results\n",
    "df = make_housing_realistic(n=900, seed=1955)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc596c0",
   "metadata": {},
   "source": [
    "## 2.1 — Example: Business & Science\n",
    "\n",
    "**Business (Housing pricing):** Predict *listing price* from `sqft`, `bedrooms`, `bathrooms`, `age_years`, `lot_size`, `dist_to_center_km`.  \n",
    "**Science (Fuel efficiency):** Predict *MPG* from `horsepower`, `displacement`, `weight`, `acceleration`, `model_year`, `origin`.\n",
    "\n",
    "> Linear → baseline & interpretability; Polynomial → curvature; Regularization → stability with correlated predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f98b79",
   "metadata": {},
   "source": [
    "### 2.1A — Business Example: Housing Pricing\n",
    "\n",
    "A real-estate analyst wants to estimate home prices using key features such as \n",
    "square footage, number of bedrooms, lot size, and distance to the city center.\n",
    "\n",
    "Before we build full models (coming in §2.2–2.6), here is an *example* that shows the \n",
    "core idea of regression: using data to predict a continuous outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b0242",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.13 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Mini-demo: single-feature regression (sqft → price)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Tiny synthetic sample (to illustrate the idea)\n",
    "sqft = np.array([800, 1200, 1500, 2000, 2500]).reshape(-1, 1)\n",
    "price = np.array([150_000, 200_000, 240_000, 300_000, 360_000])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(sqft, price)\n",
    "print(\"Coefficient:\", model.coef_[0])\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "\n",
    "# Predict price for a new home\n",
    "print(\"Predicted price for 1800 sqft:\", model.predict([[1800]])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40574b",
   "metadata": {},
   "source": [
    "### Business Regression Example: Predicting House Price from Square Footage\n",
    "\n",
    "This mini-demo shows how a **single-feature linear regression model** can capture a \n",
    "straightforward business relationship: larger homes tend to cost more.\n",
    "\n",
    "We fit a linear model using:\n",
    "- **Input (X):** home size in square feet  \n",
    "- **Output (y):** home sale price  \n",
    "\n",
    "The model learned the following:\n",
    "\n",
    "- **Coefficient:** 123.60  \n",
    "- **Intercept:** 52,247.19  \n",
    "\n",
    "This gives us the regression equation:\n",
    "\n",
    "$\\text{price} = 52{,}247.19 + 123.60 \\times \\text{sqft}\n",
    "$\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- The coefficient of **123.6** means each additional square foot adds approximately  \n",
    "  **\\$123.60** to the expected sale price.  \n",
    "- The intercept shows the model’s baseline estimate when sqft = 0  \n",
    "  (not meaningful on its own, but necessary for the line).\n",
    "\n",
    "When predicting the price of an **1800 sqft** home:\n",
    "\n",
    "- **Predicted price:** \\$274,719  \n",
    "- This falls between the prices for 1500 sqft and 2000 sqft, exactly as expected.\n",
    "\n",
    "#### Why this example matters?\n",
    "\n",
    "This simple business example illustrates:\n",
    "\n",
    "1. **How linear regression discovers relationships**  \n",
    "2. **How coefficients translate into real-world business insights**  \n",
    "3. **How we can use the model for forecasting** (e.g., pricing tools, cost estimators)\n",
    "\n",
    "This sets the stage for more complex regression models in the next sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28709e74",
   "metadata": {},
   "source": [
    "**How to read this:**\n",
    "- The model learns a simple linear relationship between `sqft` and `price`.\n",
    "- The coefficient tells us the approximate increase in price for each additional square foot.\n",
    "- This is the same principle we use with real datasets, just scaled up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba43c8",
   "metadata": {},
   "source": [
    "### 2.1B — Science Example: Fuel Efficiency (MPG)\n",
    "\n",
    "Engineers model **miles per gallon (MPG)** using features like horsepower, weight,\n",
    "engine displacement, and acceleration.\n",
    "\n",
    "Here's a micro-demo using just horsepower (HP) to predict MPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-demo: one-feature regression (horsepower → mpg)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "hp  = np.array([70, 90, 110, 130, 160]).reshape(-1, 1)\n",
    "mpg = np.array([38, 34, 30, 25, 22])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(hp, mpg)\n",
    "print(\"Coefficient:\", model.coef_[0])\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "\n",
    "print(\"Predicted MPG for 100 HP:\", model.predict([[100]])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee2d52c",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- The negative coefficient confirms the expected trend: *more horsepower → lower MPG*.\n",
    "- Real models include more features, but the concept remains exactly the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ca3dc",
   "metadata": {},
   "source": [
    "### Science Regression Example: Predicting Fuel Efficiency from Horsepower\n",
    "\n",
    "This example shows how linear regression can capture a simple scientific relationship:\n",
    "as **engine horsepower increases**, **fuel efficiency (MPG)** typically decreases.\n",
    "\n",
    "We fit a linear regression using:\n",
    "- **Input (X):** horsepower  \n",
    "- **Output (y):** miles per gallon (MPG)\n",
    "\n",
    "The model learned the following:\n",
    "\n",
    "- **Coefficient:** −0.184  \n",
    "- **Intercept:** 50.41  \n",
    "\n",
    "This gives the regression equation:\n",
    "\n",
    "MPG ≈ 50.41 − 0.184 × horsepower\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- The **negative coefficient** means that every increase of 1 horsepower reduces\n",
    "  fuel efficiency by roughly **0.18 MPG**.\n",
    "- The intercept represents the model’s estimate when horsepower = 0  \n",
    "  (not meaningful physically, but necessary for the regression line).\n",
    "\n",
    "#### Prediction Example\n",
    "\n",
    "For a **100 HP** engine, the model predicts:\n",
    "\n",
    "- **MPG ≈ 32.0**\n",
    "\n",
    "This fits smoothly between the observed data points and illustrates the model’s\n",
    "ability to **interpolate** within the range of the dataset.\n",
    "\n",
    "#### Why this example matters?\n",
    "\n",
    "This demonstration highlights how linear regression can model:\n",
    "\n",
    "- basic scientific laws (e.g., inverse relationships)  \n",
    "- negative or positive correlations  \n",
    "- continuous physical measurements  \n",
    "\n",
    "By pairing this science example with the earlier business example, you can see\n",
    "how regression applies across domains — from pricing to physical systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265506d",
   "metadata": {},
   "source": [
    "## 2.2 — Baseline: Ordinary Least Squares (OLS)\n",
    "\n",
    "We generate a realistic synthetic housing dataset, **clean** it, **split** into train/test, **fit** OLS, inspect coefficients, and evaluate metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38624d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate & inspect ---\n",
    "df = make_housing_realistic(n=900, seed=1955)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d6485",
   "metadata": {},
   "source": [
    "### First Look at the Housing Dataset\n",
    "\n",
    "This synthetic housing dataset contains seven meaningful features used to predict home prices:\n",
    "\n",
    "- **sqft** — interior area  \n",
    "- **bedrooms / bathrooms** — basic home characteristics  \n",
    "- **age_years** — age of the home  \n",
    "- **lot_size** — land area (in acres)  \n",
    "- **dist_to_center_km** — distance from the city center  \n",
    "- **price** — target variable  \n",
    "\n",
    "From the first few rows, we see:\n",
    "\n",
    "- Realistic variation in home sizes, prices, and distances  \n",
    "- Some missing values (e.g., `lot_size` is NaN in row 1)  \n",
    "- Some unrealistic or noisy values (e.g., negative or extremely large `sqft`)  \n",
    "- A broad range of prices from affordable homes to luxury properties\n",
    "\n",
    "This “messiness” is intentional: it helps demonstrate how to diagnose and clean real datasets before modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b99bfa",
   "metadata": {},
   "source": [
    "**Sanity-check:** expected columns, rough ranges, missing/invalid values to clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ac9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd7700",
   "metadata": {},
   "source": [
    "### Sanity Check — Identifying Data Quality Issues\n",
    "\n",
    "The `.info()` and `.describe()` summaries highlight several issues that must be addressed before fitting an OLS model:\n",
    "\n",
    "#### 1. **Missing Values**\n",
    "- `sqft` has **821 non-null** rows (missing 79)  \n",
    "- `lot_size` has **848 non-null** rows (missing 52)  \n",
    "- `price` has **878 non-null** rows (missing 22)\n",
    "\n",
    "Missing values must be handled, since linear regression cannot train with NaNs.\n",
    "\n",
    "#### 2. **Impossible or implausible values**\n",
    "- `sqft` minimum is **−50** → impossible (should be positive)  \n",
    "- `sqft` maximum is **12,000** → extremely large outlier  \n",
    "- `dist_to_center_km` minimum is **0** and max is **999** → unrealistic distances  \n",
    "- `age_years` ranges from **1 to 110** → includes oddly old properties  \n",
    "- `lot_size` ranges from **0.123 to 0.804** (reasonable)\n",
    "\n",
    "These values indicate we will need:\n",
    "- **imputation** for missing data  \n",
    "- **winsorization or clipping** for extreme outliers  \n",
    "- **conversion to numeric types** where needed\n",
    "\n",
    "#### 3. **Scale differences across features**\n",
    "- Some features are measured in **thousands** (e.g., sqft)  \n",
    "- Others are measured in **fractions** (e.g., lot_size)  \n",
    "- Some span huge ranges (distance)\n",
    "\n",
    "This motivates the need for **feature scaling** later in the module.\n",
    "\n",
    "Overall, this summary step reveals exactly why data cleaning is essential before running OLS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad647cbc",
   "metadata": {},
   "source": [
    "### Clean and Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfda9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean & prepare ---\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "df = df.dropna(subset=['price']).copy()              # drop rows with missing target\n",
    "df.loc[df['sqft']<=0,'sqft'] = np.nan                # mark invalids to impute\n",
    "df.loc[df['dist_to_center_km']<=0,'dist_to_center_km'] = np.nan\n",
    "\n",
    "num_cols = ['sqft','bedrooms','bathrooms','age_years','lot_size','dist_to_center_km']\n",
    "imp = SimpleImputer(strategy='median')\n",
    "df[num_cols] = imp.fit_transform(df[num_cols])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd5cfe",
   "metadata": {},
   "source": [
    "**Why median?** Robust to skew/outliers; ensures complete numeric inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6bc3f2",
   "metadata": {},
   "source": [
    "### Cleaned Dataset — Ready for OLS\n",
    "\n",
    "After applying the cleaning pipeline:\n",
    "\n",
    "- Missing values in `sqft` and `lot_size` were imputed  \n",
    "- Impossible values (negative or zero `sqft`) were corrected  \n",
    "- Extreme outliers were clipped using an IQR-based rule  \n",
    "- Numeric types were enforced  \n",
    "- All features now contain valid numeric values suitable for regression\n",
    "\n",
    "The cleaned preview now shows:\n",
    "\n",
    "- Fully populated rows (no NaN values for features used in modeling)  \n",
    "- Reasonable ranges for `sqft`, `lot_size`, and `dist_to_center_km`  \n",
    "- Numeric consistency (all values in proper format)\n",
    "\n",
    "At this point, the dataset is prepared for:\n",
    "\n",
    "- training/testing splits  \n",
    "- fitting an OLS model  \n",
    "- comparing predictions to true prices  \n",
    "- analyzing residuals  \n",
    "\n",
    "This completes the setup for the baseline Ordinary Least Squares model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c95b6",
   "metadata": {},
   "source": [
    "### Split & Fit OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe17543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split & fit OLS ---\n",
    "#import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = df.drop('price', axis=1) \n",
    "y = df['price']\n",
    "Xtr,Xte,ytr,yte = train_test_split(X,y,test_size=0.2,random_state=1955)\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(Xtr,ytr)\n",
    "yhat_ols = ols.predict(Xte)\n",
    "\n",
    "\n",
    "coef = pd.Series(ols.coef_, index=Xtr.columns).sort_values()\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba7835",
   "metadata": {},
   "source": [
    "**Interpreting coefficients:** Positive → increases price (ceteris paribus).  \n",
    "Negative → decreases price (e.g., `age_years`, `dist_to_center_km`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e04cf2f",
   "metadata": {},
   "source": [
    "### Interpreting the OLS Coefficients - Longer Explanation\n",
    "\n",
    "These coefficients show how each feature contributes to the predicted house price in a\n",
    "**linear** model. Each value represents the **expected change in price** for a one-unit\n",
    "increase in that feature, *holding all other variables constant*.\n",
    "\n",
    "| Feature           | Coefficient | Interpretation |\n",
    "|------------------|------------:|----------------|\n",
    "| **lot_size**     | 493,063     | The strongest driver of price. Larger lots significantly increase home value. |\n",
    "| **bedrooms**     | 26,309      | Each additional bedroom increases the expected price by about \\$26k. |\n",
    "| **bathrooms**    | 11,636      | Each additional bathroom adds roughly \\$11.6k. |\n",
    "| **sqft**         | 23.72       | Each extra square foot adds about \\$24 to the home price. |\n",
    "| **dist_to_center_km** | −63.70  | Homes farther from the city center tend to be slightly cheaper. |\n",
    "| **age_years**    | −453.11     | Older homes are worth less, at about −\\$453 per year of age. |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "#### **1. Lot size is the dominant predictor**\n",
    "A coefficient over **\\$493k** indicates that even small changes in lot size (in acres)\n",
    "have large impacts on home value — consistent with real‐estate markets.\n",
    "\n",
    "#### **2. More bedrooms and bathrooms increase price predictably**\n",
    "These categorical-numeric features contribute meaningfully:\n",
    "- Bedrooms add more value than bathrooms  \n",
    "- Both reflect home “capacity” and desirability  \n",
    "\n",
    "#### **3. Interior square footage adds value steadily**\n",
    "The value per square foot (~\\$24) aligns with typical construction and market dynamics.\n",
    "\n",
    "#### **4. Distance from city center decreases price**\n",
    "While the effect is small compared to lot size or interior features, homes farther from\n",
    "central locations tend to be less expensive.\n",
    "\n",
    "#### **5. Older homes are valued lower**\n",
    "The model captures a depreciation pattern, which is common in housing markets.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "OLS provides a **simple, interpretable baseline** for understanding how each housing feature\n",
    "affects price. It reveals:\n",
    "\n",
    "- Which features have the strongest influence  \n",
    "- Whether the effect is positive or negative  \n",
    "- How the model sees relationships before introducing nonlinearity (Polynomial Regression)  \n",
    "- A baseline to compare against more flexible models in Sections 2.3–2.6  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5afdfe",
   "metadata": {},
   "source": [
    "### Actual vs Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b782ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare actual vs predicted (first 10 rows) ---\n",
    "display(\n",
    "    pd.DataFrame({\n",
    "        \"Actual (y)\": yte.values[:10],\n",
    "        \"Predicted (ŷ)\": yhat_ols[:10],\n",
    "        \"Residual (y - ŷ)\": (yte.values[:10] - yhat_ols[:10])\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83db62b",
   "metadata": {},
   "source": [
    "#### Understanding the Predictions Table\n",
    "\n",
    "The table above shows the first 10 predictions made by the OLS model.  \n",
    "For each home we list:\n",
    "\n",
    "- **Actual (y)** – the true price  \n",
    "- **Predicted (ŷ)** – the model’s estimate  \n",
    "- **Residual (y – ŷ)** – the error  \n",
    "\n",
    "##### What we observe:\n",
    "- Most predictions are reasonably close to the actual values (within \\$10k–\\$60k).\n",
    "- This is normal for a realistic housing dataset with prices in the \\$250k–\\$450k range.\n",
    "- A few residuals are much larger (e.g., –217,530). These typically occur when:\n",
    "  - the underlying home has extreme values (very large lot, very far distance, etc.),\n",
    "  - noise was intentionally injected into the dataset,\n",
    "  - or OLS is trying to fit a relationship that is partly nonlinear.\n",
    "\n",
    "##### Why this is expected:\n",
    "Our synthetic housing data includes:\n",
    "- **Heteroscedastic noise** (larger houses have higher variance)\n",
    "- **Nonlinear terms** (sqft / sqrt(distance)) that OLS can’t fully capture\n",
    "- **Intentional messiness** (missing sqft, invalid distances, extreme values)\n",
    "\n",
    "Therefore, the predictions table reinforces the same story shown in the residual plot:\n",
    "> OLS captures most of the signal (R² ≈ 0.86),  \n",
    "> but misses some nonlinear structure and is sensitive to extreme values.\n",
    "\n",
    "This sets up the motivation for polynomial regression (§2.3) and regularization (§2.4).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959a716",
   "metadata": {},
   "source": [
    "### Residuals & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3405413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals & metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "resid = yte - yhat_ols\n",
    "plt.scatter(yhat_ols, resid, s=16, alpha=0.7)\n",
    "plt.axhline(0, color='k')\n",
    "plt.xlabel('Predicted price')\n",
    "plt.ylabel('Residual (y - ŷ)')\n",
    "plt.title('Residuals vs Predictions — OLS')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def mape(y_true, y_pred, eps=1e-9):\n",
    "    yt=np.asarray(y_true,float)\n",
    "    yp=np.asarray(y_pred,float)\n",
    "    m=np.abs(yt)>eps\n",
    "    return np.mean(np.abs((yt[m]-yp[m])/(yt[m]+eps))*100) if m.any() else np.nan\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(yte, yhat_ols))\n",
    "mae  = mean_absolute_error(yte, yhat_ols)\n",
    "r2   = r2_score(yte, yhat_ols)\n",
    "mp   = mape(yte, yhat_ols)\n",
    "\n",
    "print(f'OLS → RMSE: {rmse:,.0f} | MAE: {mae:,.0f} | R²: {r2:.3f} | MAPE: {mp:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906acb42",
   "metadata": {},
   "source": [
    "Flat band is good. Curvature/funneling suggests trying polynomial terms or transformations (e.g., log-price)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5d598",
   "metadata": {},
   "source": [
    "#### Interpreting the Residuals vs Predictions Plot\n",
    "\n",
    "This plot shows the **residuals** for each home in the test set:\n",
    "\n",
    "- **x-axis:** Predicted price (ŷ)  \n",
    "- **y-axis:** Residual (y − ŷ), the error for each prediction  \n",
    "- The horizontal black line at 0 represents *perfect* predictions.\n",
    "\n",
    "##### What to look for\n",
    "- **Most points cluster around 0**, meaning the model usually predicts close to the true price.\n",
    "- There is **no strong curve or clear pattern**, suggesting that a linear model captures the main trend reasonably well.\n",
    "- The vertical spread of points grows slightly at higher predicted prices.  \n",
    "  This reflects *heteroscedasticity* — in our synthetic dataset, more expensive homes naturally have more variability.\n",
    "- A few points sit far above or below the line.  \n",
    "  These represent **outliers** or homes with:\n",
    "  - unusual combinations of features,\n",
    "  - very large noise,\n",
    "  - or intentionally injected “messy” values.\n",
    "\n",
    "##### Why this matters\n",
    "The plot helps us visually check whether:\n",
    "- the model is **systematically biased** (it isn’t),\n",
    "- we are missing curvature (minor hints, addressed in §2.3),\n",
    "- certain observations exert **large influence** (a few do).\n",
    "\n",
    "This plot confirms what the metrics tell us:  \n",
    "> OLS captures the overall price pattern well (R² ≈ 0.86),  \n",
    "> but still leaves room for improvement through polynomial features or regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f837d92",
   "metadata": {},
   "source": [
    "## 2.3 — Polynomial Regression (Nonlinear Effects)\n",
    "\n",
    "Compare OLS vs Polynomial (deg 2/3). Also visualize **price vs `sqft`** with other features fixed at medians.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "\n",
    "poly2 = make_pipeline(PolynomialFeatures(degree=2, include_bias=False), LinearRegression())\n",
    "poly3 = make_pipeline(PolynomialFeatures(degree=3, include_bias=False), LinearRegression())\n",
    "\n",
    "poly2.fit(Xtr, ytr)\n",
    "pred_p2 = poly2.predict(Xte)\n",
    "poly3.fit(Xtr, ytr)\n",
    "pred_p3 = poly3.predict(Xte)\n",
    "\n",
    "def summarize(name, y_true, y_pred):\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    return {'Model':name,\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_true,y_pred)),\n",
    "            'MAE' : mean_absolute_error(y_true,y_pred),\n",
    "            'R2'  : r2_score(y_true,y_pred),\n",
    "            'MAPE%': mape(y_true,y_pred)}\n",
    "\n",
    "\n",
    "pd.DataFrame([summarize('OLS',yte,yhat_ols),\n",
    "              summarize('Poly deg=2',yte,pred_p2),\n",
    "              summarize('Poly deg=3',yte,pred_p3)]).sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913904c",
   "metadata": {},
   "source": [
    "#### Understanding the Polynomial Regression Results\n",
    "\n",
    "The table compares three models:\n",
    "- **OLS (degree 1)** → Straight-line model  \n",
    "- **Polynomial degree 2** → Slight curvature  \n",
    "- **Polynomial degree 3** → More flexible curvature  \n",
    "\n",
    "| Model | RMSE | MAE | R² | MAPE% |\n",
    "|-------|------|------|------|---------|\n",
    "| OLS | 41,336 | 30,669 | 0.862 | 9.22% |\n",
    "| Poly deg=2 | 31,452 | 22,608 | 0.920 | 6.56% |\n",
    "| Poly deg=3 | **30,744** | **21,434** | **0.924** | **6.43%** |\n",
    "\n",
    "##### Interpretation\n",
    "- **Both polynomial models outperform OLS** on all metrics.  \n",
    "  This means adding curvature helps the model capture nonlinear relationships in the data.\n",
    "- **Degree 3 performs best**, but only slightly better than degree 2.  \n",
    "  This is typical:  \n",
    "  - degree 2 captures most of the useful nonlinearity,  \n",
    "  - degree 3 adds a bit more flexibility but risks overfitting if pushed too far.\n",
    "- **R² increases from 0.86 → 0.92–0.924**, showing that polynomial models explain substantially more variance.\n",
    "- **MAPE drops from 9.2% → ~6.4%**, meaning predictions are more accurate on a percentage basis.\n",
    "\n",
    "##### Key takeaways\n",
    "Polynomial regression is powerful when the relationship between features and target is **smooth but nonlinear**.  \n",
    "Here, house prices grow with square footage at a rate that changes slightly depending on home size — a pattern the polynomial models successfully capture.\n",
    "\n",
    "However:\n",
    "- Higher degrees increase risk of **overfitting**,  \n",
    "- Regularization (Section 2.4) can help control this when many polynomial features are added.\n",
    "\n",
    "This sets the stage for why we often combine **PolynomialFeatures + Ridge/Lasso** in real-world models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad56a0a",
   "metadata": {},
   "source": [
    "### Price vs sqft (others fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price vs sqft (others fixed)\n",
    "feat = 'sqft'\n",
    "fixed = Xtr.median(numeric_only=True).to_dict()\n",
    "x_grid = np.linspace(df[feat].quantile(0.05), df[feat].quantile(0.95), 200)\n",
    "rows=[]\n",
    "for x in x_grid:\n",
    "    r = fixed.copy()\n",
    "    r[feat]=x\n",
    "    rows.append(r)\n",
    "X_line = pd.DataFrame(rows)[Xtr.columns]\n",
    "\n",
    "y_ols = ols.predict(X_line)\n",
    "y_p2  = poly2.predict(X_line)\n",
    "y_p3  = poly3.predict(X_line)\n",
    "\n",
    "plt.scatter(df[feat], y, s=12, alpha=0.25)\n",
    "plt.plot(x_grid, y_ols, label='OLS')\n",
    "plt.plot(x_grid, y_p2,  label='Poly deg=2')\n",
    "plt.plot(x_grid, y_p3,  label='Poly deg=3')\n",
    "plt.xlabel('sqft')\n",
    "plt.ylabel('price')\n",
    "plt.title('price vs sqft (others fixed)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c9896",
   "metadata": {},
   "source": [
    "Higher degree adds flexibility but risks overfitting. Use the hold-out set to judge generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f84a62",
   "metadata": {},
   "source": [
    "#### Interpreting the Price vs Square Footage Plot\n",
    "\n",
    "This plot shows how the model thinks **price changes as `sqft` increases**, while all other features are held constant at their median values.\n",
    "\n",
    "##### What the lines tell us\n",
    "- **OLS (blue line)** is a straight line.  \n",
    "  It can only model a fixed “price increase per extra sqft,” even though the real relationship is slightly curved.\n",
    "- **Polynomial degree 2 (orange line)** captures a gentle upward curve.  \n",
    "  It reflects that price tends to rise faster for mid-sized homes and then level off slightly.\n",
    "- **Polynomial degree 3 (green line)** fits the curvature even more flexibly.  \n",
    "  It hugs the data more closely, especially for smaller and larger homes.\n",
    "\n",
    "##### What the points tell us\n",
    "The faint blue dots are actual houses from the dataset:\n",
    "- They show a generally upward trend: more square feet → higher price.  \n",
    "- But the spread widens for larger homes because the dataset contains **heteroscedastic noise** (higher-priced homes vary more).\n",
    "- Some outliers on the far right (e.g., `sqft ≈ 12,000`) come from the intentional “messiness” added to the dataset.\n",
    "\n",
    "##### Why this matters\n",
    "This visualization demonstrates that:\n",
    "- A straight line (OLS) **underfits** when the true relationship is curved.\n",
    "- **Polynomial models better capture real-world price dynamics**, especially when effects slow down or accelerate at different ranges.\n",
    "- Higher-degree polynomials fit the data more closely, but must be balanced with **regularization** (Section 2.4) to avoid overfitting.\n",
    "\n",
    "This plot provides an intuitive visual bridge from:\n",
    "**“Linear regression can’t capture curvature” → “Polynomial features fix that” → “Now we need regularization.”**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469cde3e",
   "metadata": {},
   "source": [
    "## 2.4 — Regularized Regression: Ridge, Lasso, ElasticNet\n",
    "\n",
    "Regularization shrinks coefficients to reduce variance and improve generalization with correlated predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "ridge = make_pipeline(StandardScaler(), Ridge(alpha=1.0, random_state=1955))\n",
    "lasso = make_pipeline(StandardScaler(), Lasso(alpha=0.1, random_state=1955, max_iter=10000))\n",
    "enet  = make_pipeline(StandardScaler(), ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=1955, max_iter=10000))\n",
    "\n",
    "pd.DataFrame([\n",
    "    summarize('Ridge', yte, ridge.fit(Xtr,ytr).predict(Xte)),\n",
    "    summarize('Lasso', yte, lasso.fit(Xtr,ytr).predict(Xte)),\n",
    "    summarize('ElasticNet', yte, enet.fit(Xtr,ytr).predict(Xte)),\n",
    "]).sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5800a7",
   "metadata": {},
   "source": [
    "**Quick Interpretation**  \n",
    "• **Ridge (L2)**: shrinks all coefficients; good for multicollinearity & stability.  \n",
    "• **Lasso (L1)**: can set some coefficients to 0 (feature selection).  \n",
    "• **ElasticNet**: blends L1+L2; balances sparsity & stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7eea2",
   "metadata": {},
   "source": [
    "#### Understanding the Regularization Results\n",
    "\n",
    "Regularization techniques (Ridge, Lasso, ElasticNet) are designed to improve linear models by\n",
    "penalizing large coefficients. This helps when:\n",
    "- features are highly correlated,\n",
    "- the model is overfitting,\n",
    "- or there are many noisy or irrelevant predictors.\n",
    "\n",
    "In this dataset, the performance of all three methods is **very similar**:\n",
    "\n",
    "| Model | RMSE | MAE | R² | MAPE% |\n",
    "|--------|---------|---------|--------|-----------|\n",
    "| **ElasticNet** | 41,085 | 30,649 | 0.864 | 9.26% |\n",
    "| **Ridge** | 41,324 | 30,662 | 0.863 | 9.22% |\n",
    "| **Lasso** | 41,336 | 30,669 | 0.862 | 9.22% |\n",
    "\n",
    "##### Why are the results so close?\n",
    "Two reasons:\n",
    "\n",
    "1. **The dataset is fairly well-behaved.**  \n",
    "   Even though we added some noise and messy values, the core features are informative and not strongly collinear.  \n",
    "   Linear regression already performs well (R² ≈ 0.86), leaving little room for improvement.\n",
    "\n",
    "2. **We haven't added polynomial terms yet.**  \n",
    "   Regularization shines most when:\n",
    "   - the feature space is large (e.g., degree-5 polynomial expansion),\n",
    "   - or there are many correlated predictors.  \n",
    "   With only a handful of original features, OLS is already stable.\n",
    "\n",
    "##### What each method is doing behind the scenes\n",
    "- **Ridge (L2):**  \n",
    "  Shrinks all coefficients toward zero. Keeps all variables but reduces their magnitude.  \n",
    "  Great for multicollinearity and reducing variance.\n",
    "\n",
    "- **Lasso (L1):**  \n",
    "  Pushes some coefficients *exactly* to zero, effectively performing **feature selection**.  \n",
    "  Useful when many features are irrelevant or noisy.\n",
    "\n",
    "- **ElasticNet (L1 + L2):**  \n",
    "  Blends the strengths of Ridge and Lasso.  \n",
    "  Good default when you expect both correlation and sparsity.\n",
    "\n",
    "##### Teaching takeaway\n",
    "Even when regularization does *not* improve RMSE or R², it is still valuable because it:\n",
    "- **stabilizes** coefficient estimates,\n",
    "- **improves interpretability** (especially Lasso),\n",
    "- often helps in the presence of **outliers** or **messy data**,\n",
    "- becomes essential when using **polynomial features** (Section 2.3) or **high-dimensional data**.\n",
    "\n",
    "This prepares us for the next step:  \n",
    "combining **PolynomialFeatures + Ridge/Lasso** to handle nonlinear relationships **without overfitting**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b12f52",
   "metadata": {},
   "source": [
    "#### Comparing Coefficients Across Models\n",
    "\n",
    "Even when RMSE/R² look similar, regularization often affects **how** the model uses each feature.\n",
    "\n",
    "Below we compare the learned coefficients for:\n",
    "\n",
    "- **OLS** (no regularization)\n",
    "- **Ridge** (L2 shrinkage)\n",
    "- **Lasso** (L1 sparsity)\n",
    "- **ElasticNet** (blend of L1 and L2)\n",
    "\n",
    "Regularization does **not** always improve accuracy, but it often:\n",
    "- stabilizes coefficients,\n",
    "- reduces extreme swings,\n",
    "- selects features (Lasso),\n",
    "- and improves interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27031b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare coefficients across models ---\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": Xtr.columns,\n",
    "    \"OLS\": ols.coef_,\n",
    "    \"Ridge\": ridge.fit(Xtr, ytr).predict(Xtr) * 0 + ridge.named_steps['ridge'].coef_ if False else ridge.fit(Xtr,ytr).named_steps['ridge'].coef_,\n",
    "    \"Lasso\": lasso.fit(Xtr, ytr).named_steps['lasso'].coef_,\n",
    "    \"ElasticNet\": enet.fit(Xtr, ytr).named_steps['elasticnet'].coef_,\n",
    "})\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d05eb",
   "metadata": {},
   "source": [
    "#### Visualizing Regularization Effects on Coefficients\n",
    "\n",
    "This chart helps us see how each method **shrinks or zeroes out** coefficients.\n",
    "\n",
    "- **OLS** often has the largest (and most unstable) coefficients.  \n",
    "- **Ridge** shrinks them smoothly.  \n",
    "- **Lasso** sharply compresses many to *zero*.  \n",
    "- **ElasticNet** produces a middle ground.\n",
    "\n",
    "Visual inspection helps you understand *why* regularization is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b322d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visual coefficient comparison chart ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models_coef = pd.DataFrame({\n",
    "    \"Feature\": Xtr.columns,\n",
    "    \"OLS\": ols.coef_,\n",
    "    \"Ridge\": ridge.named_steps['ridge'].coef_,\n",
    "    \"Lasso\": lasso.named_steps['lasso'].coef_,\n",
    "    \"ElasticNet\": enet.named_steps['elasticnet'].coef_\n",
    "})\n",
    "\n",
    "models_coef = models_coef.set_index(\"Feature\")\n",
    "\n",
    "models_coef.plot(kind=\"barh\", figsize=(10,6))\n",
    "plt.title(\"Coefficient Comparison: OLS vs Ridge vs Lasso vs ElasticNet\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.axvline(0, color='k', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66b288",
   "metadata": {},
   "source": [
    "#### Intuition: How Regularization *Feels*\n",
    "\n",
    "Regularization can be understood with simple physical analogies:\n",
    "\n",
    "##### **Ridge Regression (L2) — “Rubber Band Around the Coefficients”**\n",
    "- Imagine all coefficients are tied to zero with a **soft rubber band**.\n",
    "- They get pulled inward, but none are forced to zero.\n",
    "- Great for stabilizing correlated or noisy features.\n",
    "\n",
    "##### **Lasso Regression (L1) — “Scissors Cutting Weak Features Away”**\n",
    "- Lasso uses a **sharp edge** instead of a rubber band.\n",
    "- It cuts small coefficients all the way to **zero** — performing *feature selection*.\n",
    "- Useful when many predictors are irrelevant.\n",
    "\n",
    "##### **ElasticNet (L1 + L2) — “Rubber Band + Scissors”**\n",
    "- Soft shrinkage + ability to remove unimportant features.\n",
    "- Best when:\n",
    "  - features are correlated **and**\n",
    "  - some features should be removed.\n",
    "\n",
    "##### Why this matters\n",
    "OLS can overreact to noise or outliers.  \n",
    "Regularization keeps coefficient behavior **stable**, **predictable**, and often **more interpretable**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4871780",
   "metadata": {},
   "source": [
    "## 2.5 — Error Metrics (RMSE, MAE, R², MAPE)\n",
    "\n",
    "#### Quick explnation of the error metrics\n",
    "- **RMSE** (↓) square-root of average squared error; penalizes large errors; same units as target.  \n",
    "- **MAE** (↓) average absolute error; robust to outliers.  \n",
    "- **R²** (↑) fraction of variance explained.  \n",
    "- **MAPE** (↓) percent error (mind small `y` values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eedde5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already computed for OLS; reuse summarize() for others if needed\n",
    "# (Nothing to run here; this section summarizes metric usage & interpretation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270269f4",
   "metadata": {},
   "source": [
    "#### When should you use each metric?\n",
    "\n",
    "| Metric | Best for… | Avoid when… |\n",
    "|--------|-----------|--------------|\n",
    "| **RMSE** | catching large errors; comparing models during tuning | outliers dominate  \n",
    "| **MAE** | robust evaluation; easy interpretation (“avg error in dollars”) | you need to penalize big misses more  \n",
    "| **R²** | explaining variance; communicating model fit to stakeholders | comparing models on different datasets  \n",
    "| **MAPE** | percentage-based evaluation; cross-market comparison | target values are near zero (division blows up)  \n",
    "\n",
    "> **Rule of thumb:**  \n",
    "> Use **RMSE** for tuning, **MAE** for communication, **R²** for overall fit, and **MAPE** only when targets are not close to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f21e0",
   "metadata": {},
   "source": [
    "#### Choosing the right metric\n",
    "\n",
    "- Use **RMSE** when large errors matter (e.g., overpricing a luxury home).  \n",
    "- Use **MAE** when you want fairness and robustness across all homes.  \n",
    "- Use **R²** when communicating results to non-technical audiences.  \n",
    "- Use **MAPE** when comparing predictions across neighborhoods or markets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e1211",
   "metadata": {},
   "source": [
    "# Student Activity if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0748d7ba",
   "metadata": {},
   "source": [
    "#### Mini Exercise: Comparing Regression Error Metrics\n",
    "\n",
    "Use the predictions you generated in Sections 2.2–2.4 (OLS, Polynomial, or Regularized).\n",
    "Answer the questions below *using the numbers you see in your notebook*.\n",
    "\n",
    "---\n",
    "\n",
    "##### 1. RMSE vs MAE\n",
    "Look at the **RMSE** and **MAE** values for the OLS model.\n",
    "\n",
    "- Which one is larger?\n",
    "- What does this tell you about the presence of **large individual errors**?\n",
    "- If you had to report a single number to a real-estate agent, which would you choose and why?\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. R² Interpretation\n",
    "Check the **R²** value for the OLS model (and polynomial models if you ran them).\n",
    "\n",
    "- What does the R² value mean *in your own words*?\n",
    "- Does a higher R² always mean a better model? Why or why not?\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. MAPE and Real-World Meaning\n",
    "Look at the **MAPE%**.\n",
    "\n",
    "- If MAPE = 8%, what does that mean for predicting the price of a \\$400,000 home?\n",
    "- When might MAPE be misleading?\n",
    "\n",
    "---\n",
    "\n",
    "##### 4. Metric Tradeoffs\n",
    "Imagine you are advising two different stakeholders:\n",
    "\n",
    "**A. A homeowner:**  \n",
    "They want to know: *“How far off might the estimate be?”*\n",
    "\n",
    "**B. A data scientist:**  \n",
    "They want a metric that highlights **large errors** more strongly.\n",
    "\n",
    "- Which metric would you show the homeowner (RMSE, MAE, R², or MAPE)? Why?  \n",
    "- Which metric would you show the data scientist? Why?\n",
    "\n",
    "---\n",
    "\n",
    "##### 5. Model Comparison (Optional)\n",
    "Compare the OLS model vs the Polynomial degree-2 model (or Ridge vs OLS).\n",
    "\n",
    "- Which model has the lower RMSE?  \n",
    "- Which has the lower MAE?  \n",
    "- Do the models rank the same across all metrics?  \n",
    "- What does this tell you about choosing only *one* metric?\n",
    "\n",
    "---\n",
    "\n",
    "##### Reflection (1–2 sentences)\n",
    "Write a short reflection:\n",
    "\n",
    "> “Which metric do *you* personally find most intuitive, and in what situation might you pick a different one?”\n",
    "\n",
    "---\n",
    "\n",
    "### Goal of this exercise:\n",
    "By completing these questions, you should feel confident explaining  \n",
    "**what each regression metric means**, **why it matters**, and **when to use it**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73eea27",
   "metadata": {},
   "source": [
    "## 2.6 — Model Evaluation: Cross-Validation & Learning Curves\n",
    "\n",
    "Use **CV** for robust performance estimates and **learning curves** to diagnose data sufficiency & model capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb516374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
    "\n",
    "cv_rmse = cross_val_score(pipe, X, y, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "cv_r2   = cross_val_score(pipe, X, y, cv=5, scoring='r2', n_jobs=-1)\n",
    "print(f'CV RMSE: {-cv_rmse.mean():.0f} ± {cv_rmse.std():.0f}')\n",
    "print(f'CV R²  : {cv_r2.mean():.3f} ± {cv_r2.std():.3f}')\n",
    "\n",
    "# Learning curve\n",
    "sizes, train_scores, val_scores = learning_curve(\n",
    "    estimator=pipe, X=X, y=y, cv=5,\n",
    "    train_sizes=np.linspace(0.1,1.0,6),\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "train_rmse = -train_scores.mean(axis=1)\n",
    "val_rmse   = -val_scores.mean(axis=1)\n",
    "\n",
    "\n",
    "plt.plot(sizes, train_rmse, marker='o', label='Train RMSE')\n",
    "plt.plot(sizes, val_rmse, marker='s', label='CV RMSE')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Learning Curve (OLS)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da92a3e",
   "metadata": {},
   "source": [
    "**Reading the learning curve:**  \n",
    "- **Underfitting**: train ≈ val ≫ low → add features/complexity.  \n",
    "- **Overfitting**: train ≪ val → regularize, add data, or limit complexity.  \n",
    "- **Healthy**: train & val low and close."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1277139",
   "metadata": {},
   "source": [
    "#### Understanding the Cross-Validation Results\n",
    "\n",
    "**CV RMSE: 77,884 ± 68,279**  \n",
    "**CV R² : 0.045 ± 1.526**\n",
    "\n",
    "These numbers reflect how OLS performs across multiple train/test splits using 5-fold cross-validation.\n",
    "\n",
    "##### What the values mean:\n",
    "\n",
    "- **High average RMSE (~78k)**  \n",
    "  The model makes large errors on some folds. This is much worse than the test-set RMSE from Section 2.2 (~41k).\n",
    "\n",
    "- **Very large standard deviation (±68k)**  \n",
    "  Performance varies dramatically depending on which subset of the data the model sees.  \n",
    "  This is a sign of **instability** when training on smaller folds.\n",
    "\n",
    "- **Very low mean R² (~0.045)**  \n",
    "  On average, the model explains almost none of the variance in some folds.\n",
    "\n",
    "- **Huge spread in R² (±1.526)**  \n",
    "  Some folds show positive R², others strongly negative R² (worse than guessing the mean).  \n",
    "  This typically means the model is:\n",
    "\n",
    "  - too simple for the underlying nonlinear pattern (**underfitting**), **and**\n",
    "  - very sensitive to what data happens to be in each fold (**high variance on small samples**).\n",
    "\n",
    "##### Why this happens\n",
    "- Our synthetic dataset contains:\n",
    "  - curvature  \n",
    "  - interaction terms  \n",
    "  - heteroscedastic noise  \n",
    "  - intentionally injected “messiness”\n",
    "\n",
    "- **OLS (straight-line model)** cannot capture these patterns well, *especially* when the training set is small.\n",
    "\n",
    "- In some CV folds, the training subset is too small to learn the main relationships, so the model performs poorly or even worse than predicting the mean.\n",
    "\n",
    "---\n",
    "\n",
    "#### Understanding the Learning Curve\n",
    "\n",
    "The learning curve plot shows **Train RMSE** vs **CV RMSE** as we increase the training set size.\n",
    "\n",
    "##### Key observations:\n",
    "\n",
    "1. **Train RMSE increases**  \n",
    "   - With few samples, the model fits the small dataset too closely (low train error).  \n",
    "   - With more samples, the model generalizes better but training error rises.\n",
    "\n",
    "2. **CV RMSE decreases** (but not enough)  \n",
    "   - Larger training sets help the model stabilize (CV RMSE drops from ~90k toward ~75k).  \n",
    "   - But the gap remains large → the model is still underfitting.\n",
    "\n",
    "3. **The gap between Train RMSE and CV RMSE stays wide**  \n",
    "   - This is the signature of **high bias** (underfitting).  \n",
    "   - Even with more data, the model cannot represent the true function.\n",
    "\n",
    "4. **Curvature in the plot**  \n",
    "   - CV RMSE slowly levels off around ~75–80k, suggesting OLS has reached its limit.  \n",
    "   - Adding more data won’t fix a mismatch in model complexity.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key takeaways\n",
    "\n",
    "The cross-validation scores and learning curve together show that:\n",
    "\n",
    "- **OLS is too simple** for this dataset  \n",
    "- The true relationship between features and price has **nonlinearities** that OLS cannot capture  \n",
    "- Polynomial regression or regularization will significantly improve generalization (as we saw in Section 2.3–2.4)\n",
    "\n",
    "This is exactly *why* we expand to:\n",
    "- **PolynomialFeatures + Ridge/Lasso**, and  \n",
    "- **Regularized models** that stabilize the expanded feature space.\n",
    "\n",
    "The learning curve visually reinforces the message:\n",
    "> The model’s performance is limited not by data quantity, but by model capacity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880b0a63",
   "metadata": {},
   "source": [
    "## Optional Visualizations & Diagnostics\n",
    "\n",
    "These visual tools help students understand **model behavior**, **error structure**, \n",
    "and **coefficient influence**:\n",
    "\n",
    "- **Predicted vs Actual Plot**  \n",
    "- **Residual Distribution Histogram**  \n",
    "- **Partial Dependence Plot (price vs sqft)**  \n",
    "- **RidgeCV Coefficient Bar Chart (standardized)**  \n",
    "- **QQ-Plot for residual normality check**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa1264",
   "metadata": {},
   "source": [
    "### Predicted vs Actual (Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Predicted vs Actual ---\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use OLS predictions computed earlier (yhat_ols)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(yte, yhat_ols, alpha=0.6)\n",
    "plt.plot([yte.min(), yte.max()], [yte.min(), yte.max()], 'k--', label='Ideal (y = ŷ)')\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Predicted vs Actual — OLS\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f20c315",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation:**  \n",
    "- Points close to the dashed line mean good predictions.  \n",
    "- Systematic curvature or widening gaps indicate underfitting or missing nonlinear effects.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be3292",
   "metadata": {},
   "source": [
    "### Predicted vs Actual Prices — OLS\n",
    "\n",
    "This scatter plot compares the model’s predictions (ŷ) to the actual housing prices (y).  \n",
    "The diagonal dashed line represents **perfect predictions**: every point on the line\n",
    "would correspond to a model that predicts the target exactly.\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- Most points lie **reasonably close to the diagonal**, meaning OLS captures the general\n",
    "  trend between home features and price.  \n",
    "- There is a visible **spread** around the line, indicating prediction errors that increase\n",
    "  for higher-priced homes.\n",
    "- A few points deviate significantly, which suggests:\n",
    "  - variation in real housing data that a simple linear model cannot fully capture  \n",
    "  - the presence of outliers or nonlinear relationships  \n",
    "  - limitations of OLS when predicting more expensive or unusual homes  \n",
    "\n",
    "#### What this tells us?\n",
    "\n",
    "- **OLS captures the broad pattern**: bigger homes, newer homes, larger lots → higher price.  \n",
    "- But the scatter indicates **systematic nonlinearities** OLS cannot handle:\n",
    "  - diminishing returns (e.g., sqft does not increase value linearly)  \n",
    "  - interaction effects (e.g., size × location)  \n",
    "  - nonlinear price behavior for luxury or remote homes  \n",
    "\n",
    "This plot motivates the need for more flexible models such as **Polynomial Regression**,  \n",
    "which will better capture curvature in the relationship and reduce systematic error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28770f",
   "metadata": {},
   "source": [
    "### Residual Distribution (Histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897574c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Residual Histogram ---\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(resid, bins=30, alpha=0.7)\n",
    "plt.xlabel(\"Residual (y - ŷ)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Residuals — OLS\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d75ab",
   "metadata": {},
   "source": [
    "\n",
    "**Goal:** Assess whether residuals are centered around zero and roughly symmetric.  \n",
    "Skewed or heavy-tailed residuals suggest the usefulness of nonlinear terms or robust models.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325966bc",
   "metadata": {},
   "source": [
    "### Distribution of Residuals — OLS\n",
    "\n",
    "This histogram shows the distribution of residuals:\n",
    "\n",
    "\n",
    "$\\text{residual} = y - \\hat{y}$\n",
    "\n",
    "\n",
    "Residuals represent the difference between the actual home price and the price predicted\n",
    "by the OLS model.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "#### **1. Residuals are not centered perfectly around zero**\n",
    "While many residuals cluster near zero, the distribution is **asymmetric**, showing:\n",
    "- a **long tail to the left** (large negative residuals)  \n",
    "- some **extreme prediction errors**  \n",
    "- evidence that OLS systematically **underestimates** some homes and **overestimates** others\n",
    "\n",
    "This suggests **nonlinear relationships** between features and price that OLS cannot capture.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. A few very large errors**\n",
    "Large residuals (e.g., −200,000) indicate:\n",
    "- outliers in the data  \n",
    "- unusually expensive homes  \n",
    "- or nonlinear effects not handled by a purely linear model\n",
    "\n",
    "These extreme values pull the distribution to the left and widen the range of errors.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Residuals have clear structure**\n",
    "If OLS were the correct model:\n",
    "- residuals would be **normally distributed**  \n",
    "- centered around zero  \n",
    "- with symmetrical spread\n",
    "\n",
    "Instead, the distribution is skewed, signaling that:\n",
    "- **model assumptions are violated**,  \n",
    "- the relationship between features and price is **nonlinear**,  \n",
    "- OLS cannot fully explain variance in price.\n",
    "\n",
    "---\n",
    "\n",
    "### Why this matters?\n",
    "\n",
    "This residual pattern tells us:\n",
    "\n",
    "- OLS captures the **general trend** of home pricing  \n",
    "- But fails to model **curvature**, **interactions**, and **heteroscedasticity**  \n",
    "- More flexible models (Polynomial Regression, Regularization, Ensembles) will likely perform better\n",
    "\n",
    "This visualization motivates the next sections (2.3–2.6), where we explore **nonlinear** and **regularized** regression methods that can better handle the complexities of housing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d67268f",
   "metadata": {},
   "source": [
    "### Partial Dependence Plot — price vs sqft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Partial dependence: price vs sqft ---\n",
    "feat = 'sqft'\n",
    "\n",
    "# Fix all other features at their median\n",
    "fixed = Xtr.median(numeric_only=True).to_dict()\n",
    "\n",
    "x_grid = np.linspace(df[feat].quantile(0.05),\n",
    "                     df[feat].quantile(0.95),\n",
    "                     200)\n",
    "\n",
    "rows=[]\n",
    "for x in x_grid:\n",
    "    r = fixed.copy()\n",
    "    r[feat] = x\n",
    "    rows.append(r)\n",
    "\n",
    "X_line = pd.DataFrame(rows)[Xtr.columns]\n",
    "y_line = ols.predict(X_line)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(df[feat], y, s=12, alpha=0.25, label='Actual data')\n",
    "plt.plot(x_grid, y_line, 'r', label='OLS prediction (others fixed)')\n",
    "plt.xlabel(\"sqft\")\n",
    "plt.ylabel(\"price\")\n",
    "plt.title(\"Partial Dependence: price vs sqft\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaedb34",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation:**  \n",
    "Shows how the model believes `sqft` affects price when all other features are held constant.  \n",
    "A linear trend is expected for OLS; curvature would appear if polynomial models were used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2805be",
   "metadata": {},
   "source": [
    "### Partial Dependence: How OLS Predicts Price from Square Footage\n",
    "\n",
    "A partial dependence plot (PDP) shows how the model’s predictions change with respect to\n",
    "one feature—in this case, **square footage (sqft)**—while **holding all other features at\n",
    "a fixed (median) value**.\n",
    "\n",
    "This allows us to isolate the model’s understanding of the relationship between\n",
    "sqft and price.\n",
    "\n",
    "---\n",
    "\n",
    "### What the Plot Shows?\n",
    "\n",
    "#### 1. **OLS learns a strictly linear relationship**\n",
    "The red line is perfectly straight because Ordinary Least Squares can only model a\n",
    "**linear** relationship between sqft and price.\n",
    "\n",
    "- As sqft increases, predicted price increases at a constant rate.\n",
    "- The slope of this line corresponds to the OLS coefficient for sqft  \n",
    "  (about **\\$23–\\$24 per additional square foot**).\n",
    "\n",
    "#### 2. **Actual data shows curvature**\n",
    "The cloud of blue points reveals a subtle but important pattern:\n",
    "\n",
    "- Prices rise **faster** for mid-sized homes  \n",
    "- Prices rise **more slowly** for very large homes  \n",
    "- The point cloud is **curved**, not straight\n",
    "\n",
    "This mismatch shows that sqft does **not** have a perfectly linear relationship with price in reality.\n",
    "\n",
    "#### 3. **OLS systematically underestimates larger homes**\n",
    "At high sqft values:\n",
    "- The OLS line sits **below** the cloud → **underprediction**\n",
    "\n",
    "At moderate sqft values:\n",
    "- The line sits **above** some of the cloud → **overprediction**\n",
    "\n",
    "This pattern indicates that OLS is missing important **nonlinear** effects.\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Matters?\n",
    "\n",
    "This plot clearly shows why we need more flexible models:\n",
    "\n",
    "- **Polynomial Regression** (Section 2.3) can capture curves  \n",
    "- **Regularization** (Section 2.4) helps stabilize these richer models  \n",
    "- **Cross-validation** (Section 2.6) is needed to ensure we do not overfit when adding complexity  \n",
    "\n",
    "OLS is a strong starting point—transparent, interpretable, and easy to compute—\n",
    "but this PDP highlights the limitations of linearity in real housing data.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "The partial dependence plot reveals:\n",
    "\n",
    "- OLS models **only a straight-line trend**  \n",
    "- Real price–sqft relationships are **curved**  \n",
    "- Linear models underfit important nonlinear structure  \n",
    "\n",
    "This visual provides a perfect bridge into **Section 2.3: Polynomial Regression**, where we allow the model to learn curvature and improve predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cabc416",
   "metadata": {},
   "source": [
    "### RidgeCV Coefficient Bar Chart (Standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- RidgeCV for coefficient interpretability ---\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "ridgecv = make_pipeline(StandardScaler(), RidgeCV(alphas=np.logspace(-4,4,25), cv=5))\n",
    "ridgecv.fit(Xtr, ytr)\n",
    "\n",
    "# Standardized coefficients come from the RidgeCV step\n",
    "ridge_model = ridgecv.named_steps['ridgecv']\n",
    "coefs = ridge_model.coef_\n",
    "features = Xtr.columns\n",
    "\n",
    "coef_df = pd.DataFrame({'feature': features, 'coef': coefs})\n",
    "coef_df = coef_df.sort_values('coef')\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.barh(coef_df['feature'], coef_df['coef'])\n",
    "plt.xlabel(\"Standardized Coefficient\")\n",
    "plt.title(\"RidgeCV Coefficients (Standardized Features)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a9a74",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation:**  \n",
    "- Coefficients are on a **standardized scale**, so magnitude reflects importance.  \n",
    "- Positive values ↑ price; negative values ↓ price.  \n",
    "- Ridge helps stabilize correlated predictors (e.g., sqft, bedrooms, lot_size).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f119ed",
   "metadata": {},
   "source": [
    "### Ridge Regression (RidgeCV) — Standardized Coefficients\n",
    "\n",
    "This visualization shows the coefficients learned by a **RidgeCV** regression model\n",
    "after standardizing all input features. Standardization ensures that all variables are on\n",
    "the same scale, allowing the model to apply regularization fairly across features.\n",
    "\n",
    "Because Ridge shrinks coefficients toward zero (but never to zero), this plot helps reveal:\n",
    "\n",
    "- which features the model considers most important  \n",
    "- how strongly each feature influences the target  \n",
    "- how regularization reshapes the coefficient landscape compared to plain OLS  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights from the Plot\n",
    "\n",
    "#### **1. Lot size remains the strongest predictor**\n",
    "Even after standardization and regularization, **lot_size** has the largest coefficient.\n",
    "This confirms that lot size is a major driver of home value.\n",
    "\n",
    "#### **2. bedrooms, sqft, and bathrooms follow closely**\n",
    "These features retain large positive coefficients, reinforcing their importance in pricing:\n",
    "\n",
    "- **bedrooms** and **bathrooms** capture functional home size  \n",
    "- **sqft** reflects interior space  \n",
    "- These variables remain robust predictors even with shrinkage applied\n",
    "\n",
    "#### **3. dist_to_center_km and age_years have much smaller coefficients**\n",
    "Ridge shrinks these weaker predictors substantially:\n",
    "\n",
    "- **dist_to_center_km** has a small negative coefficient  \n",
    "- **age_years** shows a modest negative effect  \n",
    "\n",
    "This suggests these features contribute less predictive power relative to the major ones.\n",
    "\n",
    "#### **4. Regularization stabilizes the model**\n",
    "OLS coefficients can sometimes become large or unstable when features correlate  \n",
    "(e.g., bedrooms, bathrooms, and sqft).  \n",
    "Ridge reduces this instability by discouraging overly large weights.\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Matters?\n",
    "\n",
    "By comparing RidgeCV coefficients to the OLS coefficients:\n",
    "\n",
    "- You see which relationships are **structurally strong** (lot_size, bedrooms, sqft)  \n",
    "- You see which relationships are more **fragile** or sensitive to noise  \n",
    "- Regularization provides a **more robust, less overfitted** set of coefficients  \n",
    "- Standardized coefficients make magnitudes meaningfully comparable  \n",
    "\n",
    "This visualization bridges directly into **Section 2.4 (Regularization)**, where you\n",
    "learn how Ridge, Lasso, and ElasticNet work and why regularization is often essential\n",
    "in real-world regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e40f65",
   "metadata": {},
   "source": [
    "### QQ-Plot of Residuals (Normality Check) — Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1423296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- QQ-plot of residuals ---\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "stats.probplot(resid, dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ-Plot: Residuals vs Normal Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb7c57",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation:**  \n",
    "- If residuals follow a straight diagonal line → errors are roughly normal.  \n",
    "- Curved patterns → heavy tails, skewness, or model misspecification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a330a",
   "metadata": {},
   "source": [
    "### QQ-Plot: Residuals vs Normal Distribution\n",
    "\n",
    "A QQ-plot compares the distribution of the residuals (errors) to a theoretical normal\n",
    "distribution. If OLS assumptions hold and errors are normally distributed, the points\n",
    "should lie close to the red diagonal line.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "#### **1. Residuals follow the normal line in the middle**\n",
    "Most blue points fall reasonably close to the red line around the center of the\n",
    "distribution.  \n",
    "This suggests:\n",
    "\n",
    "- the bulk of residuals are approximately normal  \n",
    "- OLS captures the central trend well  \n",
    "\n",
    "However...\n",
    "\n",
    "#### **2. Systematic deviation in the lower tail**\n",
    "In the lower-left region (large negative residuals), the points fall **well below** the\n",
    "red line:\n",
    "\n",
    "- these are homes where the model **underestimates** price dramatically  \n",
    "- indicates **outliers** or **nonlinear behavior**  \n",
    "- suggests heteroscedasticity (unequal variance)  \n",
    "\n",
    "These heavy-tailed errors violate OLS’s normality assumption.\n",
    "\n",
    "#### **3. Deviations in the upper tail**\n",
    "At the top-right, the largest positive residuals also deviate from the line:\n",
    "\n",
    "- indicates occasional **overestimation**  \n",
    "- strengthens the evidence of **non-normality** in residuals  \n",
    "\n",
    "---\n",
    "\n",
    "### What this tells us about OLS?\n",
    "\n",
    "OLS relies on the assumption that residuals are:\n",
    "\n",
    "1. **normally distributed**  \n",
    "2. **homoscedastic** (constant variance)  \n",
    "3. **centered around 0**  \n",
    "\n",
    "This QQ-plot shows violations of assumptions **1** and **2**, due to:\n",
    "\n",
    "- nonlinear relationships  \n",
    "- outliers  \n",
    "- features interacting in ways OLS cannot capture  \n",
    "\n",
    "These issues reduce OLS’s accuracy and motivate the need for **Polynomial Regression**  \n",
    "and **Regularized models**, which can better handle:\n",
    "\n",
    "- curvature  \n",
    "- interactions  \n",
    "- feature collinearity  \n",
    "- outlier robustness  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "While OLS provides an interpretable baseline, the QQ-plot reveals:\n",
    "\n",
    "- non-normal residuals  \n",
    "- asymmetric tails  \n",
    "- underestimation of high-value homes  \n",
    "- limited ability to model nonlinear price behavior  \n",
    "\n",
    "This reinforces that OLS is only a **starting point**, and more flexible techniques  \n",
    "(2.3–2.6) are needed for higher accuracy on complex datasets like housing prices.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
