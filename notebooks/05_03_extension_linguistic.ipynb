{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d027de1d",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/tunnel-ai/way/blob/main/notebooks/05_03_extension_linguistic.ipynb\n",
    ")\n",
    "\n",
    "# Module 5 — NLP (Optional Extension)\n",
    "## `05_03_extension_linguistic.ipynb` — Agenda as Rhetorical Style\n",
    "\n",
    "This optional notebook is designed as **accordion time**:\n",
    "- If we move fast, we can use this to extend the module.\n",
    "- If we move slow, we are going to skip it entirely.\n",
    "\n",
    "### Big idea\n",
    "In `05_00` and `05_01`, we treated “agenda” as differences in **semantic emphasis**.\n",
    "\n",
    "Here we ask a different question:\n",
    "\n",
    "> Do differences also appear in **how research is written** (rhetorical style), not just what it discusses?\n",
    "\n",
    "We focus on **AI governance** abstracts and compute simple, interpretable linguistic features:\n",
    "- lexical diversity (Type–Token Ratio)\n",
    "- readability proxies (Flesch–Kincaid, Gunning Fog)\n",
    "- average sentence length, average word length\n",
    "- part-of-speech (POS) composition\n",
    "\n",
    "### Standalone design\n",
    "This notebook **does not assume** you already ran `05_00` or `05_01`.\n",
    "It will:\n",
    "1. Fetch a sample of abstracts from OpenAlex (no keys) and cache locally\n",
    "2. Map affiliations → (very) coarse regions (United States / Europe / China / Other / Unknown)\n",
    "3. Condition on an AI governance subset\n",
    "4. Compute linguistic features and compare across regions\n",
    "\n",
    "> **Caution:** These are descriptive proxies. They do not establish causality or intent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35249e9",
   "metadata": {},
   "source": [
    "## 0) Colab-first setup\n",
    "\n",
    "Run this notebook in Google Colab.\n",
    "\n",
    "- We clone the repo so relative paths work consistently.\n",
    "- We install a few lightweight NLP utilities (`textstat`, `nltk`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the course repo (Colab-friendly)\n",
    "!git clone -q https://github.com/tunnel-ai/way.git\n",
    "%cd way\n",
    "\n",
    "# Lightweight installs (Colab-safe)\n",
    "!pip -q install textstat nltk\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import textwrap\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import nltk\n",
    "import textstat\n",
    "\n",
    "# NLTK assets for tokenization + POS tagging\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a3ff1",
   "metadata": {},
   "source": [
    "## 1) Fetch a sample from OpenAlex (and cache it)\n",
    "\n",
    "We fetch a manageable sample of works that have:\n",
    "- an abstract\n",
    "- institutional affiliation metadata (used later for region proxy)\n",
    "\n",
    "We cache to:\n",
    "- `assets/data/openalex_abstracts_sample.csv`\n",
    "\n",
    "If the cache exists, we load it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd768fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"assets/data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "RAW_CACHE_PATH = os.path.join(DATA_DIR, \"openalex_abstracts_sample.csv\")\n",
    "\n",
    "@dataclass\n",
    "class FetchConfig:\n",
    "    per_page: int = 200\n",
    "    max_works: int = 2500         # modest, Colab-friendly\n",
    "    from_year: int = 2022\n",
    "    mailto: str | None = None     # optional\n",
    "\n",
    "CFG = FetchConfig(per_page=200, max_works=2500, from_year=2022, mailto=None)\n",
    "\n",
    "BASE = \"https://api.openalex.org/works\"\n",
    "\n",
    "def build_openalex_url(cursor=\"*\"):\n",
    "    params = {\n",
    "        \"per-page\": CFG.per_page,\n",
    "        \"cursor\": cursor,\n",
    "        \"filter\": f\"has_abstract:true,from_publication_date:{CFG.from_year}-01-01\",\n",
    "        \"sort\": \"publication_date:desc\",\n",
    "    }\n",
    "    if CFG.mailto:\n",
    "        params[\"mailto\"] = CFG.mailto\n",
    "    return BASE, params\n",
    "\n",
    "def inverted_index_to_text(inv):\n",
    "    if inv is None or not isinstance(inv, dict) or len(inv) == 0:\n",
    "        return None\n",
    "    max_pos = 0\n",
    "    for token, positions in inv.items():\n",
    "        if positions:\n",
    "            max_pos = max(max_pos, max(positions))\n",
    "    tokens = [\"\"] * (max_pos + 1)\n",
    "    for token, positions in inv.items():\n",
    "        for p in positions:\n",
    "            if 0 <= p < len(tokens) and tokens[p] == \"\":\n",
    "                tokens[p] = token\n",
    "    text = \" \".join([t for t in tokens if t])\n",
    "    return text if text.strip() else None\n",
    "\n",
    "def extract_country_codes(work: dict) -> list[str]:\n",
    "    codes = []\n",
    "    for auth in work.get(\"authorships\", []) or []:\n",
    "        for inst in auth.get(\"institutions\", []) or []:\n",
    "            cc = inst.get(\"country_code\")\n",
    "            if cc:\n",
    "                codes.append(cc.upper())\n",
    "    return sorted(set(codes))\n",
    "\n",
    "def fetch_openalex_sample():\n",
    "    rows = []\n",
    "    cursor = \"*\"\n",
    "    fetched = 0\n",
    "    while fetched < CFG.max_works:\n",
    "        url, params = build_openalex_url(cursor=cursor)\n",
    "        r = requests.get(url, params=params, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        payload = r.json()\n",
    "\n",
    "        for work in payload.get(\"results\", []) or []:\n",
    "            if fetched >= CFG.max_works:\n",
    "                break\n",
    "\n",
    "            abstract = inverted_index_to_text(work.get(\"abstract_inverted_index\"))\n",
    "            if not abstract:\n",
    "                continue\n",
    "\n",
    "            country_codes = extract_country_codes(work)\n",
    "\n",
    "            # None-safe access for primary location\n",
    "            loc = work.get(\"primary_location\") or {}\n",
    "            src = (loc.get(\"source\") or {})\n",
    "\n",
    "            rows.append({\n",
    "                \"openalex_id\": work.get(\"id\"),\n",
    "                \"doi\": work.get(\"doi\"),\n",
    "                \"title\": work.get(\"title\"),\n",
    "                \"publication_date\": work.get(\"publication_date\"),\n",
    "                \"primary_location\": src.get(\"display_name\"),\n",
    "                \"country_codes\": \"|\".join(country_codes) if country_codes else \"\",\n",
    "                \"n_country_codes\": len(country_codes),\n",
    "                \"abstract\": abstract,\n",
    "                \"type\": work.get(\"type\"),\n",
    "                \"cited_by_count\": work.get(\"cited_by_count\"),\n",
    "            })\n",
    "            fetched += 1\n",
    "\n",
    "        cursor = payload.get(\"meta\", {}).get(\"next_cursor\")\n",
    "        if not cursor:\n",
    "            break\n",
    "        time.sleep(0.15)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "if os.path.exists(RAW_CACHE_PATH):\n",
    "    df = pd.read_csv(RAW_CACHE_PATH)\n",
    "    print(f\"Loaded cached sample: {len(df):,} rows from {RAW_CACHE_PATH}\")\n",
    "else:\n",
    "    df = fetch_openalex_sample()\n",
    "    print(f\"Fetched sample: {len(df):,} rows\")\n",
    "    df.to_csv(RAW_CACHE_PATH, index=False)\n",
    "    print(f\"Saved cache to {RAW_CACHE_PATH}\")\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d9295",
   "metadata": {},
   "source": [
    "## 2) From affiliations to regions (a proxy)\n",
    "\n",
    "We map country codes to broad regions:\n",
    "- United States\n",
    "- China\n",
    "- Europe (broad)\n",
    "- Other\n",
    "- Unknown\n",
    "\n",
    "**Reminder:** affiliation ≠ nationality; region ≠ agenda; this is a coarse proxy to enable comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b996cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "EUROPE_CODES = {\n",
    "    \"AT\",\"BE\",\"BG\",\"HR\",\"CY\",\"CZ\",\"DK\",\"EE\",\"FI\",\"FR\",\"DE\",\"GR\",\"HU\",\"IE\",\"IT\",\"LV\",\"LT\",\"LU\",\n",
    "    \"MT\",\"NL\",\"PL\",\"PT\",\"RO\",\"SK\",\"SI\",\"ES\",\"SE\",\n",
    "    \"GB\",\"UK\",\n",
    "    \"NO\",\"CH\",\"IS\",\"LI\",\n",
    "    \"UA\",\"TR\",\"RS\",\"BA\",\"ME\",\"MK\",\"AL\",\"MD\",\"BY\",\"GE\",\"AM\",\"AZ\",\n",
    "}\n",
    "\n",
    "def map_region(country_codes_str: str) -> str:\n",
    "    if not isinstance(country_codes_str, str) or country_codes_str.strip() == \"\":\n",
    "        return \"Unknown\"\n",
    "    codes = {c.strip().upper() for c in country_codes_str.split(\"|\") if c.strip()}\n",
    "    if \"US\" in codes:\n",
    "        return \"United States\"\n",
    "    if \"CN\" in codes:\n",
    "        return \"China\"\n",
    "    if len(codes & EUROPE_CODES) > 0:\n",
    "        return \"Europe\"\n",
    "    return \"Other\"\n",
    "\n",
    "df[\"region\"] = df[\"country_codes\"].apply(map_region)\n",
    "df[\"region\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dec8ad",
   "metadata": {},
   "source": [
    "## 3) Minimal text cleaning\n",
    "\n",
    "For linguistic features, we want something close to the original text, but we still:\n",
    "- remove URLs\n",
    "- normalize whitespace\n",
    "\n",
    "We do **not** aggressively strip punctuation here, because sentence boundaries matter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pat = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "multi_space_pat = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_for_style(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    s = url_pat.sub(\" \", s)\n",
    "    s = multi_space_pat.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"text\"] = df[\"abstract\"].apply(clean_for_style)\n",
    "\n",
    "# quick peek\n",
    "i = 0\n",
    "print(df.loc[i, \"title\"])\n",
    "print(df.loc[i, \"region\"])\n",
    "print(textwrap.shorten(df.loc[i, \"text\"], width=420, placeholder=\"…\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3438692",
   "metadata": {},
   "source": [
    "## 4) Condition on a shared topic: AI governance\n",
    "\n",
    "We define an AI governance subset using a transparent keyword filter.\n",
    "\n",
    "This is a **topic conditioning** step (same spirit as Act II in `05_00`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c3ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOV_TERMS = [\n",
    "    \"governance\",\n",
    "    \"ethics\",\n",
    "    \"ethical\",\n",
    "    \"fairness\",\n",
    "    \"bias\",\n",
    "    \"accountability\",\n",
    "    \"transparency\",\n",
    "    \"privacy\",\n",
    "    \"regulation\",\n",
    "    \"regulatory\",\n",
    "    \"compliance\",\n",
    "    \"risk\",\n",
    "    \"responsible ai\",\n",
    "]\n",
    "\n",
    "pattern = \"|\".join(GOV_TERMS)\n",
    "\n",
    "df_gov = df[df[\"text\"].str.lower().str.contains(pattern, regex=True)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original corpus: {len(df):,}\")\n",
    "print(f\"AI governance subset: {len(df_gov):,}\")\n",
    "df_gov[\"region\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce50e1",
   "metadata": {},
   "source": [
    "### Quick inspection (optional)\n",
    "\n",
    "Skim a few abstracts to ensure the subset is on-topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cee281",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(min(3, len(df_gov))):\n",
    "    print(\"\\n\" + \"—\"*50)\n",
    "    print(df_gov.loc[i, \"title\"])\n",
    "    print(\"Region:\", df_gov.loc[i, \"region\"])\n",
    "    print(textwrap.shorten(df_gov.loc[i, \"text\"], width=420, placeholder=\"…\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b0c6db",
   "metadata": {},
   "source": [
    "## 5) Linguistic feature extraction\n",
    "\n",
    "We compute simple, interpretable features:\n",
    "\n",
    "- **Type–Token Ratio (TTR):** lexical diversity proxy  \n",
    "- **Readability (Textstat):** Flesch–Kincaid grade, Gunning Fog  \n",
    "- **Avg sentence length:** words per sentence  \n",
    "- **Avg word length:** characters per word  \n",
    "- **POS composition (NLTK):** counts of nouns, verbs, adjectives, adverbs (normalized)\n",
    "\n",
    "> These are proxies for rhetorical style. They are not “quality” measures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1716b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_word_tokens(text: str):\n",
    "    # Basic tokenization via NLTK\n",
    "    return [t for t in nltk.word_tokenize(text) if re.search(r\"[A-Za-z]\", t)]\n",
    "\n",
    "def compute_ttr(text: str) -> float:\n",
    "    toks = [t.lower() for t in safe_word_tokens(text)]\n",
    "    if len(toks) == 0:\n",
    "        return np.nan\n",
    "    return len(set(toks)) / len(toks)\n",
    "\n",
    "def avg_sentence_length(text: str) -> float:\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    if len(sents) == 0:\n",
    "        return np.nan\n",
    "    lens = []\n",
    "    for s in sents:\n",
    "        toks = safe_word_tokens(s)\n",
    "        if len(toks) > 0:\n",
    "            lens.append(len(toks))\n",
    "    return float(np.mean(lens)) if lens else np.nan\n",
    "\n",
    "def avg_word_length(text: str) -> float:\n",
    "    toks = safe_word_tokens(text)\n",
    "    if len(toks) == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean([len(t) for t in toks]))\n",
    "\n",
    "# POS: coarse buckets\n",
    "POS_BUCKETS = {\n",
    "    \"NOUN\": {\"NN\", \"NNS\", \"NNP\", \"NNPS\"},\n",
    "    \"VERB\": {\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"},\n",
    "    \"ADJ\":  {\"JJ\", \"JJR\", \"JJS\"},\n",
    "    \"ADV\":  {\"RB\", \"RBR\", \"RBS\"},\n",
    "}\n",
    "\n",
    "def pos_bucket_shares(text: str):\n",
    "    toks = safe_word_tokens(text)\n",
    "    if len(toks) == 0:\n",
    "        return {k: np.nan for k in POS_BUCKETS}\n",
    "    tags = nltk.pos_tag(toks)\n",
    "    counts = {k: 0 for k in POS_BUCKETS}\n",
    "    total = 0\n",
    "    for _, tag in tags:\n",
    "        total += 1\n",
    "        for bucket, tagset in POS_BUCKETS.items():\n",
    "            if tag in tagset:\n",
    "                counts[bucket] += 1\n",
    "                break\n",
    "    # normalize to shares\n",
    "    return {k: (counts[k] / total if total else np.nan) for k in counts}\n",
    "\n",
    "def readability_fk(text: str) -> float:\n",
    "    try:\n",
    "        return float(textstat.flesch_kincaid_grade(text))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def readability_gf(text: str) -> float:\n",
    "    try:\n",
    "        return float(textstat.gunning_fog(text))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# Compute features\n",
    "df_feat = df_gov[[\"title\", \"region\", \"text\"]].copy()\n",
    "\n",
    "df_feat[\"ttr\"] = df_feat[\"text\"].apply(compute_ttr)\n",
    "df_feat[\"fk_grade\"] = df_feat[\"text\"].apply(readability_fk)\n",
    "df_feat[\"gunning_fog\"] = df_feat[\"text\"].apply(readability_gf)\n",
    "df_feat[\"avg_sent_len\"] = df_feat[\"text\"].apply(avg_sentence_length)\n",
    "df_feat[\"avg_word_len\"] = df_feat[\"text\"].apply(avg_word_length)\n",
    "\n",
    "pos_shares = df_feat[\"text\"].apply(pos_bucket_shares)\n",
    "df_feat[\"noun_share\"] = pos_shares.apply(lambda d: d[\"NOUN\"])\n",
    "df_feat[\"verb_share\"] = pos_shares.apply(lambda d: d[\"VERB\"])\n",
    "df_feat[\"adj_share\"]  = pos_shares.apply(lambda d: d[\"ADJ\"])\n",
    "df_feat[\"adv_share\"]  = pos_shares.apply(lambda d: d[\"ADV\"])\n",
    "\n",
    "df_feat.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68c74f",
   "metadata": {},
   "source": [
    "## 6) Compare style features across regions\n",
    "\n",
    "We compare distributions by region. These plots are descriptive.\n",
    "\n",
    "**Tip:** if the subset is small for a region (e.g., China), interpret differences cautiously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\"ttr\", \"fk_grade\", \"gunning_fog\", \"avg_sent_len\", \"avg_word_len\",\n",
    "            \"noun_share\", \"verb_share\", \"adj_share\", \"adv_share\"]\n",
    "\n",
    "# Basic summary table\n",
    "summary = df_feat.groupby(\"region\")[FEATURES].agg([\"count\", \"mean\", \"median\"]).round(3)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a35370",
   "metadata": {},
   "source": [
    "### Distribution plots (accordion-friendly)\n",
    "\n",
    "We show a simple boxplot per feature.\n",
    "\n",
    "If time is tight, you can skip this entire section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8672754",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [r for r in [\"United States\", \"Europe\", \"China\", \"Other\", \"Unknown\"] if r in df_feat[\"region\"].unique()]\n",
    "\n",
    "def boxplot_feature(feature):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for r in regions:\n",
    "        vals = df_feat.loc[df_feat[\"region\"] == r, feature].dropna().to_numpy()\n",
    "        if len(vals) > 0:\n",
    "            data.append(vals)\n",
    "            labels.append(r)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.boxplot(data, labels=labels, showfliers=False)\n",
    "    plt.title(f\"{feature} by region (AI governance subset)\")\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for f in FEATURES:\n",
    "    boxplot_feature(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64903c9",
   "metadata": {},
   "source": [
    "## 7) Optional: A single “style profile” radar chart (quick glance)\n",
    "\n",
    "This is a compact way to compare regions on multiple normalized features at once.\n",
    "\n",
    "- We z-score each feature across the full governance subset.\n",
    "- We then plot mean z-scores by region.\n",
    "\n",
    "If you prefer to keep things simple, skip this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21bb461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score features (across all docs)\n",
    "Z = df_feat.copy()\n",
    "for f in FEATURES:\n",
    "    col = Z[f]\n",
    "    mu = col.mean(skipna=True)\n",
    "    sd = col.std(skipna=True)\n",
    "    Z[f + \"_z\"] = (col - mu) / sd if sd and sd > 0 else np.nan\n",
    "\n",
    "Z_FEATURES = [f + \"_z\" for f in FEATURES]\n",
    "\n",
    "profile = Z.groupby(\"region\")[Z_FEATURES].mean().dropna(how=\"all\")\n",
    "\n",
    "# Keep a few major regions for readability\n",
    "keep = [r for r in [\"United States\", \"Europe\", \"China\"] if r in profile.index]\n",
    "profile_small = profile.loc[keep]\n",
    "\n",
    "profile_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be62ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple radar plot with matplotlib (no styling beyond defaults)\n",
    "import math\n",
    "\n",
    "labels = FEATURES\n",
    "num_vars = len(labels)\n",
    "\n",
    "angles = [n / float(num_vars) * 2 * math.pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "for r in profile_small.index:\n",
    "    values = profile_small.loc[r, Z_FEATURES].to_numpy().tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, linewidth=2, label=r)\n",
    "    ax.fill(angles, values, alpha=0.08)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(labels, fontsize=9)\n",
    "ax.set_title(\"Region style profile (mean z-scores) — AI governance subset\", y=1.08)\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.25, 1.15))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59b14a",
   "metadata": {},
   "source": [
    "## 8) Wrap-up: what this extension adds\n",
    "\n",
    "- In the main notebooks, we treated “agenda” as **semantic emphasis**.\n",
    "- Here we treated “agenda” as **rhetorical style**.\n",
    "\n",
    "So we need to stretch a little to think of useful applications for this approach. Will leave that to you. In this very contrived instance, a useful takeaway isn't “region X writes better.”\n",
    "\n",
    "A useful takeaway might be:\n",
    "\n",
    "> Different institutional contexts may shape *how research is framed* (complexity, density, lexical diversity, rhetorical structure).\n",
    "\n",
    "If you want to go further, a natural next step is to compare:\n",
    "- AI governance vs a purely technical AI subtopic (e.g., optimization)\n",
    "to see whether stylistic differences increase when policy context matters.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
