{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2460e6a8",
   "metadata": {},
   "source": [
    "# Modules 1 Main Notebook\n",
    "**Seed = 1955** · Datasets: `data/housing_synth.csv`, `data/titanic_synth.csv`\n",
    "\n",
    "Welcome to **Module 1 Advanced Tech - AI, ML, and Data Science** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea37caf",
   "metadata": {},
   "source": [
    "### How to Use This Notebook\n",
    "Each section uses the pattern: **Introduction → Code Example → Step-by-Step Explanation → Output Interpretation → Summary**.\n",
    "All randomness uses **seed=1955** for reproducibility across different machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef1372",
   "metadata": {},
   "source": [
    "## Module 1 — Foundations of AI, ML, and Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e0768",
   "metadata": {},
   "source": [
    "### 1.1 Defining AI, ML, and DL — What’s the difference?\n",
    "**Introduction:**\n",
    "- **AI** is the umbrella term: systems that perform tasks requiring human-like intelligence.\n",
    "- **ML** is data-driven learning from examples rather than rules.\n",
    "- **DL** uses multi-layer neural networks to learn rich representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557e247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 1955\n"
     ]
    }
   ],
   "source": [
    "SEED = 1955\n",
    "print('Seed set to', SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab51cb",
   "metadata": {},
   "source": [
    "### 1.2 Categories of Machine Learning — Matching problems to paradigms\n",
    "**Introduction:**\n",
    "- **Supervised** (features→label): regression, classification.\n",
    "- **Unsupervised** (no labels): clustering, dimensionality reduction, anomalies.\n",
    "- **Reinforcement Learning** (rewards): policies learned by trial-and-error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0440042",
   "metadata": {},
   "source": [
    "### 1.3 The ML Pipeline — End-to-end mindset\n",
    "**Introduction:** problem → data → features → model → evaluation → deployment → monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc01649",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.13 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# A minimal scaffold for an end-to-end pipeline on a tabular dataset (no plots)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df = pd.read_csv('../data/titanic_synth.csv')\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "cat = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat)\n",
    "], remainder='passthrough')\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('pre', pre),\n",
    "    ('model', DecisionTreeClassifier(max_depth=4, random_state=1955))\n",
    "])\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1955)\n",
    "clf.fit(Xtr, ytr)\n",
    "pred = clf.predict(Xte)\n",
    "acc = accuracy_score(yte, pred)\n",
    "print('Accuracy:', acc)\n",
    "print(classification_report(yte, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fecefe5",
   "metadata": {},
   "source": [
    "This short program builds a classification pipeline to predict whether a passenger survived (1) or not (0) using a synthetic Titanic dataset.\n",
    "It demonstrates the entire supervised ML workflow: load data → preprocess → split → train → predict → evaluate.\n",
    "\n",
    "**Code Explanation**\n",
    "\n",
    "First, we need to import the necessary libraries. Here's a brief explanation of each library:\n",
    "\n",
    "- `pandas`: Used for data manipulation and analysis.\n",
    "- `scikit-learn`: Used for machine learning algorithms and tools.\n",
    "    - `train_test_split`: divides the data into training and test sets.\n",
    "    - `OneHotEncoder`: converts categorical features into numeric binary columns. Example: sex = {male, female} becomes sex_male, sex_female.\n",
    "    - `ColumnTransformer`: applies different preprocessing to different column types. Here it ensures only categorical columns are encoded, while numeric ones pass through unchanged.\n",
    "    - `Pipeline`: links preprocessing and modeling steps together. This guarantees the same transformations are applied during both training and prediction.\n",
    "    - `DecisionTreeClassifier`: Imports the Decision Tree model — a rule-based algorithm that splits data by feature thresholds.\n",
    "    - `accuracy_score`, `classification_report`: Imports tools to evaluate model performance. Accuracy: overall correctness. Classification report: includes precision, recall, F1-score, and support.\n",
    "\n",
    "Next, we'll **load the dataset** from a CSV file into a pandas DataFrame.The line: \n",
    "\n",
    "`df = pd.read_csv('../data/titanic_synth.csv')`\n",
    "\n",
    "Loads the dataset into a Pandas DataFrame. Each row represents a passenger; each column is a feature. \n",
    "\n",
    "The next step in the pipeline is to **define the problem**. Here we need to define our variables with:\n",
    "\n",
    "`X = df.drop('Survived', axis=1)`\n",
    "\n",
    "`y = df['Survived']`\n",
    "\n",
    "That code defines the data into: X: features (inputs),  and y: target (the label we want to predict).\n",
    "\n",
    "Next, we need to **preprocess** the data. \n",
    "First, we need to find which columns are categorical (string type) and stores them in a list for encoding. We accomplish that with \n",
    "\n",
    "`cat = X.select_dtypes(include=['object']).columns.tolist()`\n",
    "\n",
    "Next, we define the preprocessing step:\n",
    "\n",
    "`pre = ColumnTransformer([`\n",
    "    `('cat', OneHotEncoder(handle_unknown='ignore'), cat)`\n",
    "`], remainder='passthrough')`\n",
    "\n",
    "- `'cat'`: label for this transformation.\n",
    "\n",
    "- `OneHotEncoder(handle_unknown='ignore')`: converts categories to 0/1 columns, ignores unseen ones at inference.\n",
    "\n",
    "- `cat`: list of categorical columns.\n",
    "\n",
    "- `remainder='passthrough'`: keeps numeric features as they are.\n",
    "\n",
    "Next, we need to combine the entire workflow into one Pipeline. Remeber the Pipeline will link the preprocessing to the modeling steps\n",
    "\n",
    "`clf = Pipeline([`\n",
    "   `('pre', pre),`\n",
    "   `('model', DecisionTreeClassifier(max_depth=4, random_state=1955))`\n",
    "`])`\n",
    "\n",
    "The code above combines the entire workflow into one Pipeline:\n",
    "\n",
    "`'pre'`: preprocessing step.\n",
    "\n",
    "`'model'`: classifier (Decision Tree).\n",
    "\n",
    "`max_depth=4` keeps the tree from growing too deep (avoids overfitting).\n",
    "\n",
    "`random_state=1955` ensures reproducibility.\n",
    "\n",
    "\n",
    "The next step is to **Split** the data for trainning and for testing.\n",
    "\n",
    "`Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1955)`\n",
    "\n",
    "The code Splits the data into 80% of the data for trainning, and 20% for testing.\n",
    "\n",
    "`stratify=y` keeps class ratios consistent (same percentage of survivors).\n",
    "\n",
    "`random_state=1955` ensures everyone in class gets the same split.\n",
    "\n",
    "\n",
    "Once we have split the data, it's finally time to **Train** the model. We use:\n",
    "\n",
    "`clf.fit(Xtr, ytr)`\n",
    "\n",
    "The code automatically preprocesses the training data using OneHotEncoder and fits a decision tree model on the transformed data.\n",
    "\n",
    "Next, we can start making **prediction** with our model. \n",
    "`pred = clf.predict(Xte)` Applies the same preprocessing to the test data and generates predictions.\n",
    "\n",
    "\n",
    "Finally, we need to evaluate the model. The follwoing code will calculate and print the desire metrics:\n",
    "\n",
    "`acc = accuracy_score(yte, pred)`\n",
    "`print('Accuracy:', acc)`\n",
    "\n",
    "The previous code calculates and prints accuracy, the proportion of correct predictions.\n",
    "\n",
    "`print(classification_report(yte, pred))` \n",
    "\n",
    "This code prints precision, recall, F1-score, and support for each class. This gives a more complete picture than accuracy alone.\n",
    "\n",
    "\n",
    "**Summary:** The real value comes from pipeline hygiene, not just the model choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0dd045",
   "metadata": {},
   "source": [
    "#### Defining X and y for a simple dataset\n",
    "\n",
    "Example: Defining `X` and `y` (Housing Dataset)\n",
    "\n",
    "In supervised learning, we separate our dataset into:\n",
    "- **`X` (features)** → input variables used for prediction  \n",
    "- **`y` (target)** → the outcome we want the model to learn\n",
    "\n",
    "Once these are defined, we split the data into **training** and **testing** sets to fairly evaluate performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9cc91b",
   "metadata": {},
   "source": [
    "##### Step 1: Import and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a simple housing dataset\n",
    "df = pd.read_csv('../data/housing_synth.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741af9e0",
   "metadata": {},
   "source": [
    "After running the code, you will see part of the dataset. The sample columns may appear as follows:\n",
    "\n",
    "`['sqft', 'bedrooms', 'bathrooms', 'age_years', 'lot_size', 'dist_to_center_km', 'price']`\n",
    "\n",
    "To define our target variable—the outcome we want the model to learn—we must identify which feature to focus on. In this case, our target variable is `price`. Therefore, we need to exclude `price` from the list of features we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a3c2e",
   "metadata": {},
   "source": [
    "##### Step 2: Define Features (X) and Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3fd7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate inputs (X) and output (y)\n",
    "X = df.drop('price', axis=1)   # all columns except 'price'\n",
    "y = df['price']                # target column\n",
    "\n",
    "# Check the dimensions\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fc020f",
   "metadata": {},
   "source": [
    "- X includes variables like square footage, number of bedrooms, and distance to city center.\n",
    "\n",
    "- y contains the house price we want to predict.\n",
    "\n",
    "Keeping them separate ensures the model only learns relationships from the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e0bc1",
   "metadata": {},
   "source": [
    "### 1.4 Data Preprocessing — Imputation & scaling\n",
    "**Introduction:** Raw data is messy. Impute, encode, and scale before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3cc22",
   "metadata": {},
   "source": [
    "#### Data Cleaning Demo — Titanic Example\n",
    "\n",
    "In this example, we’ll clean a small Titanic-style dataset that has:\n",
    "- Missing **Age** values  \n",
    "- A categorical **Sex** column that needs encoding  \n",
    "- Inconsistent **Fare** entries (negative / missing)  \n",
    "- **Outlier Fares** (unrealistically high values)\n",
    "\n",
    "---\n",
    "\n",
    "##### Step 1: Import Libraries and Create the Dataset\n",
    "\n",
    "The following code creates a very simple dataset with passanger from the Titanic. The data set includes `Name`, `Age`, `Sex` and `Fare` price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63680f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    Name   Age     Sex    Fare\n",
      "0   John  22.0    male    7.25\n",
      "1   Mary   NaN  female   71.83\n",
      "2   Alex  35.0    male   -5.00\n",
      "3  Linda  58.0  female  512.33\n",
      "4  James   NaN    male    8.05\n",
      "5   Anna  29.0  female     NaN\n",
      "6    Tom  44.0    male  300.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample Titanic-like DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Name': ['John', 'Mary', 'Alex', 'Linda', 'James', 'Anna', 'Tom'],\n",
    "    'Age':  [22, np.nan, 35, 58, np.nan, 29, 44],\n",
    "    'Sex':  ['male', 'female', 'male', 'female', 'male', 'female', 'male'],\n",
    "    # includes negatives, missing, and very large values to simulate real-world data struggles\n",
    "    'Fare': [7.25, 71.83, -5.00, 512.33, 8.05, np.nan, 300.00]\n",
    "})\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148f00ae",
   "metadata": {},
   "source": [
    "##### Step 2: Identify Missing Values\n",
    "This helps you see which columns need cleaning (here: Age, Fare)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      "Name    0\n",
      "Age     2\n",
      "Sex     0\n",
      "Fare    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values per column:\")\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1cc694",
   "metadata": {},
   "source": [
    "##### Step 3: Fix Missing Ages\n",
    "This block handles **missing values** in the `Age` column using the `SimpleImputer` class from scikit-learn. **Note:** The following example uses the `mean` as the startegy. If you would like to see the other strategies change `mean` in the code below to one of the other strategies. Also, don't forget to re run te previous to cells to make sure the dataset resets to its original state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5157179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean age is: 37.6\n",
      "\n",
      "Data after imputing missing ages:\n",
      "    Name   Age     Sex    Fare\n",
      "0   John  22.0    male    7.25\n",
      "1   Mary  37.6  female   71.83\n",
      "2   Alex  35.0    male   -5.00\n",
      "3  Linda  58.0  female  512.33\n",
      "4  James  37.6    male    8.05\n",
      "5   Anna  29.0  female     NaN\n",
      "6    Tom  44.0    male  300.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Replace missing ages with the mean age\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data['Age'] = imputer.fit_transform(data[['Age']])\n",
    "\n",
    "age_mean = data['Age'].mean()\n",
    "print(f\"The mean age is: {age_mean}\")\n",
    "print(\"\\nData after imputing missing ages:\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17740d4d",
   "metadata": {},
   "source": [
    "##### Step-by-Step Breakdown\n",
    "\n",
    "**`from sklearn.impute import SimpleImputer`**  \n",
    "Imports the `SimpleImputer` class, which is used to fill in missing values (NaN) in a dataset.\n",
    "\n",
    "---\n",
    "\n",
    "**`imputer = SimpleImputer(strategy='mean')`**  \n",
    "Creates an instance of the imputer and sets the strategy to `'mean'`.  \n",
    "- `'mean'`: replaces missing values with the **average** of the non-missing values in that column.  \n",
    "- Other options include:\n",
    "  - `'median'` → replaces with the median value  \n",
    "  - `'most_frequent'` → replaces with the mode (most common value)  \n",
    "  - `'constant'` → replaces with a fixed value you define using `fill_value`\n",
    "\n",
    "---\n",
    "\n",
    "**`data['Age'] = imputer.fit_transform(data[['Age']])`**  \n",
    "- `fit_transform()` does two things:\n",
    "  1. **`fit()`** → calculates the mean of the existing (non-NaN) `Age` values.  \n",
    "  2. **`transform()`** → replaces all missing `Age` values with that mean.  \n",
    "- The result is a **NumPy array**, so we assign it back to the DataFrame column `data['Age']`.\n",
    "\n",
    "> Note: `[['Age']]` (double brackets) keeps the column as a 2D array, which `SimpleImputer` expects as input.\n",
    "\n",
    "---\n",
    "\n",
    "**`print(\"\\nData after imputing missing ages:\")`**  \n",
    "**`print(data)`**  \n",
    "Prints the updated DataFrame so you can confirm that all missing `Age` values have been replaced.\n",
    "\n",
    "---\n",
    "\n",
    "##### Summary\n",
    "- Missing ages are now filled with the column’s mean value.  \n",
    "- This avoids data loss while maintaining the central tendency of the data.  \n",
    "- Imputation is an essential preprocessing step before model training, ensuring algorithms receive complete, numeric data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe72855",
   "metadata": {},
   "source": [
    "##### Step 4: Fix Inconsistent Fares (negative / missing)\n",
    "Next we need to clean inconsistent or missing fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d34552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after fixing inconsistent fares (negatives/missing → median):\n",
      "    Name   Age     Sex    Fare\n",
      "0   John  22.0    male    7.25\n",
      "1   Mary  37.6  female   71.83\n",
      "2   Alex  35.0    male   71.83\n",
      "3  Linda  58.0  female  512.33\n",
      "4  James  37.6    male    8.05\n",
      "5   Anna  29.0  female   71.83\n",
      "6    Tom  44.0    male  300.00\n"
     ]
    }
   ],
   "source": [
    "# Replace negative or missing fares with median of valid positive fares\n",
    "median_fare = data.loc[data['Fare'] > 0, 'Fare'].median()\n",
    "data['Fare'] = data['Fare'].apply(lambda x: median_fare if (pd.isna(x) or x <= 0) else x)\n",
    "\n",
    "print(\"\\nData after fixing inconsistent fares (negatives/missing → median):\")\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb4f71",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "median_fare = data.loc[data['Fare'] > 0, 'Fare'].median()\n",
    "```\n",
    "\n",
    "This line finds the **median (middle)** fare from all valid, positive fare values in the dataset.\n",
    "\n",
    "| Expression              | Meaning                                                                         |\n",
    "| ----------------------- | ------------------------------------------------------------------------------- |\n",
    "| `data['Fare'] > 0`      | Creates a Boolean mask to select only rows where the fare is greater than zero. |\n",
    "| `data.loc[..., 'Fare']` | Uses `.loc[]` to access the `Fare` column only for those valid rows.            |\n",
    "| `.median()`             | Calculates the median of those selected fares.                                  |\n",
    "\n",
    "The result is stored in the variable **`median_fare`**, which serves as a **reference value** to replace missing or invalid fares later.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why median instead of mean\n",
    "\n",
    "The **median** is less sensitive to extreme outliers (e.g., very high first-class fares), making it a more **robust “typical” value** than the mean when dealing with skewed data.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "data['Fare'] = data['Fare'].apply(lambda x: median_fare if (pd.isna(x) or x <= 0) else x)\n",
    "```\n",
    "\n",
    "This line **cleans the `Fare` column** by replacing invalid or missing values with the median fare computed above.\n",
    "\n",
    "---\n",
    "\n",
    "* `pd.isna(x)` → checks if the fare value is missing (`NaN`).\n",
    "* `x <= 0` → identifies fares that are negative or zero (invalid).\n",
    "* `lambda x: ...` → defines a short, anonymous function that applies this rule to each value in the column.\n",
    "* If either condition is true (missing or invalid), it replaces the fare with **`median_fare`**.\n",
    "* Otherwise, it keeps the original value (`else x`).\n",
    "\n",
    "---\n",
    "\n",
    "**Result:**\n",
    "All missing or invalid fare values are replaced with a reliable, central value — the median.\n",
    "This ensures the dataset remains consistent, realistic, and ready for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd53c27",
   "metadata": {},
   "source": [
    "##### Step 5: Handle Fare Outliers (IQR method — robust)\n",
    "Next, we need to handle outlier in the data. We are using the IQR (interquartile range) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR-based capping (winsorization): cap extreme high fares at the upper whisker\n",
    "q1, q3 = data['Fare'].quantile([0.25, 0.75])\n",
    "iqr = q3 - q1\n",
    "upper_whisker = q3 + 1.5 * iqr\n",
    "\n",
    "# Keep a copy to show before/after if desired\n",
    "fare_before = data['Fare'].copy()\n",
    "data['Fare'] = np.where(data['Fare'] > upper_whisker, upper_whisker, data['Fare'])\n",
    "\n",
    "print(\"\\nIQR capping applied:\")\n",
    "print(f\"Q1={q1:.2f}, Q3={q3:.2f}, IQR={iqr:.2f}, Upper whisker={upper_whisker:.2f}\")\n",
    "print(\"\\nFares (before → after) for affected rows:\")\n",
    "changed = pd.DataFrame({'before': fare_before, 'after': data['Fare']})\n",
    "print(changed[changed['before'] != changed['after']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4097db4a",
   "metadata": {},
   "source": [
    "##### Code explanation\n",
    "\n",
    "**`q1, q3 = data['Fare'].quantile([0.25, 0.75])`**\n",
    "\n",
    "* Calculates the **first quartile (Q1)** and **third quartile (Q3)** of the `Fare` column.\n",
    "* Q1 = value at the 25th percentile\n",
    "* Q3 = value at the 75th percentile\n",
    "\n",
    "These values help define the **interquartile range (IQR)** — the middle 50% of the data.\n",
    "\n",
    "\n",
    "\n",
    "**`iqr = q3 - q1`**\n",
    "\n",
    "* Computes the **Interquartile Range (IQR)**.\n",
    "* The IQR measures the spread of the middle 50% of the data, helping to identify unusually high or low values (outliers).\n",
    "\n",
    "\n",
    "\n",
    "**`upper_whisker = q3 + 1.5 * iqr`**\n",
    "\n",
    "* Calculates the **upper limit** for normal fare values, often called the *upper whisker* in boxplots.\n",
    "* Any fare **greater than this value** is considered an **outlier**.\n",
    "* The multiplier `1.5` is a standard rule-of-thumb for detecting moderate outliers.\n",
    "\n",
    "\n",
    "\n",
    "**`fare_before = data['Fare'].copy()`**\n",
    "\n",
    "* Creates a copy of the `Fare` column before modifications.\n",
    "* This lets you compare original and capped values later — great for demonstrations and audits.\n",
    "\n",
    "\n",
    "\n",
    "**`data['Fare'] = np.where(data['Fare'] > upper_whisker, upper_whisker, data['Fare'])`**\n",
    "\n",
    "* Applies **IQR-based capping** (also known as *winsorization*).\n",
    "* `np.where()` checks each value:\n",
    "\n",
    "  * If `Fare` > `upper_whisker`, it replaces it with `upper_whisker`.\n",
    "  * Otherwise, it leaves the value unchanged.\n",
    "* This ensures that extreme outliers don’t skew your model or distort visualizations.\n",
    "\n",
    "\n",
    "\n",
    "**`print(\"\\nIQR capping applied:\")`**\n",
    "**`print(f\"Q1={q1:.2f}, Q3={q3:.2f}, IQR={iqr:.2f}, Upper whisker={upper_whisker:.2f}\")`**\n",
    "\n",
    "* Displays the calculated thresholds for reference (rounded to two decimal places).\n",
    "\n",
    "\n",
    "\n",
    "**`changed = pd.DataFrame({'before': fare_before, 'after': data['Fare']})`**\n",
    "**`print(changed[changed['before'] != changed['after']])`**\n",
    "\n",
    "* Builds a comparison table showing which fares were capped (changed).\n",
    "* Filters only rows where the `before` and `after` values differ.\n",
    "\n",
    "---\n",
    "\n",
    "* **IQR capping (winsorization)** replaces extreme outliers with the highest “reasonable” value, preserving data shape while preventing outliers from dominating your model.\n",
    "* This is safer than simply dropping outliers because it keeps all records but reduces distortion.\n",
    "* After this step, the `Fare` column is clean, consistent, and ready for modeling or visualization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306b867",
   "metadata": {},
   "source": [
    "##### Step 6: Encode Categorical Variable (Sex)\n",
    "Encoding categorical data into a numerical format is essential for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encode 'Sex' (drop_first=True to avoid redundancy)\n",
    "data_encoded = pd.get_dummies(data, columns=['Sex'], drop_first=True)\n",
    "\n",
    "print(\"\\nData after encoding 'Sex':\")\n",
    "print(data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b3d10c",
   "metadata": {},
   "source": [
    "##### Code Explanation\n",
    "\n",
    "**`pd.get_dummies(data, columns=['Sex'], drop_first=True)`**\n",
    "\n",
    "* This line uses **pandas’ `get_dummies()`** function to convert the categorical column **`Sex`** into **numeric (binary)** columns — a process called **One-Hot Encoding**.\n",
    "* Machine learning models typically require numerical input, so categorical variables (like “male” or “female”) must be encoded as numbers.\n",
    "\n",
    "\n",
    "| Parameter         | Meaning                                                                                                                                            |\n",
    "| ----------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `data`            | The original DataFrame.                                                                                                                            |\n",
    "| `columns=['Sex']` | Specifies which column(s) to encode. In this case, we’re encoding only the `Sex` column.                                                           |\n",
    "| `drop_first=True` | Drops the **first category** to avoid the “dummy variable trap.” This prevents multicollinearity (redundant information) when using linear models. |\n",
    "\n",
    "\n",
    "What is happening here?\n",
    "* The `Sex` column likely contains two unique values: **`male`** and **`female`**.\n",
    "* One-hot encoding creates a new binary column for each category:\n",
    "\n",
    "  * `Sex_male` = 1 if the passenger is male, 0 otherwise.\n",
    "  * (The female category is **implied** when `Sex_male = 0`, because `drop_first=True` removes the redundant column.)\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "print(\"\\nData after encoding 'Sex':\")\n",
    "print(data_encoded)\n",
    "```\n",
    "\n",
    "* Prints the cleaned and encoded DataFrame to confirm that:\n",
    "\n",
    "  * The original `Sex` column was replaced with a numeric column (`Sex_male`).\n",
    "  * The dataset is now ready for modeling — since all features are numeric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2b522",
   "metadata": {},
   "source": [
    "##### Step 7: Final Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93864340",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCleaned Data Overview:\")\n",
    "print(data_encoded.info())\n",
    "print(\"\\nCleaned Data Preview:\")\n",
    "print(data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea121c",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "In this demo, we cleaned our data and prepared it for analysis. This is what we did:\n",
    "\n",
    "* Missing **Age** → imputed (mean).\n",
    "* Invalid **Fare** (negative/missing) → median.\n",
    "* **Outliers** in **Fare** → capped (IQR or business rule).\n",
    "* **Sex** encoded → numeric features for modeling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021e125",
   "metadata": {},
   "source": [
    "### 1.5 Feature Engineering — Improve signal-to-noise\n",
    "**Introduction:** Domain knowledge → better features → simpler models, better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3ae50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Titanic Data:\n",
      "      Name  Age    Fare\n",
      "0     John    8    7.25\n",
      "1     Mary   17   10.50\n",
      "2      Sam   26   35.00\n",
      "3     Lucy   38   71.83\n",
      "4      Tom   52    8.05\n",
      "5     Anna   72  512.33\n",
      "6    James   15    9.25\n",
      "7     Ella   28   27.50\n",
      "8     Mike   61   82.10\n",
      "9   Sophie   45   30.00\n",
      "10  George   33   45.80\n",
      "11    Lily    5   12.50\n",
      "\n",
      "After Binning Ages into Categories:\n",
      "      Name  Age Age_Group\n",
      "0     John    8      0-10\n",
      "1     Mary   17     11-25\n",
      "2      Sam   26     26-40\n",
      "3     Lucy   38     26-40\n",
      "4      Tom   52     41-60\n",
      "5     Anna   72       60+\n",
      "6    James   15     11-25\n",
      "7     Ella   28     26-40\n",
      "8     Mike   61       60+\n",
      "9   Sophie   45     41-60\n",
      "10  George   33     26-40\n",
      "11    Lily    5      0-10\n",
      "\n",
      "Passenger count by Age Group:\n",
      "Age_Group\n",
      "0-10     2\n",
      "11-25    2\n",
      "26-40    4\n",
      "41-60    2\n",
      "60+      2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Create an expanded Titanic-like dataset ---\n",
    "titanic = pd.DataFrame({\n",
    "    'Name': [\n",
    "        'John', 'Mary', 'Sam', 'Lucy', 'Tom', 'Anna',\n",
    "        'James', 'Ella', 'Mike', 'Sophie', 'George', 'Lily'\n",
    "    ],\n",
    "    'Age': [8, 17, 26, 38, 52, 72, 15, 28, 61, 45, 33, 5],\n",
    "    'Fare': [7.25, 10.50, 35.00, 71.83, 8.05, 512.33, 9.25, 27.50, 82.10, 30.00, 45.80, 12.50]\n",
    "})\n",
    "\n",
    "print(\"Original Titanic Data:\")\n",
    "print(titanic)\n",
    "\n",
    "# --- Step 2: Define bins and apply pd.cut() ---\n",
    "bins = [0, 10, 25, 40, 60, 200]\n",
    "labels = ['0-10', '11-25', '26-40', '41-60', '60+']\n",
    "\n",
    "titanic['Age_Group'] = pd.cut(titanic['Age'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "print(\"\\nAfter Binning Ages into Categories:\")\n",
    "print(titanic[['Name', 'Age', 'Age_Group']])\n",
    "\n",
    "# --- Step 3 (optional): Count how many passengers per group ---\n",
    "print(\"\\nPassenger count by Age Group:\")\n",
    "print(titanic['Age_Group'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041619cc",
   "metadata": {},
   "source": [
    "##### Binning (Grouping) Ages with `pd.cut()`\n",
    "\n",
    "This example shows how to divide a continuous numeric column (`Age`) into **discrete ranges** (bins) using `pandas.cut()`.\n",
    "\n",
    "---\n",
    "\n",
    "##### Step 1: Create the dataset\n",
    "We start with 12 Titanic passengers and their ages and fares.\n",
    "\n",
    "| Name | Age | Fare |\n",
    "|------|-----|------|\n",
    "| John | 8 | 7.25 |\n",
    "| Mary | 17 | 10.50 |\n",
    "| Sam | 26 | 35.00 |\n",
    "| Lucy | 38 | 71.83 |\n",
    "| Tom | 52 | 8.05 |\n",
    "| Anna | 72 | 512.33 |\n",
    "| James | 15 | 9.25 |\n",
    "| Ella | 28 | 27.50 |\n",
    "| Mike | 61 | 82.10 |\n",
    "| Sophie | 45 | 30.00 |\n",
    "| George | 33 | 45.80 |\n",
    "| Lily | 5 | 12.50 |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Define bins and labels\n",
    "```python\n",
    "bins = [0, 10, 25, 40, 60, 200]\n",
    "labels = ['0-10', '11-25', '26-40', '41-60', '60+']\n",
    "```\n",
    "* The **bins** define the numeric breakpoints for each age range.\n",
    "* The **labels** are human-readable categories for each range.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Apply `pd.cut()`\n",
    "\n",
    "```python\n",
    "titanic['Age_Group'] = pd.cut(\n",
    "    titanic['Age'], \n",
    "    bins=bins, \n",
    "    labels=labels, \n",
    "    include_lowest=True\n",
    ")\n",
    "```\n",
    "\n",
    "This function divides each `Age` into one of the predefined ranges.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Count the passengers in each group\n",
    "\n",
    "```python\n",
    "titanic['Age_Group'].value_counts().sort_index()\n",
    "```\n",
    "\n",
    "This shows how many passengers fall into each age category.\n",
    "\n",
    "| Age_Group | Count |\n",
    "| --------- | ----- |\n",
    "| 0-10      | 2     |\n",
    "| 11-25     | 2     |\n",
    "| 26-40     | 4     |\n",
    "| 41-60     | 2     |\n",
    "| 60+       | 3     |\n",
    "\n",
    "---\n",
    "\n",
    "##### Why Use Binning?\n",
    "\n",
    "* Makes numeric data **more interpretable**.\n",
    "* Helps visualize and analyze trends across age groups.\n",
    "* Useful when building features that reflect **categories** (like “child,” “young adult,” “senior”).\n",
    "* Simplifies model input when exact numeric precision isn’t critical.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "`pd.cut()` is an elegant way to group continuous values into labeled ranges — a foundational skill for **feature engineering** and **data preparation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4241a",
   "metadata": {},
   "source": [
    "### 1.6 Evaluation Metrics — Picking the right yardstick\n",
    "\n",
    "- Classification → accuracy, precision, recall, F1. \n",
    "\n",
    "- Regression → RMSE, MAE, R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63afa5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Create a small Titanic-like dataset\n",
    "data = pd.DataFrame({\n",
    "    'Sex_male': [1, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
    "    'Age': [22, 38, 26, 35, 28, 19, 42, 50, 30, 45],\n",
    "    'Survived': [0, 1, 1, 0, 1, 1, 0, 0, 1, 0]\n",
    "})\n",
    "\n",
    "X = data[['Sex_male', 'Age']]\n",
    "y = data['Survived']\n",
    "\n",
    "# Step 3: Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1955, stratify=y)\n",
    "\n",
    "# Step 4: Train a simple Decision Tree\n",
    "model = DecisionTreeClassifier(max_depth=3, random_state=1955)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Step 7: Display Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Did Not Survive', 'Survived'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix — Titanic Mini Example')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Accuracy : {acc:.2f}\")\n",
    "print(f\"Precision: {prec:.2f}\")\n",
    "print(f\"Recall   : {rec:.2f}\")\n",
    "print(f\"F1 Score : {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6dc50",
   "metadata": {},
   "source": [
    "##### Understanding the Confusion Matrix: Accuracy vs Precision vs Recall\n",
    "\n",
    "This example trains a **small Decision Tree** to predict passenger survival based on age and gender.  \n",
    "The **confusion matrix** below summarizes how many predictions were correct vs. incorrect.\n",
    "\n",
    "##### What is a Confusion Matrix?\n",
    "\n",
    "| **Actual / Predicted** | **Predicted: No (0)** | **Predicted: Yes (1)** |\n",
    "|-------------------------|-----------------------|------------------------|\n",
    "| **Actual: No (0)**      | True Negatives (TN)   | False Positives (FP)   |\n",
    "| **Actual: Yes (1)**     | False Negatives (FN)  | True Positives (TP)    |\n",
    "\n",
    "It tells us *how the model’s predictions are distributed* — not just how often it’s right.\n",
    "\n",
    "\n",
    "#### Metrics Derived from the Matrix\n",
    "\n",
    "- **Accuracy** = (TP + TN) / (TP + TN + FP + FN)  \n",
    "  - The fraction of total predictions that were correct.  \n",
    "  - Example: 8 out of 10 passengers correctly classified → 0.80 (80%).\n",
    "\n",
    "- **Precision** = TP / (TP + FP)  \n",
    "  - Of all passengers predicted as “Survived,” how many actually survived?  \n",
    "  - Measures *how reliable a positive prediction is.*\n",
    "\n",
    "- **Recall** = TP / (TP + FN)  \n",
    "  - Of all passengers who actually survived, how many did the model correctly identify?  \n",
    "  - Measures *how complete the positive predictions are.*\n",
    "\n",
    "- **F1 Score** = 2 × (Precision × Recall) / (Precision + Recall)  \n",
    "  - A balanced measure when you care about both precision and recall.\n",
    "\n",
    "---\n",
    "\n",
    "##### Example Output (Typical)\n",
    "- Accuracy : 0.80\n",
    "- Precision: 0.75\n",
    "- Recall : 0.60\n",
    "- F1 Score : 0.67\n",
    "\n",
    "\n",
    "##### Interpreting These Results\n",
    "- **Accuracy (80%)** looks good, but the confusion matrix might show that the model **missed several survivors (false negatives)**.  \n",
    "- **Precision (75%)** means that when the model predicts “Survived,” it’s right 75% of the time.  \n",
    "- **Recall (60%)** shows it found only 60% of all actual survivors — it’s cautious but misses some positives.\n",
    "\n",
    "\n",
    "##### Takeaway\n",
    "- **Accuracy** can be misleading on **imbalanced datasets** (e.g., many more deaths than survivors).  \n",
    "- **Precision** focuses on *quality* of positive predictions.  \n",
    "- **Recall** focuses on *quantity* of true positives found.  \n",
    "- The **confusion matrix** gives you the *complete picture* of your classifier’s performance.\n",
    "\n",
    "In real projects, always look beyond accuracy — analyze the full confusion matrix to understand *how* your model is getting results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171823b6",
   "metadata": {},
   "source": [
    "### 1.7 Hands-On (Mini Project) \n",
    "See additional notebooks:\n",
    "- M1_1.7A_Titanic_Classification.ipynb\n",
    "- M1_1.7B_Housing_Regression.ipynb\n",
    "- M1_1.7C_Go_Further_Lab.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
