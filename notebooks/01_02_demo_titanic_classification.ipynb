{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e02ae9",
   "metadata": {},
   "source": [
    "# Module 1 — Hands-On 1.7 (A): Titanic Classification (Survival)\n",
    "**Goal:** Apply the ML pipeline to a small dataset.\n",
    "\n",
    "**Steps:** load → minimal EDA/cleaning → encode/split → simple model → metric (accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed806e",
   "metadata": {},
   "source": [
    "## Step 1: Load a Real-World Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d4a4c",
   "metadata": {},
   "source": [
    "### When the real file isn’t available, synthetic Titanic-like data is created to complete the project\n",
    "\n",
    "If the `data/titanic_synth.csv` file isn’t found, we create a small synthetic dataset so you can run the notebook. The code uses a **fixed random seed (`SEED=1955`)** so every time you run the example, it produces the same numbers (reproducible). Also, the code, builds **240 rows** (data points), each representing one passenger.\n",
    "\n",
    "**Columns generated:**\n",
    "- **`Age`** — integer from **1 to 79** (years).\n",
    "- **`Sex`** — one of `{\"male\", \"female\"}`.\n",
    "- **`Fare`** — floating-point ticket price between **\\$5 and \\$120** (rounded to 2 decimals).\n",
    "- **`Embarked`** — port of embarkation:  \n",
    "  - `S` = **Southampton** (United Kingdom)  \n",
    "  - `C` = **Cherbourg** (France)  \n",
    "  - `Q` = **Queenstown** (now Cobh, Ireland)\n",
    "- **`Survived`** — outcome flag: `1` = survived, `0` = did not survive.\n",
    "\n",
    "This synthetic dataset mimics the shape of the real Titanic data so that you can practice the **full ML pipeline** (load → clean → encode → model → evaluate) without relying on external downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0203334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>34.6</td>\n",
       "      <td>7.22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>35.4</td>\n",
       "      <td>5.84</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>36.2</td>\n",
       "      <td>6.73</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.8</td>\n",
       "      <td>6.64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.1</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived   age  fare  sibsp  parch  pclass     sex embarked\n",
       "0         1  34.6  7.22      0      2       2  female        C\n",
       "1         0  35.4  5.84      1      2       2  female        S\n",
       "2         0  36.2  6.73      1      1       1  female        S\n",
       "3         1  35.8  6.64      1      1       3  female        C\n",
       "4         1  20.1  7.63      0      2       1  female        S"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "SEED = 1955\n",
    "try:\n",
    "    df = pd.read_csv('../data/titanic_synth.csv')\n",
    "except FileNotFoundError:\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    df = pd.DataFrame({\n",
    "        'Age': rng.integers(1, 80, 240),\n",
    "        'Sex': rng.choice(['male','female'], 240),\n",
    "        'Fare': rng.uniform(5, 120, 240).round(2),\n",
    "        'Embarked': rng.choice(['S','C','Q'], 240),\n",
    "        'Survived': rng.integers(0, 2, 240)\n",
    "    })\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06807c00",
   "metadata": {},
   "source": [
    "### What `df.head()` is telling you, and what to check for\n",
    "\n",
    "`df.head()` prints the **first few rows** (default 5). Use it to quickly check the data:\n",
    "\n",
    "- **Column names & order** — Do you see the features you expect (`Age`, `Sex`, `Fare`, `Embarked`, `Survived`)? Any typos?\n",
    "- **Value shapes** — Are `Age` and `Fare` numeric-looking? Are categorical fields like `Sex` and `Embarked` strings?\n",
    "- **Obvious anomalies** — Negative fares? Ages of 0 or 999? Blank or `NaN` cells?\n",
    "- **Consistent coding** — e.g., `male/female` vs `Male/Female` (case consistency matters before encoding).\n",
    "\n",
    "> Tip: `head()` is your **first line of defense** against bad assumptions. It doesn’t replace full validation, but it helps you spot glaring issues early.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ecd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inject a bit of realistic \"messiness\" for hands-on cleaning ---\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# ~8% missing ages, ~5% missing fares, ~4% missing embarked\n",
    "df.loc[rng.random(len(df)) < 0.08, 'Age'] = np.nan\n",
    "df.loc[rng.random(len(df)) < 0.05, 'Fare'] = np.nan\n",
    "df.loc[rng.random(len(df)) < 0.04, 'Embarked'] = None\n",
    "\n",
    "# A few outlier fares (e.g., data entry issues)\n",
    "out_idx = df.sample(3, random_state=SEED).index\n",
    "df.loc[out_idx, 'Fare'] = df.loc[out_idx, 'Fare'].fillna(0) + 500  # make them very large\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e4a5e",
   "metadata": {},
   "source": [
    "### Why we’re adding “messiness”\n",
    "\n",
    "Real-world data is rarely perfect. We deliberately introduce:\n",
    "- **Missing values** (`Age`, `Fare`, `Embarked`) to practice **imputation** and handling categorical NaNs.\n",
    "- **Outliers** in `Fare` to practice **simple anomaly handling** (e.g., capping or inspecting unusual values).\n",
    "\n",
    "> These small imperfections give us something meaningful to clean before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5976cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #shows dtypes/missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db20bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all') #summarizes ranges/frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f96e5",
   "metadata": {},
   "source": [
    "### Understanding `df.info()` and `df.describe(include='all')`\n",
    "\n",
    "These two commands are part of the **exploratory data analysis (EDA)** step. They help you **understand the structure and quality** of your dataset before you start cleaning or modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### `df.info()`\n",
    "- Displays a concise **summary of the DataFrame** — column names, data types, and counts of non-null values.\n",
    "- Use it to quickly check:\n",
    "  - Which columns are **numeric**, **categorical (object)**, or **boolean**.\n",
    "  - Whether any columns have **missing values** (the “non-null count” will be lower than total rows).\n",
    "  - The overall **memory usage** (useful for large datasets).\n",
    "\n",
    "If you see the **output:** above you will be able to spot missing values — notice that `Age`, `Fare`, and `Embarked` have fewer than 240 non-null entries.\n",
    "\n",
    "> **Why it matters:** `df.info()` helps you decide which columns might need imputation or type conversion before modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### `df.describe(include='all')`\n",
    "- Gives **summary statistics** for each column.\n",
    "- By default, `.describe()` only includes numeric columns.  \n",
    "  Adding `include='all'` makes it include **both numeric and categorical** data.\n",
    "\n",
    "**For numeric columns**, it shows:\n",
    "| Statistic | Meaning |\n",
    "|------------|----------|\n",
    "| `count` | Number of non-missing values |\n",
    "| `mean` | Average |\n",
    "| `std` | Standard deviation (spread) |\n",
    "| `min`, `25%`, `50%`, `75%`, `max` | Distribution spread (quartiles) |\n",
    "\n",
    "**For categorical columns**, it shows:\n",
    "| Statistic | Meaning |\n",
    "|------------|----------|\n",
    "| `count` | Number of non-missing values |\n",
    "| `unique` | Number of distinct categories |\n",
    "| `top` | Most frequent category |\n",
    "| `freq` | How often that top category appears |\n",
    "\n",
    "**Why it matters:** `df.describe(include='all')` gives a **first quantitative sense** of your data’s shape and scale — useful for spotting:\n",
    "- Missing or extreme values (e.g., `min=0` where that makes no sense).\n",
    "- Unbalanced categorical distributions (e.g., too few males/females).\n",
    "- Skewed numeric variables (large `max` vs `mean`).\n",
    "\n",
    "Together, these two commands form the **“X-ray”** of your dataset — they help you see the big picture before doing any transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b004d7",
   "metadata": {},
   "source": [
    "## Step 2: Minimal EDA & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c3946",
   "metadata": {},
   "source": [
    "**Why this step matters:**  \n",
    "Garbage in → garbage out. Even small cleaning choices can change your model’s conclusions.\n",
    "\n",
    "**What to try:**  \n",
    "- Identify columns with missing values using `df.isna().sum()`.  \n",
    "- Impute **one** column (e.g., `Age` or `sqft`) to see impact.  \n",
    "- Consider simple outlier handling (e.g., cap extreme `Fare`).\n",
    "\n",
    "**Common pitfalls:**  \n",
    "- Dropping too many rows with `dropna()` and losing valuable data.  \n",
    "- Applying imputation differently to train vs test (always fit on train, apply to test via pipelines).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f6cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() #It counts how many missing (NaN or None) values exist in each column of the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b653368",
   "metadata": {},
   "source": [
    "This result means that:\n",
    "- 20 rows have a missing Age,\n",
    "- 6 have missing Fare,\n",
    "- 9 missing Embarked,\n",
    "- none missing Sex or Survived."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ef151",
   "metadata": {},
   "source": [
    "### Handling missing or invalid Age values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "if df['Age'].isna().any():\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df['Age'] = imputer.fit_transform(df[['Age']])\n",
    "\n",
    "# Validation\n",
    "print(\"Missing Age values after cleaning:\", df['Age'].isna().sum())\n",
    "print(df['Age'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee6dd87",
   "metadata": {},
   "source": [
    "#### What this imputation code does (and why)\n",
    "\n",
    "Step-by-Step:\n",
    "\n",
    "1. Check for missingness: df['Age'].isna().any() — only run imputation if needed.\n",
    "2. Choose a strategy: strategy='mean' replaces missing values with the column’s average.\n",
    "3. Fit + transform: fit_transform computes the mean from non-missing rows and fills NaNs. (We pass [['Age']] as a 2D array to match scikit-learn’s expected shape.)\n",
    "\n",
    "#### When to use which strategy?\n",
    "\n",
    "- Mean: fast, OK if distribution is fairly symmetric.\n",
    "- Median: more robust to outliers (often better for skewed numeric data like prices).\n",
    "- Most frequent: for categorical text columns (e.g., fill missing Embarked with the mode).\n",
    "- Constant: when you need a placeholder value (e.g., 0 or \"Unknown\").\n",
    "\n",
    "**Always document your choice and consider how it might bias the model. Imputation is a modeling decision.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d3fa7",
   "metadata": {},
   "source": [
    "### Handling missing or invalid Fare values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f09a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Handle missing or invalid Fare values ---\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Replace negative or zero fares with NaN (invalid values)\n",
    "df.loc[df['Fare'] <= 0, 'Fare'] = np.nan\n",
    "\n",
    "# Cap extreme outlier fares using IQR (winsorization)\n",
    "q1, q3 = df['Fare'].quantile([0.25, 0.75])\n",
    "iqr = q3 - q1\n",
    "upper_whisker = q3 + 1.5 * iqr\n",
    "df['Fare'] = np.where(df['Fare'] > upper_whisker, upper_whisker, df['Fare'])\n",
    "\n",
    "# Impute remaining missing values with the median Fare\n",
    "fare_imputer = SimpleImputer(strategy='median')\n",
    "df['Fare'] = fare_imputer.fit_transform(df[['Fare']])\n",
    "\n",
    "# Quick validation\n",
    "print(\"Missing Fares after cleaning:\", df['Fare'].isna().sum())\n",
    "print(df['Fare'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8951bd",
   "metadata": {},
   "source": [
    "### Handling missing or invalid Embarked values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Handle missing values in the Embarked column ---\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Check how many are missing\n",
    "print(\"Missing Embarked values before cleaning:\", df['Embarked'].isna().sum())\n",
    "print(df['Embarked'].value_counts())\n",
    "\n",
    "# Create an imputer that fills missing values with the most frequent value (mode)\n",
    "embarked_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Apply the imputer to the column\n",
    "#df['Embarked'] = embarked_imputer.fit_transform(df[['Embarked']]). Note this line creates an error because SimpleImputer.fit_transform returns a 2-D array \n",
    "# of shape (n_rows, 1), but assigning to a single column like df['Embarked'] expects a 1-D Series. You just need to flatten the result.\n",
    "mode_val = df['Embarked'].mode(dropna=True).iloc[0]\n",
    "df['Embarked'] = df['Embarked'].fillna(mode_val)\n",
    "\n",
    "# Confirm that all missing values have been filled\n",
    "print(\"Missing Embarked values after cleaning:\", df['Embarked'].isna().sum())\n",
    "print(df['Embarked'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5768e9d",
   "metadata": {},
   "source": [
    "#### Fixing missing values in `Embarked`\n",
    "\n",
    "Since `Embarked` is a **categorical feature**, we handle missing values by filling them with the **most frequent category** (the mode). In this version, we use pure **pandas**, which is simpler and avoids shape issues.\n",
    "\n",
    "#### Step-by-step\n",
    "1. **Find the mode** - `mode_val = df['Embarked'].mode(dropna=True).iloc[0]` identifies the most common value (e.g., 'S' for Southampton).\n",
    "\n",
    "\n",
    "##### Step-by-step\n",
    "1. **Count missing values** – see how many entries lack a port of embarkation.\n",
    "2. **Choose a strategy** – for categorical features, `'most_frequent'` is common because:\n",
    "   - It preserves existing category proportions.\n",
    "   - It avoids introducing a fake “unknown” class unless that’s analytically useful.\n",
    "3. **Find the mode** - `mode_val = df['Embarked'].mode(dropna=True).iloc[0]` identifies the most common value (e.g., 'S' for Southampton).\n",
    "4. **Fill missing values** - `df['Embarked'] = df['Embarked'].fillna(mode_val)` every missing cell is replaced with that most frequent value.\n",
    "5. **Validate** – recheck missing counts and category frequencies.\n",
    "\n",
    "\n",
    "**Why this approach works well:**  \n",
    "Filling with the most common port (e.g., `'S'` for Southampton) is a reasonable assumption unless you have reason to believe missing values cluster differently.  \n",
    "For more advanced pipelines, you could later encode `'Missing'` as its own category to preserve that information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ac443",
   "metadata": {},
   "source": [
    "## Step 3: Encode & Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6fd4a5",
   "metadata": {},
   "source": [
    "**Why this step matters:**  \n",
    "- Models need numbers. Categorical features (e.g., `Sex`, `Embarked`) require encoding.  \n",
    "- Train/test split ensures unbiased evaluation of generalization.\n",
    "\n",
    "**Good practices:**  \n",
    "- Use a **`ColumnTransformer` + `Pipeline`** so preprocessing is learned on training data and applied consistently to test/new data.  \n",
    "- Set `random_state` for reproducibility; use `stratify=y` for classification to keep class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37afaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "], remainder='passthrough')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca85c8d",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- One-Hot encode categoricals.\n",
    "- Stratified split preserves class balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe31798",
   "metadata": {},
   "source": [
    "## Step 4: Fit a Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe36fea",
   "metadata": {},
   "source": [
    "**Why start simple?**  \n",
    "- A **Decision Tree** (classification) or **Linear Regression** (regression) is interpretable and fast.  \n",
    "- Helps build intuition before exploring more complex models (Random Forest, XGBoost, Neural Nets).\n",
    "\n",
    "**Common pitfalls:**  \n",
    "- Overfitting by using deep trees / over-parameterized models too early.  \n",
    "- Confusing training performance with generalization (we care about test/validation metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb055cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('pre', pre),\n",
    "    ('model', DecisionTreeClassifier(max_depth=4, random_state=SEED))\n",
    "])\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afac950",
   "metadata": {},
   "source": [
    "**Explanation:** Shallow Decision Tree keeps things interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79798ac",
   "metadata": {},
   "source": [
    "## Step 5: Brief Metric Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5ab0e",
   "metadata": {},
   "source": [
    "**Pick metrics that match the goal:**\n",
    "- **Classification**:  \n",
    "  - **Accuracy** for balanced classes.  \n",
    "  - **Precision/Recall/F1** when positives are rare or costs differ (e.g., missing a survivor vs false alarm).  \n",
    "- **Regression**:  \n",
    "  - **RMSE/MAE** for absolute error.  \n",
    "  - **R²** to gauge explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(f'Accuracy: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841b7b0",
   "metadata": {},
   "source": [
    "**Discussion:** When can accuracy be misleading (imbalance, unequal error costs)?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
