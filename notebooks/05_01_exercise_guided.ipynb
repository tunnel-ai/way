{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf22f75b",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/tunnel-ai/way/blob/main/notebooks/05_01_exercise_guided.ipynb\n",
    ")\n",
    "\n",
    "# Module 5 — NLP: Exercise (Guided)\n",
    "## `05_01_exercise_guided.ipynb`\n",
    "\n",
    "**Goal:** Reproduce *Act II* from `05_00_main.ipynb` (AI governance subset), then perform **one controlled perturbation** to test whether the *agenda-as-emphasis* signal is robust.\n",
    "\n",
    "### What you will do\n",
    "1. Load the cached OpenAlex sample created in `05_00_main.ipynb`\n",
    "2. Filter to **AI governance** abstracts (keyword conditioning)\n",
    "3. Build a TF–IDF representation (**baseline: unigrams + bigrams**)\n",
    "4. Compute **top characteristic terms by region**\n",
    "5. Make **one change** (perturbation): **unigrams only**\n",
    "6. Compare results and write a short reflection\n",
    "\n",
    "### What you will not do\n",
    "- You will not invent a new topic definition (that is for `05_02`)\n",
    "- You will not chase metrics (this is about interpretability and robustness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06cee6d",
   "metadata": {},
   "source": [
    "## 0) Colab-first setup\n",
    "\n",
    "This notebook is designed to run in Colab from the `tunnel-ai/way` GitHub repo.\n",
    "\n",
    "- First cell clones the repo and sets the working directory.\n",
    "- We then load the cached dataset produced by `05_00_main.ipynb`.\n",
    "\n",
    "If you have not run `05_00_main.ipynb` yet, do that first so the cache exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the course repo (Colab-friendly)\n",
    "!git clone -q https://github.com/tunnel-ai/way.git\n",
    "\n",
    "# Move into the repo\n",
    "%cd way\n",
    "\n",
    "# Basic imports\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25966afa",
   "metadata": {},
   "source": [
    "## 1) Load the cached OpenAlex sample\n",
    "\n",
    "`05_00_main.ipynb` caches a CSV at:\n",
    "\n",
    "- `assets/data/openalex_abstracts_sample.csv`\n",
    "\n",
    "We will load that file here.\n",
    "\n",
    "If the file is missing, run `05_00_main.ipynb` first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1136f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"assets/data/openalex_abstracts_sample.csv\"\n",
    "assert os.path.exists(DATA_PATH), (\n",
    "    f\"Missing cache: {DATA_PATH}\\n\"\n",
    "    \"Fix: run notebooks/05_00_main.ipynb first to generate the cached sample.\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Loaded rows:\", len(df))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d281cce",
   "metadata": {},
   "source": [
    "## 2) Minimal cleaning (same philosophy as `05_00`)\n",
    "\n",
    "Key principle:\n",
    "\n",
    "> Cleaning is an argument about what information you consider irrelevant.\n",
    "\n",
    "We keep cleaning light and transparent:\n",
    "- lowercase\n",
    "- remove URLs\n",
    "- keep letters, spaces, and hyphens\n",
    "- normalize whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pat = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "multi_space_pat = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    s = url_pat.sub(\" \", s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z\\s\\-]\", \" \", s)     # keep letters, spaces, hyphens\n",
    "    s = multi_space_pat.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Use the cached abstract column\n",
    "df[\"text\"] = df[\"abstract\"].apply(clean_text)\n",
    "\n",
    "# Quick peek\n",
    "i = 0\n",
    "print(df.loc[i, \"title\"])\n",
    "print(\"\\nRAW:\\n\", textwrap.shorten(str(df.loc[i, \"abstract\"]), width=320, placeholder=\"…\"))\n",
    "print(\"\\nCLEAN:\\n\", textwrap.shorten(str(df.loc[i, \"text\"]), width=320, placeholder=\"…\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f46a24",
   "metadata": {},
   "source": [
    "## 3) Condition on a shared topic: AI governance\n",
    "\n",
    "We define an AI governance subset using a **transparent keyword filter**.\n",
    "\n",
    "This is not perfect. It is a deliberate, inspectable construction of a corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41742b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOV_TERMS = [\n",
    "    \"governance\",\n",
    "    \"ethics\",\n",
    "    \"ethical\",\n",
    "    \"fairness\",\n",
    "    \"bias\",\n",
    "    \"accountability\",\n",
    "    \"transparency\",\n",
    "    \"privacy\",\n",
    "    \"regulation\",\n",
    "    \"regulatory\",\n",
    "    \"compliance\",\n",
    "    \"risk\",\n",
    "    \"responsible ai\",\n",
    "]\n",
    "\n",
    "pattern = \"|\".join(GOV_TERMS)\n",
    "\n",
    "df_gov = df[df[\"text\"].str.contains(pattern, regex=True)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original corpus size: {len(df):,}\")\n",
    "print(f\"AI governance subset: {len(df_gov):,}\")\n",
    "df_gov[\"region\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e62050d",
   "metadata": {},
   "source": [
    "### Quick inspection\n",
    "\n",
    "Before modeling, inspect a few examples to confirm the subset “looks like” governance.\n",
    "\n",
    "**TODO:** Read 2–3 abstracts. Do they seem on-topic?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(min(3, len(df_gov))):\n",
    "    print(\"\\n\" + \"—\"*50)\n",
    "    print(df_gov.loc[i, \"title\"])\n",
    "    print(\"Region:\", df_gov.loc[i, \"region\"])\n",
    "    print(textwrap.shorten(str(df_gov.loc[i, \"abstract\"]), width=380, placeholder=\"…\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419b88a",
   "metadata": {},
   "source": [
    "## 4) Baseline representation: TF–IDF (unigrams + bigrams)\n",
    "\n",
    "We reuse a simple TF–IDF baseline with explicit choices:\n",
    "- English stopwords\n",
    "- `min_df` / `max_df` to control vocabulary extremes\n",
    "- **(1,2)** n-grams (unigrams + bigrams)\n",
    "\n",
    "This is our *baseline* representation for the exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e0cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_base = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    ngram_range=(1, 2),\n",
    ")\n",
    "\n",
    "X_base = vectorizer_base.fit_transform(df_gov[\"text\"])\n",
    "feature_names_base = vectorizer_base.get_feature_names_out()\n",
    "\n",
    "print(\"TF–IDF baseline matrix:\", X_base.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d5479",
   "metadata": {},
   "source": [
    "## 5) Agenda-as-emphasis: characteristic terms by region (baseline)\n",
    "\n",
    "We treat “agenda” as **differences in emphasis within a shared topic**.\n",
    "\n",
    "One proxy for emphasis:\n",
    "> which terms have the highest average TF–IDF weight within a region’s documents?\n",
    "\n",
    "**TODO:** Run the cell and scan the output. What differences do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms_by_region(df_sub, X_sub, feature_names, region, top_n=12):\n",
    "    mask = (df_sub[\"region\"] == region).to_numpy()\n",
    "    if mask.sum() == 0:\n",
    "        return []\n",
    "    mean_tfidf = X_sub[mask].mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[-top_n:][::-1]\n",
    "    return [feature_names[i] for i in top_idx]\n",
    "\n",
    "REGIONS = [\"United States\", \"Europe\", \"China\", \"Other\"]\n",
    "\n",
    "baseline_terms = {}\n",
    "for r in REGIONS:\n",
    "    baseline_terms[r] = top_terms_by_region(df_gov, X_base, feature_names_base, r, top_n=12)\n",
    "    print(f\"\\nBaseline top terms — {r}\")\n",
    "    print(baseline_terms[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38abc2a",
   "metadata": {},
   "source": [
    "## 6) Controlled perturbation: unigrams only\n",
    "\n",
    "Now we make **one change**:\n",
    "\n",
    "- Baseline: `ngram_range=(1,2)`\n",
    "- Perturbation: `ngram_range=(1,1)`\n",
    "\n",
    "Everything else stays fixed.\n",
    "\n",
    "This tests whether the perceived “agenda” signal is robust to a reasonable representational change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a6c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_uni = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    ngram_range=(1, 1),\n",
    ")\n",
    "\n",
    "X_uni = vectorizer_uni.fit_transform(df_gov[\"text\"])\n",
    "feature_names_uni = vectorizer_uni.get_feature_names_out()\n",
    "\n",
    "print(\"TF–IDF unigram matrix:\", X_uni.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b2a33",
   "metadata": {},
   "source": [
    "## 7) Recompute characteristic terms by region (perturbation)\n",
    "\n",
    "**TODO:** Compare these lists to the baseline. What persisted? What changed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8baea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_terms = {}\n",
    "for r in REGIONS:\n",
    "    perturbed_terms[r] = top_terms_by_region(df_gov, X_uni, feature_names_uni, r, top_n=12)\n",
    "    print(f\"\\nUnigrams-only top terms — {r}\")\n",
    "    print(perturbed_terms[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37e03a",
   "metadata": {},
   "source": [
    "## 8) Side-by-side comparison table\n",
    "\n",
    "This table is the core artifact of the exercise.\n",
    "\n",
    "**TODO:** Read the table row by row. Identify at least:\n",
    "- 1 signal that persists across both representations\n",
    "- 1 signal that appears representation-dependent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for r in REGIONS:\n",
    "    rows.append({\n",
    "        \"Region\": r,\n",
    "        \"Baseline (1–2 grams)\": \", \".join(baseline_terms.get(r, [])),\n",
    "        \"Perturbed (unigrams)\": \", \".join(perturbed_terms.get(r, [])),\n",
    "    })\n",
    "\n",
    "comp = pd.DataFrame(rows)\n",
    "comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3fc4c6",
   "metadata": {},
   "source": [
    "## 9) Reflection (short, structured)\n",
    "\n",
    "Write short answers in markdown *in this notebook* (not in code).\n",
    "\n",
    "### Prompt\n",
    "1. In 2–3 sentences, describe the **baseline** differences in emphasis you observed across regions.\n",
    "2. In 2–3 sentences, describe how those differences **changed** under the unigram-only perturbation.\n",
    "3. In 2–3 sentences, assess whether the perceived agenda signal appears **robust, fragile, or representation-dependent**.\n",
    "\n",
    "Remember:\n",
    "- We are not claiming to discover “national agendas.”\n",
    "- We are examining how *representations* shape the perception of difference within a shared topic.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
