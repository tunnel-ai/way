{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 3 \u2014 Supervised Learning: Classification (Open Exercise)\n",
        "\n",
        "**File:** `03_02_exercise_open.ipynb`  \n",
        "**Target:** `is_fraud` (binary)  \n",
        "**Dataset:** `generate_transaction_risk_dataset(seed=1955)` (canonical)\n",
        "\n",
        "---\n",
        "\n",
        "## Open in Colab\n",
        "\n",
        "> If you are viewing this notebook on GitHub, use the badge below to open it in Colab.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](REPLACE_WITH_GITHUB_COLAB_LINK)\n",
        "\n",
        "---\n",
        "\n",
        "## Deliverable (what you will submit)\n",
        "\n",
        "By the end, your notebook must include:\n",
        "\n",
        "1. **A single final model pipeline** (preprocessing + model) trained on the canonical dataset.  \n",
        "2. A **Decision Log** documenting **two** modeling decisions you made (see menu below), each supported by evidence (metrics + at least one plot).  \n",
        "3. Evaluation on a held-out validation split using **all** of the following:\n",
        "   - Confusion matrix at a chosen threshold\n",
        "   - Precision, Recall, F1\n",
        "   - ROC-AUC\n",
        "   - PR-AUC (Average Precision)\n",
        "\n",
        "> Important: Fraud is **imbalanced**. Accuracy alone is not sufficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Colab-first setup (run this cell first)\n",
        "!git clone https://github.com/tunnel-ai/way.git\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/way/src\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score,\n",
        "    RocCurveDisplay, PrecisionRecallDisplay\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from core.generators.transaction_risk_dgp import generate_transaction_risk_dataset"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate canonical dataset (do not modify the generator)\n",
        "df = generate_transaction_risk_dataset(seed=1955)\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Define target, features, and a fixed split\n",
        "\n",
        "- Target: `is_fraud`\n",
        "- Use a **stratified** split because fraud is rare.\n",
        "- Keep the split fixed so your results are reproducible.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TARGET = \"is_fraud\"\n",
        "\n",
        "# TODO: Decide what to do with these two target columns as features.\n",
        "# - For classification (is_fraud), you MUST exclude transaction_loss_amount (it is post-event).\n",
        "DROP_ALWAYS = [TARGET, \"transaction_loss_amount\"]\n",
        "\n",
        "X = df.drop(columns=DROP_ALWAYS)\n",
        "y = df[TARGET].astype(int)\n",
        "\n",
        "# Stratified split (fixed random_state for reproducibility)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train fraud rate:\", y_train.mean())\n",
        "print(\"Val fraud rate:  \", y_val.mean())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Baseline(s)\n",
        "\n",
        "Compute at least one baseline that respects class imbalance.\n",
        "\n",
        "Suggested baselines:\n",
        "- **Always predict non-fraud** (`\u0177 = 0`)\n",
        "- (Optional) Predict fraud with the **base rate** probability\n",
        "\n",
        "Report: confusion matrix + precision/recall/F1 + ROC-AUC + PR-AUC where applicable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Baseline: always predict non-fraud\n",
        "y_pred0 = np.zeros_like(y_val)\n",
        "cm0 = confusion_matrix(y_val, y_pred0)\n",
        "\n",
        "precision0 = precision_score(y_val, y_pred0, zero_division=0)\n",
        "recall0 = recall_score(y_val, y_pred0, zero_division=0)\n",
        "f10 = f1_score(y_val, y_pred0, zero_division=0)\n",
        "\n",
        "# For ROC-AUC / PR-AUC we need probabilities; for this baseline use constant p=0\n",
        "y_prob0 = np.zeros_like(y_val, dtype=float)\n",
        "roc0 = roc_auc_score(y_val, y_prob0)\n",
        "pr0 = average_precision_score(y_val, y_prob0)\n",
        "\n",
        "print(\"Confusion matrix (always non-fraud):\\n\", cm0)\n",
        "print({\"precision\": precision0, \"recall\": recall0, \"f1\": f10, \"roc_auc\": roc0, \"pr_auc\": pr0})"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Decision menu (choose **two**)\n",
        "\n",
        "You must choose **two** decisions from this menu, implement them, and justify them with evidence.\n",
        "\n",
        "**Decision A \u2014 High-cardinality `merchant_id` handling**\n",
        "- A1) Drop `merchant_id`\n",
        "- A2) One-hot encode top-K most frequent merchants + \"other\"\n",
        "- A3) Frequency encoding (count-based, target-agnostic)\n",
        "\n",
        "**Decision B \u2014 Model family**\n",
        "- B1) Logistic Regression (interpretable baseline)\n",
        "- B2) Decision Tree (nonlinear rules)\n",
        "- B3) Random Forest / Gradient Boosting (stronger performance, less interpretable)\n",
        "\n",
        "**Decision C \u2014 Threshold rule**\n",
        "- C1) Use 0.50\n",
        "- C2) Choose threshold to maximize F1\n",
        "- C3) Choose threshold using a simple **cost tradeoff** (FP cost vs FN cost)\n",
        "\n",
        "**Decision D \u2014 Feature set**\n",
        "- D1) Transaction context only\n",
        "- D2) Context + customer/device signals\n",
        "- D3) Exclude suspicious / noisy features (state your rationale)\n",
        "\n",
        "---\n",
        "\n",
        "### Decision Log (fill this in as you work)\n",
        "\n",
        "- Decision 1: ___ (A/B/C/D + option)  \n",
        "  Rationale: ___  \n",
        "  Evidence: metrics + plot(s) ___\n",
        "\n",
        "- Decision 2: ___ (A/B/C/D + option)  \n",
        "  Rationale: ___  \n",
        "  Evidence: metrics + plot(s) ___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Build a preprocessing pipeline (avoid leakage)\n",
        "\n",
        "Rules:\n",
        "- Do **not** manually fit encoders/scalers on the full dataset.\n",
        "- Use a `Pipeline` + `ColumnTransformer` so preprocessing is fit on **train only**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Identify column types\n",
        "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
        "\n",
        "# TODO (Decision A): decide how to handle merchant_id\n",
        "# Option A1: drop merchant_id\n",
        "# Option A2: one-hot encode (will be big)\n",
        "# Option A3: frequency encoding (target-agnostic)\n",
        "#\n",
        "# NOTE: OneHotEncoder on full merchant_id may be large; a top-K approach can help.\n",
        "\n",
        "# Example: drop merchant_id (A1)\n",
        "if \"merchant_id\" in cat_cols:\n",
        "    # Comment this out if you *do* decide to encode merchant_id\n",
        "    X_train = X_train.drop(columns=[\"merchant_id\"])\n",
        "    X_val = X_val.drop(columns=[\"merchant_id\"])\n",
        "    cat_cols = [c for c in cat_cols if c != \"merchant_id\"]\n",
        "\n",
        "numeric_pipe = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipe = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_pipe, num_cols),\n",
        "        (\"cat\", categorical_pipe, cat_cols),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "preprocess"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Choose a model (Decision B)\n",
        "\n",
        "Pick **one** as your final model, but you may compare 2\u20133 quickly.\n",
        "\n",
        "Recommended starting point:\n",
        "- LogisticRegression with `class_weight=\"balanced\"` (handles imbalance)\n",
        "\n",
        "Then compare to:\n",
        "- DecisionTreeClassifier\n",
        "- RandomForestClassifier\n",
        "- HistGradientBoostingClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO (Decision B): choose your model\n",
        "# Start with logistic regression as a baseline.\n",
        "model = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    class_weight=\"balanced\",\n",
        "    n_jobs=None\n",
        ")\n",
        "\n",
        "# Example alternatives (uncomment to try):\n",
        "# model = DecisionTreeClassifier(max_depth=6, class_weight=\"balanced\", random_state=42)\n",
        "# model = RandomForestClassifier(n_estimators=300, random_state=42, class_weight=\"balanced\", n_jobs=-1)\n",
        "# model = HistGradientBoostingClassifier(max_depth=6, learning_rate=0.1, random_state=42)\n",
        "\n",
        "clf = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"model\", model)\n",
        "])\n",
        "\n",
        "clf"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predicted probabilities for the positive class\n",
        "y_prob = clf.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Default threshold (you may change this in Decision C)\n",
        "threshold = 0.50\n",
        "y_pred = (y_prob >= threshold).astype(int)\n",
        "\n",
        "# Core metrics\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "precision = precision_score(y_val, y_pred, zero_division=0)\n",
        "recall = recall_score(y_val, y_pred, zero_division=0)\n",
        "f1 = f1_score(y_val, y_pred, zero_division=0)\n",
        "roc = roc_auc_score(y_val, y_prob)\n",
        "pr = average_precision_score(y_val, y_prob)\n",
        "\n",
        "print(\"Confusion matrix @ threshold =\", threshold, \"\\n\", cm)\n",
        "print({\"precision\": precision, \"recall\": recall, \"f1\": f1, \"roc_auc\": roc, \"pr_auc\": pr})"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Diagnostics: ROC and Precision\u2013Recall curves\n",
        "\n",
        "You must include at least one diagnostic plot in your evidence.\n",
        "For imbalanced classification, the **PR curve** is often more informative than ROC.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig = plt.figure(figsize=(6, 4))\n",
        "RocCurveDisplay.from_predictions(y_val, y_prob)\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(6, 4))\n",
        "PrecisionRecallDisplay.from_predictions(y_val, y_prob)\n",
        "plt.title(\"Precision\u2013Recall Curve\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Thresholding (Decision C)\n",
        "\n",
        "Pick one approach:\n",
        "- **C1:** Use 0.50\n",
        "- **C2:** Choose threshold that maximizes F1 on validation\n",
        "- **C3:** Choose threshold using a simple cost tradeoff\n",
        "\n",
        "For C3, define:\n",
        "- cost_fp: cost of investigating a legitimate transaction\n",
        "- cost_fn: cost of missing a fraud event\n",
        "\n",
        "Then choose the threshold that minimizes expected cost.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Threshold sweep\n",
        "thresholds = np.linspace(0.01, 0.99, 99)\n",
        "\n",
        "rows = []\n",
        "for t in thresholds:\n",
        "    yp = (y_prob >= t).astype(int)\n",
        "    rows.append({\n",
        "        \"threshold\": t,\n",
        "        \"precision\": precision_score(y_val, yp, zero_division=0),\n",
        "        \"recall\": recall_score(y_val, yp, zero_division=0),\n",
        "        \"f1\": f1_score(y_val, yp, zero_division=0),\n",
        "    })\n",
        "\n",
        "thr_df = pd.DataFrame(rows)\n",
        "\n",
        "# TODO (Decision C2): pick the threshold that maximizes F1\n",
        "best_row = thr_df.loc[thr_df[\"f1\"].idxmax()]\n",
        "best_t = float(best_row[\"threshold\"])\n",
        "best_row, best_t"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# OPTIONAL (Decision C3): cost-based thresholding\n",
        "# Set these numbers to reflect a plausible tradeoff (you choose).\n",
        "cost_fp = 1.0    # e.g., cost of reviewing a false alert\n",
        "cost_fn = 25.0   # e.g., cost of missing a fraud event\n",
        "\n",
        "cost_rows = []\n",
        "for t in thresholds:\n",
        "    yp = (y_prob >= t).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_val, yp).ravel()\n",
        "    expected_cost = cost_fp * fp + cost_fn * fn\n",
        "    cost_rows.append({\"threshold\": t, \"fp\": fp, \"fn\": fn, \"expected_cost\": expected_cost})\n",
        "\n",
        "cost_df = pd.DataFrame(cost_rows)\n",
        "best_cost_row = cost_df.loc[cost_df[\"expected_cost\"].idxmin()]\n",
        "best_cost_row"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Final Decision Log (required)\n",
        "\n",
        "In 6\u201312 sentences total, record your two decisions and the evidence.\n",
        "\n",
        "- Decision 1:  \n",
        "- Decision 2:  \n",
        "\n",
        "Include:\n",
        "- The final threshold you used and why\n",
        "- The final model family and why\n",
        "- Whether you included `merchant_id` and why\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Extend (optional)\n",
        "\n",
        "If you finish early, do **one** of the following:\n",
        "\n",
        "- Compare two feature sets (context only vs context + customer/device)\n",
        "- Compare logistic regression vs one tree-based model\n",
        "- Add `merchant_id` in a careful way and discuss overfitting risk\n",
        "- Report and interpret the *precision at top-K* flagged transactions (operational metric)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "name": "03_02_exercise_open.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}