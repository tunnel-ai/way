{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16ae6f6",
   "metadata": {},
   "source": [
    "\n",
    "# üß™ Module 3 ‚Äî Hands-On Exercise A  \n",
    "## Heart Disease Classification (Logistic Regression ‚Ä¢ Trees ‚Ä¢ Ensembles ‚Ä¢ Metrics)\n",
    "\n",
    "### Goal\n",
    "- Compare **four classifiers** side-by-side  \n",
    "- Practice evaluating with **multiple metrics** (not just accuracy)  \n",
    "- Explore **thresholding**, **ROC curves**, and **model tuning**  \n",
    "- Gain intuition for **trade-offs** between interpretability and performance  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3fd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Dataset helper\n",
    "from datasets_module3 import make_heart_disease_synth\n",
    "\n",
    "SEED = 1955\n",
    "\n",
    "# --- Step 1: Load Dataset ---\n",
    "df = make_heart_disease_synth(n=600, seed=SEED)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3339dc4",
   "metadata": {},
   "source": [
    "\n",
    "### üîç Step 1 ‚Äî Explore the Dataset\n",
    "Use `df.head()`, `df.info()`, and `df.describe()` to understand the features and the target (`disease`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556cdf0",
   "metadata": {},
   "source": [
    "\n",
    "## üßº Step 2 ‚Äî Clean & Prepare the Data\n",
    "We will:\n",
    "- Drop missing targets  \n",
    "- Identify numeric and categorical columns  \n",
    "- Build a preprocessing pipeline (impute + scale/encode)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cc556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Step 2: Clean & Prepare ---\n",
    "\n",
    "X = df.drop('disease', axis=1)\n",
    "y = df['disease']\n",
    "\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    ('num', num_pipe, num_cols),\n",
    "    ('cat', cat_pipe, cat_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691bc5f",
   "metadata": {},
   "source": [
    "## üîÄ Step 3 ‚Äî Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff726269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "Xtr.shape, Xte.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac77246",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 4 ‚Äî Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_reg = Pipeline([\n",
    "    ('pre', pre),\n",
    "    ('model', LogisticRegression(max_iter=500, random_state=SEED))\n",
    "])\n",
    "\n",
    "log_reg.fit(Xtr, ytr)\n",
    "yhat_lr = log_reg.predict(Xte)\n",
    "yprob_lr = log_reg.predict_proba(Xte)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "print(\"Accuracy :\", accuracy_score(yte, yhat_lr))\n",
    "print(\"Precision:\", precision_score(yte, yhat_lr))\n",
    "print(\"Recall   :\", recall_score(yte, yhat_lr))\n",
    "print(\"F1 Score :\", f1_score(yte, yhat_lr))\n",
    "print(\"AUC      :\", roc_auc_score(yte, yprob_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e1e15c",
   "metadata": {},
   "source": [
    "## üå≥ Step 5 ‚Äî Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree = Pipeline([\n",
    "    ('pre', pre),\n",
    "    ('model', DecisionTreeClassifier(max_depth=4, random_state=SEED))\n",
    "])\n",
    "\n",
    "tree.fit(Xtr, ytr)\n",
    "yhat_tree = tree.predict(Xte)\n",
    "yprob_tree = tree.predict_proba(Xte)[:, 1]\n",
    "\n",
    "print(\"Decision Tree Metrics:\")\n",
    "print(\"Accuracy :\", accuracy_score(yte, yhat_tree))\n",
    "print(\"Precision:\", precision_score(yte, yhat_tree))\n",
    "print(\"Recall   :\", recall_score(yte, yhat_tree))\n",
    "print(\"F1 Score :\", f1_score(yte, yhat_tree))\n",
    "print(\"AUC      :\", roc_auc_score(yte, yprob_tree))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942a9d84",
   "metadata": {},
   "source": [
    "## üå≤ Step 6 ‚Äî Random Forest & Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest\n",
    "rf = Pipeline([\n",
    "    ('pre', pre),\n",
    "    ('model', RandomForestClassifier(n_estimators=200, random_state=SEED))\n",
    "])\n",
    "\n",
    "# Gradient Boosting\n",
    "gb = Pipeline([\n",
    "    ('pre', pre),\n",
    "    ('model', GradientBoostingClassifier(\n",
    "        learning_rate=0.05, \n",
    "        n_estimators=200, \n",
    "        max_depth=3,\n",
    "        random_state=SEED))\n",
    "])\n",
    "\n",
    "rf.fit(Xtr, ytr)\n",
    "gb.fit(Xtr, ytr)\n",
    "\n",
    "yhat_rf = rf.predict(Xte)\n",
    "yprob_rf = rf.predict_proba(Xte)[:, 1]\n",
    "\n",
    "yhat_gb = gb.predict(Xte)\n",
    "yprob_gb = gb.predict_proba(Xte)[:, 1]\n",
    "\n",
    "print(\"Random Forest AUC:\", roc_auc_score(yte, yprob_rf))\n",
    "print(\"Gradient Boosting AUC:\", roc_auc_score(yte, yprob_gb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aea3a9",
   "metadata": {},
   "source": [
    "## üìä Step 7 ‚Äî Compare All Models (Metrics Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(name, pred, prob):\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(yte, pred),\n",
    "        \"Precision\": precision_score(yte, pred),\n",
    "        \"Recall\": recall_score(yte, pred),\n",
    "        \"F1\": f1_score(yte, pred),\n",
    "        \"AUC\": roc_auc_score(yte, prob)\n",
    "    }\n",
    "\n",
    "results = pd.DataFrame([\n",
    "    evaluate(\"Logistic Regression\", yhat_lr, yprob_lr),\n",
    "    evaluate(\"Decision Tree\", yhat_tree, yprob_tree),\n",
    "    evaluate(\"Random Forest\", yhat_rf, yprob_rf),\n",
    "    evaluate(\"Gradient Boosting\", yhat_gb, yprob_gb)\n",
    "])\n",
    "\n",
    "results.sort_values(\"AUC\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0671ecd4",
   "metadata": {},
   "source": [
    "## üìà Step 8 ‚Äî ROC Curves (All Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "\n",
    "for name, prob in [\n",
    "    (\"Logistic\", yprob_lr),\n",
    "    (\"Tree\", yprob_tree),\n",
    "    (\"RF\", yprob_rf),\n",
    "    (\"GB\", yprob_gb)\n",
    "]:\n",
    "    fpr, tpr, _ = roc_curve(yte, prob)\n",
    "    plt.plot(fpr, tpr, label=name)\n",
    "\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves ‚Äî All Models\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f6334",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 9 ‚Äî Model Tuning (Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\"model__max_depth\": [2,3,4,5,6,8]}\n",
    "\n",
    "gs_tree = GridSearchCV(\n",
    "    Pipeline([('pre', pre), ('model', DecisionTreeClassifier(random_state=SEED))]),\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gs_tree.fit(Xtr, ytr)\n",
    "\n",
    "print(\"Best Tree Depth:\", gs_tree.best_params_['model__max_depth'])\n",
    "print(\"Best CV Acc    :\", gs_tree.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202dc91c",
   "metadata": {},
   "source": [
    "\n",
    "## üß† Step 10 ‚Äî Reflection Questions\n",
    "\n",
    "- Which model performed best overall? Why?  \n",
    "- Which metric (Accuracy, Precision, Recall, F1, AUC) changed your opinion the most?  \n",
    "- When might you prefer Logistic Regression over Random Forest?  \n",
    "- Would you deploy Gradient Boosting if interpretability mattered?  \n",
    "- How did tuning the Decision Tree affect performance?  \n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}