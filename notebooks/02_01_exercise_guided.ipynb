{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c122c97a",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tunnel-ai/way/blob/main/notebooks/02_01_exercise_guided.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec45c71",
   "metadata": {},
   "source": [
    "# Module 2 — Regression (Guided Exercise)\n",
    "\n",
    "**Notebook:** `02_01_exercise_guided.ipynb`  \n",
    "**Goal** here is *Adapt* (structured adaptation)\n",
    "\n",
    "You will adapt the **regression workflow** from `02_00_main.ipynb` to model:\n",
    "\n",
    "- **Target:** `transaction_loss_amount` (continuous; zero for most transactions; heavy right tail when fraud occurs)\n",
    "\n",
    "**What you are practicing**\n",
    "- Implementing a **reproducible** supervised learning pipeline\n",
    "- Avoiding **data leakage** using scikit-learn Pipelines\n",
    "- Making a small number of *specific* adaptations (baselines, target transform, regularization)\n",
    "\n",
    "**Rules**\n",
    "- Use the canonical dataset generator exactly as provided.\n",
    "- Don't edit the generator code.\n",
    "- implement the TODOs correctly (single intended modeling path).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb889b73",
   "metadata": {},
   "source": [
    "## 0) Colab setup (required)\n",
    "\n",
    "Run this cell first. It clones the course repo and makes the `core` package importable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd39a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-first setup\n",
    "!git clone https://github.com/tunnel-ai/way.git\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/content/way/src\")\n",
    "\n",
    "# Quick sanity check\n",
    "import core\n",
    "print(\"Imported core from:\", core.__file__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec768501",
   "metadata": {},
   "source": [
    "## 1) Generate the dataset\n",
    "\n",
    "We will generate the dataset using:\n",
    "\n",
    "`generate_transaction_risk_dataset(seed=1955)`\n",
    "\n",
    "This is the *same dataset* used throughout Modules 1–4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from core.generators.transaction_risk_dgp import generate_transaction_risk_dataset\n",
    "\n",
    "df = generate_transaction_risk_dataset(seed=1955)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64361480",
   "metadata": {},
   "source": [
    "## 2) Define target and leakage exclusions\n",
    "\n",
    "We are predicting **transaction_loss_amount**.\n",
    "\n",
    "**Important:** the dataset also contains other targets/labels (e.g., `is_fraud`).  \n",
    "For regression in this module, treat those as *leakage risks* and exclude them from features.\n",
    "\n",
    "You will:\n",
    "- explicitly define **X** and **y**\n",
    "- explicitly define **leakage exclusions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"transaction_loss_amount\"\n",
    "\n",
    "# Explicit leakage exclusions (labels / targets / obvious post-outcome signals)\n",
    "LEAKAGE_COLS = [\n",
    "    \"is_fraud\",                # classification target\n",
    "    \"transaction_loss_amount\", # regression target\n",
    "]\n",
    "\n",
    "# If other label-ish columns exist, add them here (keep this list explicit).\n",
    "# Example (if present): \"fraud_probability\", \"loss_bucket\", etc.\n",
    "\n",
    "# Build X, y\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "X = df.drop(columns=[c for c in LEAKAGE_COLS if c in df.columns]).copy()\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"Target summary:\")\n",
    "display(y.describe(percentiles=[0.5, 0.9, 0.95, 0.99]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f65937",
   "metadata": {},
   "source": [
    "## 3) Quick target check (zero inflation + heavy tail)\n",
    "\n",
    "This is not a full EDA—just enough to understand the target you're modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "zero_rate = (y == 0).mean()\n",
    "print(f\"Zero rate (fraction of transactions with $0 loss): {zero_rate:.3f}\")\n",
    "\n",
    "# Plot non-zero loss distribution (log scale) to visualize heavy tail\n",
    "nonzero = y[y > 0]\n",
    "print(\"Non-zero count:\", len(nonzero))\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(np.log1p(nonzero), bins=50)\n",
    "plt.title(\"log1p(transaction_loss_amount) for fraud transactions (y>0)\")\n",
    "plt.xlabel(\"log1p(loss)\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb16c37b",
   "metadata": {},
   "source": [
    "## 4) Train/validation split\n",
    "\n",
    "We use a single held-out validation split for this guided exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69992f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=1955,\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Valid:\", X_valid.shape, y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778bfd57",
   "metadata": {},
   "source": [
    "## 5) Metrics helper (MAE, RMSE, R²)\n",
    "\n",
    "Evaluate models on the validation set using:\n",
    "- **MAE** (robust-ish)\n",
    "- **RMSE** (penalizes large errors; tail-sensitive)\n",
    "- **R²** (variance explained; can be negative)\n",
    "\n",
    "**TODO:** Implement the `evaluate_regression()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_regression(y_true, y_pred):\n",
    "    \"\"\"Return MAE, RMSE, R2 in a dict.\"\"\"\n",
    "    # TODO: compute MAE, RMSE, and R^2 and return them in a dict.\n",
    "    # Hint: RMSE = sqrt(MSE)\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Quick test (should run after TODO is implemented)\n",
    "# print(evaluate_regression(np.array([0, 1, 2]), np.array([0, 2, 2])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ef704",
   "metadata": {},
   "source": [
    "## 6) Baseline 1: Predict **zero** for every transaction\n",
    "\n",
    "Because most transactions have **$0 loss**, a \"predict zero\" baseline is meaningful.\n",
    "\n",
    "**TODO:** Compute baseline metrics on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create y_pred_zero for the validation set (all zeros) and evaluate it\n",
    "# y_pred_zero = ...\n",
    "# baseline_zero_metrics = evaluate_regression(y_valid, y_pred_zero)\n",
    "# print(baseline_zero_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b1ed7",
   "metadata": {},
   "source": [
    "## 7) Preprocessing + OLS baseline model (Pipeline)\n",
    "\n",
    "We will use:\n",
    "- numeric features → impute + scale\n",
    "- categorical features → impute + one-hot encode\n",
    "- model → LinearRegression (OLS)\n",
    "\n",
    "**Design choice for guided notebook:** we will **exclude `merchant_id`** (high cardinality) to keep the workflow stable.  \n",
    "You will deal with high-cardinality encoding decisions in `02_02_exercise_open.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09676c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Split columns by dtype (simple heuristic)\n",
    "categorical_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\"]\n",
    "numeric_cols = [c for c in X_train.columns if c not in categorical_cols]\n",
    "\n",
    "# Guided constraint: drop merchant_id if present (high-cardinality)\n",
    "if \"merchant_id\" in categorical_cols:\n",
    "    categorical_cols.remove(\"merchant_id\")\n",
    "\n",
    "print(\"Numeric cols:\", len(numeric_cols))\n",
    "print(\"Categorical cols:\", len(categorical_cols))\n",
    "print(\"Example categorical:\", categorical_cols[:10])\n",
    "\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, numeric_cols),\n",
    "        (\"cat\", categorical_pipe, categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "ols_model = LinearRegression()\n",
    "\n",
    "ols_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", ols_model),\n",
    "])\n",
    "\n",
    "ols_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f9c162",
   "metadata": {},
   "source": [
    "## 8) Fit OLS pipeline and evaluate\n",
    "\n",
    "**TODO:** Fit the pipeline and compute validation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fit the pipeline\n",
    "# ols_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# TODO: predict on validation\n",
    "# y_pred_ols = ols_pipeline.predict(X_valid)\n",
    "\n",
    "# TODO: evaluate\n",
    "# ols_metrics = evaluate_regression(y_valid, y_pred_ols)\n",
    "# print(ols_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057ddc3",
   "metadata": {},
   "source": [
    "## 9) Residual diagnostic (quick)\n",
    "\n",
    "Residuals can reveal:\n",
    "- nonlinearity\n",
    "- heteroskedasticity\n",
    "- systematic under/over-prediction in certain ranges\n",
    "\n",
    "This is *not* a full statistical diagnostic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell assumes you have y_pred_ols from the previous section.\n",
    "# If you named it differently, update accordingly.\n",
    "\n",
    "# TODO: compute residuals and plot residuals vs predictions\n",
    "# residuals = y_valid - y_pred_ols\n",
    "\n",
    "# plt.figure(figsize=(7,4))\n",
    "# plt.scatter(y_pred_ols, residuals, alpha=0.3)\n",
    "# plt.axhline(0, linestyle=\"--\")\n",
    "# plt.title(\"Residuals vs Predicted (OLS)\")\n",
    "# plt.xlabel(\"Predicted loss ($)\")\n",
    "# plt.ylabel(\"Residual ($)\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a47cf3",
   "metadata": {},
   "source": [
    "## 10) Adaptation 1: Model `log1p(loss)` and evaluate in dollars\n",
    "\n",
    "Because the loss distribution is heavy-tailed, modeling `log1p(y)` often improves stability.\n",
    "\n",
    "**TODO:**  \n",
    "1) Create `y_train_log = log1p(y_train)`  \n",
    "2) Fit the same pipeline but on `y_train_log`  \n",
    "3) Predict on validation, then convert predictions back to dollars using `expm1()`  \n",
    "4) Evaluate in dollars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb46d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "# We'll reuse the *same* pipeline structure (same preprocessing).\n",
    "# Only the target changes.\n",
    "\n",
    "# TODO 1: log-transform training target\n",
    "# y_train_log = ...\n",
    "\n",
    "# TODO 2: clone the pipeline and fit on log target\n",
    "# ols_log_pipeline = clone(ols_pipeline)\n",
    "# ols_log_pipeline.fit(X_train, y_train_log)\n",
    "\n",
    "# TODO 3: predict log-loss on validation and invert transform\n",
    "# y_pred_log = ols_log_pipeline.predict(X_valid)\n",
    "# y_pred_log_dollars = np.expm1(y_pred_log)\n",
    "\n",
    "# TODO 4: evaluate in dollars\n",
    "# ols_log_metrics = evaluate_regression(y_valid, y_pred_log_dollars)\n",
    "# print(ols_log_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880fa8c",
   "metadata": {},
   "source": [
    "## 11) Adaptation 2: Ridge Regression with cross-validated alpha\n",
    "\n",
    "Ridge adds an L2 penalty that shrinks coefficients and often improves generalization,\n",
    "especially with many correlated predictors after one-hot encoding.\n",
    "\n",
    "**TODO:** Replace the OLS model with `RidgeCV` and evaluate.\n",
    "\n",
    "Constraints:\n",
    "- Keep the preprocessing identical.\n",
    "- Use a small, sensible alpha grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = np.logspace(-3, 3, 25)\n",
    "\n",
    "ridge_model = RidgeCV(alphas=alphas)\n",
    "\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", ridge_model),\n",
    "])\n",
    "\n",
    "# TODO: fit ridge_pipeline on training data and evaluate on validation\n",
    "# ridge_pipeline.fit(X_train, y_train)\n",
    "# y_pred_ridge = ridge_pipeline.predict(X_valid)\n",
    "# ridge_metrics = evaluate_regression(y_valid, y_pred_ridge)\n",
    "# print(ridge_metrics)\n",
    "\n",
    "# Optional: inspect chosen alpha\n",
    "# print(\"Chosen alpha:\", ridge_pipeline.named_steps[\"model\"].alpha_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc8ed6",
   "metadata": {},
   "source": [
    "## 12) Compare models (baseline vs OLS vs log-OLS vs Ridge)\n",
    "\n",
    "**TODO:** Create a small comparison table of the metrics you computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a DataFrame comparing:\n",
    "# - baseline_zero_metrics\n",
    "# - ols_metrics\n",
    "# - ols_log_metrics\n",
    "# - ridge_metrics\n",
    "#\n",
    "# Example:\n",
    "# results = pd.DataFrame([\n",
    "#     {\"model\": \"baseline_zero\", **baseline_zero_metrics},\n",
    "#     {\"model\": \"ols\", **ols_metrics},\n",
    "#     {\"model\": \"ols_log1p\", **ols_log_metrics},\n",
    "#     {\"model\": \"ridge\", **ridge_metrics},\n",
    "# ]).set_index(\"model\")\n",
    "# display(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7423dc13",
   "metadata": {},
   "source": [
    "## 13) Check in\n",
    "\n",
    "1) **Why is the “predict zero” baseline meaningful for this target?**  \n",
    "2) Did modeling `log1p(loss)` help? If yes, *why* might it help for a heavy-tailed target?  \n",
    "3) If Ridge helped, what problem is it addressing in this feature space (especially after one-hot encoding)?  \n",
    "4) Identify **one leakage risk** you avoided in this workflow, and how you avoided it.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
