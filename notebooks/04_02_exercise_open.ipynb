{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cab419a",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tunnel-ai/way/blob/main/notebooks/04_02_exercise_open.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfd6029",
   "metadata": {},
   "source": [
    "# Module 4 — 04_02 Open Exercise (Unsupervised Learning)\n",
    "\n",
    "\n",
    "**Decision-focused** unsupervised learning exercise.\n",
    "\n",
    "Build an unsupervised view of the canonical transaction dataset and defend your choices.\n",
    "\n",
    "---\n",
    "\n",
    "## Constraints (to keep results comparable)\n",
    "\n",
    "- Use the canonical dataset: `generate_transaction_risk_dataset(seed=1955)`\n",
    "- Do **not** use `is_fraud` or `transaction_loss_amount` as model inputs\n",
    "- Use **one** numeric feature set you define (Prepare to justify it!)\n",
    "- Use **at least two** unsupervised methods from:\n",
    "  - K-Means\n",
    "  - DBSCAN\n",
    "  - PCA (as visualization and/or preprocessing)\n",
    "  - Isolation Forest\n",
    "\n",
    "## Some outputs that might help you argue your case\n",
    "\n",
    "1) **Feature set statement** (what you included/excluded and why)  \n",
    "2) **Two-method comparison** (what each method revealed that the other did not)  \n",
    "3) **One visualization** in a reduced space (PCA 2D)  \n",
    "4) **One “investigation list”**: top 25 candidate unusual transactions (your criteria)  \n",
    "5) **Validation (optional but encouraged)**: After discovery, check whether your “unusual” list is enriched for fraud or high loss\n",
    "\n",
    "## Decision Log\n",
    "\n",
    "Defend **two** decisions:\n",
    "\n",
    "- Decision A (choose one):\n",
    "  - how you handled scaling / transformations\n",
    "  - which variables you treated as “behavior” vs “identifiers”\n",
    "- Decision B (choose one):\n",
    "  - choice of k (K-Means) OR eps/min_samples (DBSCAN) OR contamination (Isolation Forest)\n",
    "  - whether you used PCA before clustering/anomaly detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6719259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Colab) First-time setup: clone repo + add src/ to Python path\n",
    "# If you're running locally, you likely don't need this cell.\n",
    "\n",
    "# !git clone https://github.com/tunnel-ai/way.git\n",
    "# import sys\n",
    "# sys.path.insert(0, \"/content/way/src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ea87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.generators.transaction_risk_dgp import generate_transaction_risk_dataset\n",
    "\n",
    "df = generate_transaction_risk_dataset(seed=1955)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eb00fc",
   "metadata": {},
   "source": [
    "## 1) Define your feature set (your first major decision)\n",
    "\n",
    "**Task**\n",
    "- Choose a numeric feature set that you believe captures transaction behavior.\n",
    "- Explicitly exclude outcomes (`is_fraud`, `transaction_loss_amount`) and anything you believe is a pure identifier.\n",
    "\n",
    "\n",
    "> Tip: Start with a sensible baseline feature set, then adjust if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e9f7af",
   "metadata": {},
   "source": [
    "### Feature set statement \n",
    "\n",
    "This would be a good place to document some arguments if you like... \n",
    "\n",
    "- Included features:\n",
    "- Excluded features:\n",
    "- Why these choices make sense for clustering / anomaly detection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your numeric feature set here (edit freely)\n",
    "candidate_numeric = [\n",
    "    \"transaction_amount\",\n",
    "    \"transaction_hour\",\n",
    "    \"transaction_day\",\n",
    "    \"account_age_days\",\n",
    "    \"customer_risk_score\",\n",
    "    \"prior_transaction_count\",\n",
    "    \"prior_fraud_count\",\n",
    "]\n",
    "\n",
    "NUM_FEATURES = [c for c in candidate_numeric if c in df.columns]\n",
    "X_num = df[NUM_FEATURES].copy()\n",
    "\n",
    "NUM_FEATURES, X_num.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5fbc6f",
   "metadata": {},
   "source": [
    "## 2) Scaling / transformations (Decision A)\n",
    "\n",
    "Most unsupervised methods depend on geometry. You should decide whether to:\n",
    "- standardize\n",
    "- apply transforms (e.g., log for heavy-tailed features like amounts)\n",
    "- or leave raw scales (rarely recommended)\n",
    "\n",
    "**Task**\n",
    "- Implement your scaling/transformation choice.\n",
    "- Briefly justify it in the Decision Log near the bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd12b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your choice: (a) log-transform transaction_amount, then standardize everything.\n",
    "X_work = X_num.copy()\n",
    "\n",
    "if \"transaction_amount\" in X_work.columns:\n",
    "    X_work[\"transaction_amount\"] = np.log1p(X_work[\"transaction_amount\"])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_work)\n",
    "\n",
    "pd.DataFrame(X_scaled, columns=NUM_FEATURES).agg([\"mean\", \"std\"]).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9929266",
   "metadata": {},
   "source": [
    "## 3) PCA (required visualization)\n",
    "\n",
    "**Task**\n",
    "- Fit PCA on `X_scaled`\n",
    "- Plot cumulative explained variance\n",
    "- Create a 2D PCA scatterplot (unlabeled)\n",
    "\n",
    "This gives you a shared “map” to compare methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd1dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=min(10, len(NUM_FEATURES)), random_state=1955)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "explained = pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(np.cumsum(explained), marker=\"o\")\n",
    "plt.ylim(0, 1.01)\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA explained variance (cumulative)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "explained[:5], explained.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d222a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1, pc2 = X_pca[:, 0], X_pca[:, 1]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(pc1, pc2, s=10, alpha=0.5)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA projection (unlabeled)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8eb602",
   "metadata": {},
   "source": [
    "## 4) Method 1 (choose one): K-Means OR DBSCAN\n",
    "\n",
    "Pick **one** to run first, then you’ll run a second method afterward.\n",
    "\n",
    "### Option A: K-Means\n",
    "- Decide k\n",
    "- Fit K-Means\n",
    "- Report cluster counts + silhouette score\n",
    "- Visualize clusters in PCA space\n",
    "\n",
    "### Option B: DBSCAN\n",
    "- Use a k-distance plot to choose eps\n",
    "- Fit DBSCAN\n",
    "- Report noise rate + cluster counts\n",
    "- Visualize DBSCAN labels in PCA space\n",
    "\n",
    "**Task**\n",
    "- Implement Method 1 and record your tuning decision (Decision B).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 1 (default): K-Means\n",
    "k = 4  # Decision B: choose and justify\n",
    "kmeans = KMeans(n_clusters=k, n_init=10, random_state=1955)\n",
    "labels_m1 = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "counts_m1 = pd.Series(labels_m1).value_counts().sort_index()\n",
    "sil_m1 = silhouette_score(X_scaled, labels_m1)\n",
    "\n",
    "counts_m1, sil_m1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(pc1, pc2, c=labels_m1, s=10, alpha=0.5)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA space colored by Method 1 labels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d36b5a4",
   "metadata": {},
   "source": [
    "## 5) Method 2 (choose a different method than Method 1)\n",
    "\n",
    "Run a second method from the list:\n",
    "- K-Means\n",
    "- DBSCAN\n",
    "- Isolation Forest\n",
    "\n",
    "**Task**\n",
    "- Implement Method 2\n",
    "- Visualize the result in PCA space (if applicable)\n",
    "- Write 4–8 lines comparing Method 1 vs Method 2:\n",
    "  - What did one reveal that the other did not?\n",
    "  - Which is more actionable for investigation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 2 (default): Isolation Forest\n",
    "iso = IsolationForest(n_estimators=300, contamination=0.02, random_state=1955)  # Decision B if you choose this\n",
    "iso.fit(X_scaled)\n",
    "\n",
    "anomaly_score = -iso.decision_function(X_scaled)\n",
    "df_scores = df.copy()\n",
    "df_scores[\"anomaly_score\"] = anomaly_score\n",
    "\n",
    "df_scores[\"anomaly_score\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(df_scores[\"anomaly_score\"], bins=50, alpha=0.8)\n",
    "plt.xlabel(\"Anomaly score (higher = more unusual)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Isolation Forest anomaly score distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12023593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight top anomalies in PCA space\n",
    "top_n = 300\n",
    "idx_top = np.argsort(anomaly_score)[-top_n:]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(pc1, pc2, s=8, alpha=0.25)\n",
    "plt.scatter(pc1[idx_top], pc2[idx_top], s=15, alpha=0.9)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(f\"Top {top_n} anomalies highlighted in PCA space\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf457f4c",
   "metadata": {},
   "source": [
    "### Method comparison\n",
    "\n",
    "- Method 1 summary:\n",
    "- Method 2 summary:\n",
    "- What Method 1 revealed that Method 2 did not:\n",
    "- What Method 2 revealed that Method 1 did not:\n",
    "- If you had limited investigation budget/time, which output would you use first and why:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d924933",
   "metadata": {},
   "source": [
    "## 6) Investigation list\n",
    "\n",
    "Think about a top-25 list of transactions you would investigate.\n",
    "\n",
    "**Your choice:** this can be based on:\n",
    "- anomaly score (Isolation Forest)\n",
    "- DBSCAN noise points\n",
    "- “small cluster” membership in K-Means\n",
    "- or a hybrid rule you define\n",
    "\n",
    "**Try it out**\n",
    "- Produce a table with top 25 candidates including:\n",
    "  - your score / reason\n",
    "  - transaction_amount\n",
    "  - customer_risk_score\n",
    "  - transaction_hour\n",
    "  - channel (if exists)\n",
    "  - merchant_category (if exists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default approach: top anomaly scores\n",
    "cols = [\"transaction_amount\", \"customer_risk_score\", \"transaction_hour\"]\n",
    "for c in [\"channel\", \"merchant_category\"]:\n",
    "    if c in df_scores.columns:\n",
    "        cols.append(c)\n",
    "\n",
    "investigation = df_scores.sort_values(\"anomaly_score\", ascending=False).head(25)[[\"anomaly_score\"] + cols]\n",
    "investigation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abd1bad",
   "metadata": {},
   "source": [
    "## 7) Validation (optional)\n",
    "\n",
    "Now that you have a discovery result, you may use outcomes as a diagnostic lens.\n",
    "\n",
    "**Task**\n",
    "- Compute fraud rate and loss summary for your top-25 list\n",
    "- Compare to the overall dataset\n",
    "\n",
    "Interpretation prompt:\n",
    "- If enrichment is weak, what might that mean about your definition of “unusual”?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90bfb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcomes (diagnostic only)\n",
    "y_fraud = df[\"is_fraud\"].astype(int)\n",
    "y_loss = df[\"transaction_loss_amount\"]\n",
    "\n",
    "# Fraud rate: overall vs top-25 list\n",
    "fraud_overall = y_fraud.mean()\n",
    "fraud_top25 = y_fraud.loc[investigation.index].mean()\n",
    "\n",
    "# Loss: overall vs top-25 list\n",
    "loss_overall = y_loss.mean()\n",
    "loss_top25 = y_loss.loc[investigation.index].mean()\n",
    "\n",
    "fraud_overall, fraud_top25, loss_overall, loss_top25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8db26d0",
   "metadata": {},
   "source": [
    "## Think about\n",
    "\n",
    "### Decision A\n",
    "- What did you choose for scaling / transformation?\n",
    "- Why is that reasonable for geometry-based methods?\n",
    "\n",
    "### Decision B\n",
    "- What tuning choice did you make (k, eps, contamination, PCA usage)?\n",
    "- What evidence supports the choice (plot, score, stability, interpretability)?\n",
    "\n",
    "### Final takeaway\n",
    "- What “structure” do you believe exists in this dataset?\n",
    "- What would you do next if this were a real investigation?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
