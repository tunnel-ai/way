{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "99ed7120",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tunnel-ai/way/blob/main/notebooks/06_00_main_v3.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06_00_main — CNNs on EuroSAT (TFDS)\n",
        "\n",
        "**Module 6: Neural Networks (vision anchor)**\n",
        "\n",
        "In this notebook we will:\n",
        "- Load **EuroSAT RGB** from **TensorFlow Datasets (TFDS)**\n",
        "- Compare a **dense network baseline** (flattened pixels) vs a **CNN**\n",
        "- Add one “guardrail” (early stopping) to stabilize training\n",
        "- Do lightweight **error analysis** (confusions + example mistakes)\n",
        "\n",
        "> Teaching intent: the *same data* behaves very differently depending on whether the architecture respects spatial structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab note: TensorFlow is preinstalled. Avoid upgrading it unless you have a reason.\n",
        "# We do NOT use TFDS for EuroSAT here because the original TFDS download URL can return 403\n",
        "# and checksum validation can fail if a mirror zip is used.\n",
        "#\n",
        "# If you see any import errors, restart runtime: Runtime → Restart runtime\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "\n",
        "# Reproducibility (enough for interpretable curves)\n",
        "tf.keras.utils.set_random_seed(1955)\n",
        "\n",
        "# (Optional) confirm GPU\n",
        "print(\"GPU available:\", bool(tf.config.list_physical_devices(\"GPU\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Get EuroSAT RGB (direct download) and build datasets\n",
        "\n",
        "We download **EuroSAT RGB** from a stable archival host and load it using `image_dataset_from_directory`.\n",
        "This avoids TFDS download-host issues (403) and checksum mismatches when mirrors are used.\n",
        "\n",
        "We then create **train/val/test** splits and preserve the same notebook flow as before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pathlib, zipfile\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Download + unzip EuroSAT RGB\n",
        "# -----------------------------\n",
        "# We use the Zenodo archival source for EuroSAT RGB (stable).\n",
        "# This avoids TFDS download host issues and checksum mismatches.\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "ZIP_PATH = f\"{DATA_ROOT}/EuroSAT_RGB.zip\"\n",
        "EXTRACT_DIR = f\"{DATA_ROOT}/EuroSAT_RGB\"\n",
        "\n",
        "pathlib.Path(DATA_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download (quiet). If the file already exists, wget overwrites; that's fine for reproducibility.\n",
        "!wget -q -O \"{ZIP_PATH}\" \"https://zenodo.org/records/7711810/files/EuroSAT_RGB.zip\"\n",
        "\n",
        "# Extract once\n",
        "if not os.path.exists(EXTRACT_DIR):\n",
        "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "        z.extractall(EXTRACT_DIR)\n",
        "\n",
        "print(\"Folder preview (to confirm class directory):\")\n",
        "!find \"{EXTRACT_DIR}\" -maxdepth 2 -type d | head -n 30\n",
        "\n",
        "# -----------------------------\n",
        "# Locate the class directory automatically\n",
        "# -----------------------------\n",
        "# The EuroSAT RGB zip may unpack into slightly different folder layouts depending on source/version.\n",
        "# We detect the directory that contains the class subfolders (e.g., AnnualCrop, Forest, Highway, ...).\n",
        "\n",
        "def find_class_dir(root):\n",
        "    for current_root, dirs, files in os.walk(root):\n",
        "        # EuroSAT has ~10 class folders; choose a directory with many subdirectories.\n",
        "        if len(dirs) >= 8:\n",
        "            return current_root\n",
        "    raise RuntimeError(f\"Could not locate class directory automatically under: {root}\")\n",
        "\n",
        "CLASS_DIR = find_class_dir(EXTRACT_DIR)\n",
        "print(\"Detected CLASS_DIR:\", CLASS_DIR)\n",
        "print(\"Class folders:\", sorted(os.listdir(CLASS_DIR)))\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Build tf.data datasets from directory\n",
        "# -----------------------------\n",
        "BATCH_SIZE = 64\n",
        "IMG_SIZE = (64, 64)   # EuroSAT patches are 64×64\n",
        "SEED = 1955\n",
        "\n",
        "# Load the full labeled dataset (shuffled)\n",
        "full_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    CLASS_DIR,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "label_names = full_ds.class_names\n",
        "num_classes = len(label_names)\n",
        "print(\"Classes:\", num_classes)\n",
        "print(label_names)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Create train/val/test splits (by batches)\n",
        "# -----------------------------\n",
        "val_fraction = 0.10\n",
        "test_fraction = 0.10\n",
        "\n",
        "num_batches = tf.data.experimental.cardinality(full_ds).numpy()\n",
        "num_test = int(num_batches * test_fraction)\n",
        "num_val = int(num_batches * val_fraction)\n",
        "num_train = num_batches - num_val - num_test\n",
        "\n",
        "ds_train_raw = full_ds.take(num_train)\n",
        "ds_val_raw   = full_ds.skip(num_train).take(num_val)\n",
        "ds_test_raw  = full_ds.skip(num_train + num_val).take(num_test)\n",
        "\n",
        "print(f\"Batches — train: {num_train}, val: {num_val}, test: {num_test} (total: {num_batches})\")\n",
        "\n",
        "# Confirm shapes\n",
        "for xb, yb in ds_train_raw.take(1):\n",
        "    print(\"X batch:\", xb.shape, xb.dtype)\n",
        "    print(\"y batch:\", yb.shape, yb.dtype)\n",
        "    IMG_SHAPE = xb.shape[1:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Quick visual sanity-check\n",
        "\n",
        "We want to see what the labels *mean* at 64×64 resolution, and whether any classes look naturally confusable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grab a small batch for inspection\n",
        "sample_images = []\n",
        "sample_labels = []\n",
        "for img, lab in ds_train_raw.take(20):\n",
        "    sample_images.append(img.numpy())\n",
        "    sample_labels.append(int(lab.numpy()))\n",
        "\n",
        "# Plot a grid\n",
        "cols = 5\n",
        "rows = int(np.ceil(len(sample_images) / cols))\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, (img, lab) in enumerate(zip(sample_images, sample_labels), start=1):\n",
        "    plt.subplot(rows, cols, i)\n",
        "    plt.imshow(img)\n",
        "    plt.title(label_names[lab], fontsize=9)\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Data pipeline (tf.data)\n",
        "\n",
        "We’ll keep preprocessing minimal and explicit:\n",
        "- Convert to float32\n",
        "- Normalize to **[0, 1]**\n",
        "- Batch + prefetch\n",
        "\n",
        "No helper abstractions here—everything stays visible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Preprocess inline using Dataset.map with a lambda (keeps the logic visible)\n",
        "ds_train = ds_train_raw.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y), num_parallel_calls=AUTOTUNE)\n",
        "ds_val   = ds_val_raw.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y), num_parallel_calls=AUTOTUNE)\n",
        "ds_test  = ds_test_raw.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "ds_train = ds_train.shuffle(2048).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "ds_val   = ds_val.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "ds_test  = ds_test.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "# Confirm shapes\n",
        "for xb, yb in ds_train.take(1):\n",
        "    print(\"X batch:\", xb.shape, xb.dtype)\n",
        "    print(\"y batch:\", yb.shape, yb.dtype)\n",
        "    IMG_SHAPE = xb.shape[1:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Baseline model (intentionally “wrong”): Dense NN on flattened pixels\n",
        "\n",
        "Flattening treats the image as just a long vector and destroys locality.  \n",
        "We use this baseline to create contrast: *what do we lose by ignoring spatial structure?*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dense_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=IMG_SHAPE),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
        "], name=\"dense_baseline\")\n",
        "\n",
        "dense_model.summary()\n",
        "\n",
        "dense_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS_BASELINE = 8\n",
        "\n",
        "history_dense = dense_model.fit(\n",
        "    ds_train,\n",
        "    validation_data=ds_val,\n",
        "    epochs=EPOCHS_BASELINE,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot learning curves (baseline)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(history_dense.history[\"accuracy\"], label=\"train acc\")\n",
        "plt.plot(history_dense.history[\"val_accuracy\"], label=\"val acc\")\n",
        "plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.title(\"Dense baseline accuracy\")\n",
        "plt.legend(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(history_dense.history[\"loss\"], label=\"train loss\")\n",
        "plt.plot(history_dense.history[\"val_loss\"], label=\"val loss\")\n",
        "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(\"Dense baseline loss\")\n",
        "plt.legend(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) CNN model: same data, different inductive bias\n",
        "\n",
        "A CNN assumes:\n",
        "- nearby pixels matter together (locality)\n",
        "- learned filters can be reused across the image (weight sharing)\n",
        "\n",
        "This is the structural “match” for vision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=IMG_SHAPE),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
        "], name=\"cnn_small\")\n",
        "\n",
        "cnn_model.summary()\n",
        "\n",
        "cnn_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 One training guardrail: Early stopping\n",
        "\n",
        "We’ll stop training when validation loss stops improving. This is a pragmatic default for live experimentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "EPOCHS_CNN = 15\n",
        "\n",
        "history_cnn = cnn_model.fit(\n",
        "    ds_train,\n",
        "    validation_data=ds_val,\n",
        "    epochs=EPOCHS_CNN,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot learning curves (CNN)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(history_cnn.history[\"accuracy\"], label=\"train acc\")\n",
        "plt.plot(history_cnn.history[\"val_accuracy\"], label=\"val acc\")\n",
        "plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.title(\"CNN accuracy\")\n",
        "plt.legend(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(history_cnn.history[\"loss\"], label=\"train loss\")\n",
        "plt.plot(history_cnn.history[\"val_loss\"], label=\"val loss\")\n",
        "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(\"CNN loss\")\n",
        "plt.legend(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Test-set evaluation\n",
        "\n",
        "Accuracy is not the only story, but it’s a good first check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dense_test = dense_model.evaluate(ds_test, verbose=0)\n",
        "cnn_test   = cnn_model.evaluate(ds_test, verbose=0)\n",
        "\n",
        "print(\"Dense baseline — test loss, test acc:\", dense_test)\n",
        "print(\"CNN          — test loss, test acc:\", cnn_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Confusion matrix + quick error analysis\n",
        "\n",
        "We’ll inspect:\n",
        "- which classes the model confuses\n",
        "- a handful of high-confidence mistakes (useful for discussion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect predictions on the test set\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_prob = []\n",
        "\n",
        "for xb, yb in ds_test:\n",
        "    probs = cnn_model.predict(xb, verbose=0)\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    y_true.extend(yb.numpy().tolist())\n",
        "    y_pred.extend(preds.tolist())\n",
        "    y_prob.extend(np.max(probs, axis=1).tolist())\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "y_prob = np.array(y_prob)\n",
        "\n",
        "cm = tf.math.confusion_matrix(y_true, y_pred, num_classes=num_classes).numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(cm)\n",
        "plt.title(\"CNN confusion matrix (test set)\")\n",
        "plt.xlabel(\"predicted\")\n",
        "plt.ylabel(\"true\")\n",
        "plt.colorbar()\n",
        "plt.xticks(range(num_classes), label_names, rotation=90, fontsize=8)\n",
        "plt.yticks(range(num_classes), label_names, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show a few high-confidence mistakes\n",
        "# We'll scan the test set again, keep images for mistakes.\n",
        "mistake_imgs = []\n",
        "mistake_true = []\n",
        "mistake_pred = []\n",
        "mistake_conf = []\n",
        "\n",
        "for xb, yb in ds_test:\n",
        "    probs = cnn_model.predict(xb, verbose=0)\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "    confs = np.max(probs, axis=1)\n",
        "\n",
        "    for i in range(xb.shape[0]):\n",
        "        true_i = int(yb[i].numpy())\n",
        "        pred_i = int(preds[i])\n",
        "        conf_i = float(confs[i])\n",
        "        if pred_i != true_i:\n",
        "            mistake_imgs.append(xb[i].numpy())\n",
        "            mistake_true.append(true_i)\n",
        "            mistake_pred.append(pred_i)\n",
        "            mistake_conf.append(conf_i)\n",
        "\n",
        "# Sort mistakes by confidence (descending)\n",
        "idx = np.argsort(-np.array(mistake_conf))\n",
        "\n",
        "top_k = 12\n",
        "idx = idx[:min(top_k, len(idx))]\n",
        "\n",
        "cols = 4\n",
        "rows = int(np.ceil(len(idx) / cols))\n",
        "plt.figure(figsize=(12, 8))\n",
        "for j, k in enumerate(idx, start=1):\n",
        "    plt.subplot(rows, cols, j)\n",
        "    plt.imshow(mistake_imgs[k])\n",
        "    t = label_names[mistake_true[k]]\n",
        "    p = label_names[mistake_pred[k]]\n",
        "    c = mistake_conf[k]\n",
        "    plt.title(f\"true: {t}\\npred: {p} ({c:.2f})\", fontsize=8)\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Total test mistakes found: {len(mistake_imgs)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) (Optional) Transfer learning\n",
        "\n",
        "If you have time in a live session, transfer learning is a useful “industrial default” demonstration:\n",
        "- Resize inputs to match a pretrained backbone\n",
        "- Freeze backbone, train a small head\n",
        "- Compare stability and accuracy\n",
        "\n",
        "**Toggle the flag below** if you want to run this section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_TRANSFER = False  # set True if you want to run this section\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_TRANSFER:\n",
        "    # We'll use MobileNetV2 as a lightweight pretrained backbone.\n",
        "    # It expects larger images, so we'll resize on the fly in the pipeline.\n",
        "    TARGET_SIZE = (160, 160)\n",
        "\n",
        "    ds_train_tl = ds_train_raw.map(\n",
        "        lambda x, y: (tf.image.resize(tf.image.convert_image_dtype(x, tf.float32), TARGET_SIZE), y),\n",
        "        num_parallel_calls=AUTOTUNE\n",
        "    ).shuffle(2048).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "    ds_val_tl = ds_val_raw.map(\n",
        "        lambda x, y: (tf.image.resize(tf.image.convert_image_dtype(x, tf.float32), TARGET_SIZE), y),\n",
        "        num_parallel_calls=AUTOTUNE\n",
        "    ).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "    ds_test_tl = ds_test_raw.map(\n",
        "        lambda x, y: (tf.image.resize(tf.image.convert_image_dtype(x, tf.float32), TARGET_SIZE), y),\n",
        "        num_parallel_calls=AUTOTUNE\n",
        "    ).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "    backbone = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=TARGET_SIZE + (3,),\n",
        "        include_top=False,\n",
        "        weights=\"imagenet\"\n",
        "    )\n",
        "    backbone.trainable = False\n",
        "\n",
        "    tl_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=TARGET_SIZE + (3,)),\n",
        "        backbone,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
        "    ], name=\"mobilenetv2_transfer\")\n",
        "\n",
        "    tl_model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    history_tl = tl_model.fit(\n",
        "        ds_train_tl,\n",
        "        validation_data=ds_val_tl,\n",
        "        epochs=10,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    tl_test = tl_model.evaluate(ds_test_tl, verbose=0)\n",
        "    print(\"Transfer learning — test loss, test acc:\", tl_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Wrap-up (what to say out loud)\n",
        "\n",
        "- The dense baseline *can* learn something, but it wastes capacity re-learning spatial structure.\n",
        "- The CNN improves by building the right bias into the architecture.\n",
        "- Training is not “set and forget”: early stopping (and other guardrails) prevent wasted compute and overfit.\n",
        "- Confusions are not just mistakes—they are clues about what the data makes inherently hard.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "06_00_main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
