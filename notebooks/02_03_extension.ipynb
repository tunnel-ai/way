{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3495970",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tunnel-ai/way/blob/main/notebooks/02_03_extension.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadfd4b1",
   "metadata": {},
   "source": [
    "# Module 2 — Supervised Learning (Regression)\n",
    "## 02_03_extension — Optional Exploration / Challenge (Regression)\n",
    "\n",
    "**Rough goal:** Experiment beyond the baseline\n",
    "\n",
    "### To do:\n",
    "**Pick ONE exploration track** and run a small, controlled experiment on the **canonical dataset**.\n",
    "\n",
    "### Keep fixed\n",
    "- Use the canonical generator: `generate_transaction_risk_dataset(seed=1955)`\n",
    "- Use the same train/validation split (fixed random seed)\n",
    "- Report the same core metrics (MAE, RMSE, R²)\n",
    "- Do your best to keep your work **reproducible** (no manual tweaks that are hard to rerun)\n",
    "\n",
    "### Deliverable\n",
    "At the end, think about (and discuss if time allows):\n",
    "- What did you try?\n",
    "- What happened (metrics + one plot)?\n",
    "- What do you think explains the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95194a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-first setup (do not modify)\n",
    "!git clone https://github.com/tunnel-ai/way.git\n",
    "import sys\n",
    "sys.path.insert(0, \"/content/way/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb453aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bc91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical dataset (ground truth) — do not change the generator or seed\n",
    "from core.generators.transaction_risk_dgp import generate_transaction_risk_dataset\n",
    "\n",
    "df = generate_transaction_risk_dataset(seed=1955)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6c0e4",
   "metadata": {},
   "source": [
    "## 1) Target and guardrails\n",
    "\n",
    "We are predicting:\n",
    "\n",
    "- **Target:** `transaction_loss_amount` (continuous; zero for non-fraud, heavy right tail for fraud)\n",
    "\n",
    "A key modeling reality is **zero-inflation + heavy tails**:\n",
    "- Many transactions have *exactly zero* loss\n",
    "- Fraud transactions can have very large loss amounts\n",
    "\n",
    "That is why **baseline comparisons** matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d846a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"transaction_loss_amount\"\n",
    "\n",
    "# Conservative leakage exclusions (keep consistent with the main notebook. We will use this later.)\n",
    "LEAKAGE_EXCLUSIONS = [\n",
    "    \"is_fraud\",                 # directly determines whether loss can be > 0\n",
    "]\n",
    "\n",
    "# Basic checks\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"Target column '{TARGET}' not found in dataset.\")\n",
    "\n",
    "y = df[TARGET].astype(float)\n",
    "X = df.drop(columns=[TARGET] + [c for c in LEAKAGE_EXCLUSIONS if c in df.columns])\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"X columns:\", X.shape[1])\n",
    "print(\"Target summary:\")\n",
    "print(y.describe())\n",
    "print(\"\\n% zero loss:\", (y == 0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527435b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split (fixed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=1955\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \" Val:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6df3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: regression metrics in dollars\n",
    "def regression_report(y_true, y_pred, label=\"Model\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return pd.Series({\"label\": label, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
    "\n",
    "# Baseline: always predict 0 (important for zero-inflated targets)\n",
    "baseline0 = np.zeros_like(y_val)\n",
    "baseline_row = regression_report(y_val, baseline0, label=\"Baseline: predict 0\")\n",
    "baseline_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0bc6d4",
   "metadata": {},
   "source": [
    "## 2) A compact baseline pipeline (reference)\n",
    "\n",
    "This is a lightweight reference pipeline you can reuse across tracks.\n",
    "\n",
    "- Numeric: impute median + scale\n",
    "- Categorical: impute most_frequent + one-hot encode\n",
    "- Model: Linear Regression (baseline)\n",
    "\n",
    "You may swap the **model** in your chosen track, but try to keep the **preprocessing logic** stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7421c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns by dtype (simple heuristic)\n",
    "numeric_cols = [c for c in X_train.columns if pd.api.types.is_numeric_dtype(X_train[c])]\n",
    "categorical_cols = [c for c in X_train.columns if c not in numeric_cols]\n",
    "\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, numeric_cols),\n",
    "        (\"cat\", categorical_pipe, categorical_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "baseline_model = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "baseline_model.fit(X_train, y_train)\n",
    "pred_val = baseline_model.predict(X_val)\n",
    "\n",
    "baseline_lr_row = regression_report(y_val, pred_val, label=\"Baseline: LinearRegression + OHE\")\n",
    "pd.DataFrame([baseline_row, baseline_lr_row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc3e222",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3) Choose ONE exploration track\n",
    "\n",
    "Pick **one** of the tracks below\n",
    "\n",
    "### Track A — Robustness to heavy tails (Huber-like thinking)\n",
    "**Question:** Do we get better dollar-loss performance if we reduce sensitivity to extreme outliers?\n",
    "\n",
    "Approach options:\n",
    "- Use **SGDRegressor** with **Huber** loss (robust to outliers)\n",
    "- Compare to the linear baseline\n",
    "\n",
    "### Track B — Two-stage modeling (fraud → loss|fraud)\n",
    "**Question:** Does explicitly modeling “zero vs positive” help?\n",
    "\n",
    "Approach:\n",
    "1) Train a classifier to predict whether loss > 0 (proxy for fraud event)\n",
    "2) Train a regression model only on the positive-loss subset\n",
    "3) Combine predictions:\n",
    "   - If classifier predicts “no-loss”, predict 0\n",
    "   - If classifier predicts “loss”, use regression estimate\n",
    "\n",
    "### Track C — High-cardinality `merchant_id` (encoding stress test)\n",
    "**Question:** How much does `merchant_id` help, and what does it cost?\n",
    "\n",
    "Approach:\n",
    "- Compare:\n",
    "  1) Drop `merchant_id`\n",
    "  2) Bucket top-K `merchant_id` values and one-hot encode\n",
    "- Keep everything else the same\n",
    "\n",
    "> Avoid modeling the whole zoo...One track, one comparison, one conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f26d7",
   "metadata": {},
   "source": [
    "### Track A starter (robust regression)\n",
    "\n",
    "Uncomment and run this track if you choose A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219631e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRACK A (Robust regression) =====\n",
    "# Uncomment the block below to run Track A\n",
    "\n",
    "# from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# robust_model = Pipeline(steps=[\n",
    "#     (\"prep\", preprocess),\n",
    "#     (\"model\", SGDRegressor(\n",
    "#         loss=\"huber\",\n",
    "#         epsilon=1.35,          # Huber transition point\n",
    "#         alpha=1e-4,            # regularization strength\n",
    "#         max_iter=2000,\n",
    "#         random_state=1955\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# robust_model.fit(X_train, y_train)\n",
    "# pred_val_robust = robust_model.predict(X_val)\n",
    "\n",
    "# results = pd.DataFrame([\n",
    "#     baseline_row,\n",
    "#     baseline_lr_row,\n",
    "#     regression_report(y_val, pred_val_robust, label=\"Track A: SGDRegressor (Huber) + OHE\")\n",
    "# ])\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ee84c",
   "metadata": {},
   "source": [
    "### Track B starter (two-stage modeling)\n",
    "\n",
    "Uncomment and run this track if you choose B.\n",
    "\n",
    "Notes:\n",
    "- This is still “supervised learning,” but it mixes **classification + regression**.\n",
    "- It directly targets the **zero-inflation** structure of `transaction_loss_amount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fde29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRACK B (Two-stage: loss event -> loss magnitude) =====\n",
    "# Uncomment the block below to run Track B\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# # Stage 1: predict whether loss > 0\n",
    "# y_train_event = (y_train > 0).astype(int)\n",
    "# y_val_event = (y_val > 0).astype(int)\n",
    "\n",
    "# clf = Pipeline(steps=[\n",
    "#     (\"prep\", preprocess),\n",
    "#     (\"model\", LogisticRegression(max_iter=1000))\n",
    "# ])\n",
    "\n",
    "# clf.fit(X_train, y_train_event)\n",
    "# proba_val = clf.predict_proba(X_val)[:, 1]\n",
    "# auc = roc_auc_score(y_val_event, proba_val)\n",
    "\n",
    "# # Choose a threshold (simple default). You can tune this, but keep it reasonable and documented.\n",
    "# threshold = 0.5\n",
    "# pred_event = (proba_val >= threshold).astype(int)\n",
    "\n",
    "# # Stage 2: regress loss magnitude on positive-loss training examples only\n",
    "# pos_mask_train = y_train > 0\n",
    "# X_train_pos = X_train.loc[pos_mask_train]\n",
    "# y_train_pos = y_train.loc[pos_mask_train]\n",
    "\n",
    "# reg_pos = Pipeline(steps=[\n",
    "#     (\"prep\", preprocess),\n",
    "#     (\"model\", LinearRegression())\n",
    "# ])\n",
    "# reg_pos.fit(X_train_pos, y_train_pos)\n",
    "\n",
    "# # Combine predictions\n",
    "# pred_loss_val = np.zeros_like(y_val, dtype=float)\n",
    "# pred_loss_val[pred_event == 1] = reg_pos.predict(X_val.loc[pred_event == 1])\n",
    "\n",
    "# results = pd.DataFrame([\n",
    "#     baseline_row,\n",
    "#     baseline_lr_row,\n",
    "#     regression_report(y_val, pred_loss_val, label=f\"Track B: two-stage (thr={threshold:.2f})\")\n",
    "# ])\n",
    "# print(\"Stage-1 AUC (loss>0):\", auc)\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f2e4a",
   "metadata": {},
   "source": [
    "### Track C starter (merchant_id stress test)\n",
    "\n",
    "Uncomment and run this track if you choose C.\n",
    "\n",
    "This track is about **feature engineering constraints**:\n",
    "- `merchant_id` is high-cardinality (can explode one-hot)\n",
    "- Top-K bucketing is a practical compromise: keep frequent merchants, bucket the rest as “OTHER”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d361ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRACK C (merchant_id top-K bucketing) =====\n",
    "# Uncomment the block below to run Track C\n",
    "\n",
    "# def bucket_top_k(series: pd.Series, k: int = 50, other_label: str = \"__OTHER__\") -> pd.Series:\n",
    "#     top = series.value_counts().nlargest(k).index\n",
    "#     return series.where(series.isin(top), other_label)\n",
    "\n",
    "# # Variant 1: DROP merchant_id\n",
    "# X_train_drop = X_train.drop(columns=[\"merchant_id\"], errors=\"ignore\")\n",
    "# X_val_drop = X_val.drop(columns=[\"merchant_id\"], errors=\"ignore\")\n",
    "\n",
    "# num_cols_drop = [c for c in X_train_drop.columns if pd.api.types.is_numeric_dtype(X_train_drop[c])]\n",
    "# cat_cols_drop = [c for c in X_train_drop.columns if c not in num_cols_drop]\n",
    "\n",
    "# preprocess_drop = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"num\", numeric_pipe, num_cols_drop),\n",
    "#         (\"cat\", categorical_pipe, cat_cols_drop)\n",
    "#     ],\n",
    "#     remainder=\"drop\"\n",
    "# )\n",
    "\n",
    "# model_drop = Pipeline(steps=[\n",
    "#     (\"prep\", preprocess_drop),\n",
    "#     (\"model\", LinearRegression())\n",
    "# ])\n",
    "# model_drop.fit(X_train_drop, y_train)\n",
    "# pred_drop = model_drop.predict(X_val_drop)\n",
    "\n",
    "# # Variant 2: BUCKET merchant_id to top-K and one-hot\n",
    "# X_train_bucket = X_train.copy()\n",
    "# X_val_bucket = X_val.copy()\n",
    "\n",
    "# if \"merchant_id\" in X_train_bucket.columns:\n",
    "#     X_train_bucket[\"merchant_id\"] = bucket_top_k(X_train_bucket[\"merchant_id\"], k=50)\n",
    "#     # Use train top-K buckets to map val consistently:\n",
    "#     top_train = X_train_bucket[\"merchant_id\"].value_counts().nlargest(50).index\n",
    "#     X_val_bucket[\"merchant_id\"] = X_val_bucket[\"merchant_id\"].where(\n",
    "#         X_val_bucket[\"merchant_id\"].isin(top_train), \"__OTHER__\"\n",
    "#     )\n",
    "\n",
    "# num_cols_b = [c for c in X_train_bucket.columns if pd.api.types.is_numeric_dtype(X_train_bucket[c])]\n",
    "# cat_cols_b = [c for c in X_train_bucket.columns if c not in num_cols_b]\n",
    "\n",
    "# preprocess_bucket = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"num\", numeric_pipe, num_cols_b),\n",
    "#         (\"cat\", categorical_pipe, cat_cols_b)\n",
    "#     ],\n",
    "#     remainder=\"drop\"\n",
    "# )\n",
    "\n",
    "# model_bucket = Pipeline(steps=[\n",
    "#     (\"prep\", preprocess_bucket),\n",
    "#     (\"model\", LinearRegression())\n",
    "# ])\n",
    "# model_bucket.fit(X_train_bucket, y_train)\n",
    "# pred_bucket = model_bucket.predict(X_val_bucket)\n",
    "\n",
    "# results = pd.DataFrame([\n",
    "#     baseline_row,\n",
    "#     baseline_lr_row,\n",
    "#     regression_report(y_val, pred_drop, label=\"Track C: drop merchant_id\"),\n",
    "#     regression_report(y_val, pred_bucket, label=\"Track C: top-50 merchant_id bucket + OHE\"),\n",
    "# ])\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1de8cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4) Diagnostic plot \n",
    "\n",
    "After you run your chosen track, create **one diagnostic plot** that supports your explanation.\n",
    "\n",
    "Suggested options:\n",
    "- Actual vs Predicted (scatter, maybe log scale)\n",
    "- Residuals vs Predicted\n",
    "- Residuals vs transaction_amount (to show heteroskedasticity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace `y_hat` with the predictions from your chosen track.\n",
    "# Example: y_hat = pred_val_robust   (Track A)\n",
    "# Example: y_hat = pred_loss_val     (Track B)\n",
    "# Example: y_hat = pred_bucket       (Track C)\n",
    "\n",
    "y_hat = pred_val  # default: baseline linear model\n",
    "\n",
    "resid = y_val.values - y_hat\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(y_hat, resid, alpha=0.25)\n",
    "plt.axhline(0, linewidth=1)\n",
    "plt.xlabel(\"Predicted loss (dollars)\")\n",
    "plt.ylabel(\"Residual (actual - predicted)\")\n",
    "plt.title(\"Residuals vs Predicted (diagnostic)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac87765",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5) Think about (and discuss if time allows)\n",
    "\n",
    "- Track chosen\n",
    "- What you changed relative to the baseline\n",
    "- Metrics (MAE, RMSE, R²) summary\n",
    "- One diagnostic observation from your plot\n",
    "- Your best explanation (connect to zero-inflation, heavy tails, high-cardinality encoding, or model misspecification)\n",
    "- If you had 30 more minutes, what would you try next? An hour?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
