{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4869c37",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/tunnel-ai/way/blob/main/notebooks/05_00_main.ipynb\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfe4aa",
   "metadata": {},
   "source": [
    "# Module 5 — NLP: From Text to Perceived Agendas (Instructor Main)\n",
    "\n",
    "## Act I: Why national agendas do (not?) emerge automatically\n",
    "\n",
    "Our naive question/claim in this workbook: “If we cluster all scientific abstracts, do national research agendas emerge?”\n",
    "\n",
    "In Act II: \"Within a shared, globally contested topic, do regional differences in emphasis emerge?”\n",
    "\n",
    "\n",
    "\n",
    "**Notebook:** `05_00_main`  \n",
    "**Cadence:** magic helpers would be good here... but I want to try and keep the live-run narrative going, so the code looks a little less tidy.\n",
    "\n",
    "**Big idea:** We will treat scientific abstracts as analytical artifacts, convert them into representations,\n",
    "and examine how different representations can create *perceptions* of differences across coarse regions.\n",
    "\n",
    "**Important:** A few notes on this... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd3366",
   "metadata": {},
   "source": [
    "## 0) Colab-first setup\n",
    "\n",
    "This notebook is designed to run in Google Colab.\n",
    "- We fetch data from the **OpenAlex** public API (no keys).\n",
    "- We cache a local CSV so you can re-run without re-fetching everything.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9365ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import textwrap\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a55adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you care to set a random seed (I won;t use one here)\n",
    "RNG = np.random.default_rng(1955)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output paths (adjust if your repo uses a different structure)\n",
    "DATA_DIR = \"assets/data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "RAW_CACHE_PATH = os.path.join(DATA_DIR, \"openalex_abstracts_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224510d3",
   "metadata": {},
   "source": [
    "## 1) From affiliations to regions (a necessary proxy)\n",
    "\n",
    "To explore how scientific agendas *appear* across different parts of the world, we need a way to group documents.\n",
    "For this module, we use a **coarse, affiliation-based proxy** to assign each paper to a geographic region.\n",
    "\n",
    "### What we are doing\n",
    "- We query OpenAlex for works that have abstracts and institutional affiliations.\n",
    "- We extract **institution country codes** from the metadata.\n",
    "- We assign each work to a broad region (e.g., United States, Europe, China).\n",
    "- If no clear mapping can be made, we assign **Unknown**.\n",
    "\n",
    "### What this mapping is not\n",
    "- Not author nationality\n",
    "- Not a claim about funding sources or political priorities\n",
    "- Not a unified \"national agenda\"\n",
    "- Not a clean solution for multinational collaborations\n",
    "\n",
    "Affiliation is a **proxy**, not a truth.\n",
    "\n",
    "Our question is:\n",
    "> What patterns appear when we impose structure on scientific writing and then aggregate those structures by region?\n",
    "\n",
    "Keep in mind:\n",
    "- If the mapping changes, the story may change.\n",
    "- If the representation changes, the story may change.\n",
    "- Visualization confidence is not interpretive certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569fb0a",
   "metadata": {},
   "source": [
    "## 2) Fetch a sample of abstracts from OpenAlex (no keys)\n",
    "\n",
    "OpenAlex provides a free REST API. We'll pull a *manageable sample* of works with:\n",
    "- an abstract\n",
    "- institutional affiliation(s) with country code(s)\n",
    "\n",
    "We'll keep the sample modest to stay Colab-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9814973",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FetchConfig:\n",
    "    per_page: int = 200\n",
    "    max_works: int = 2000          # keep Colab-friendly; increase if needed\n",
    "    from_year: int = 2022          # adjust to widen/narrow\n",
    "    concept_id: str | None = None  # optional: focus on a concept\n",
    "    mailto: str | None = None      # optional but polite: OpenAlex suggests adding a mailto\n",
    "\n",
    "CFG = FetchConfig(\n",
    "    per_page=200,\n",
    "    max_works=2000,\n",
    "    from_year=2022,\n",
    "    concept_id=None,   # if we wanted to test certain concepts: \"C154945302\" (Artificial intelligence) (maybe verify first)\n",
    "    mailto=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604bec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://api.openalex.org/works\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_openalex_url(cursor=\"*\"):\n",
    "    params = {\n",
    "        \"per-page\": CFG.per_page,\n",
    "        \"cursor\": cursor,\n",
    "        # basic filters: has abstract AND has institutions (via authorships)\n",
    "        \"filter\": f\"has_abstract:true,from_publication_date:{CFG.from_year}-01-01\",\n",
    "        # sorting by recency tends to pull clearer modern language\n",
    "        \"sort\": \"publication_date:desc\",\n",
    "    }\n",
    "    if CFG.concept_id:\n",
    "        params[\"filter\"] += f\",concept.id:{CFG.concept_id}\"\n",
    "    if CFG.mailto:\n",
    "        params[\"mailto\"] = CFG.mailto\n",
    "    return BASE, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index_to_text(inv):\n",
    "    \"\"\"\n",
    "    OpenAlex stores abstracts as an 'inverted index' (token -> list of positions).\n",
    "    We'll reconstruct an approximate text by placing tokens back at their positions.\n",
    "    This preserves word content, but not necessarily punctuation/casing.\n",
    "    \"\"\"\n",
    "    if inv is None or not isinstance(inv, dict) or len(inv) == 0:\n",
    "        return None\n",
    "    # Determine length (max position)\n",
    "    max_pos = 0\n",
    "    for token, positions in inv.items():\n",
    "        if positions:\n",
    "            max_pos = max(max_pos, max(positions))\n",
    "    tokens = [\"\"] * (max_pos + 1)\n",
    "    for token, positions in inv.items():\n",
    "        for p in positions:\n",
    "            if 0 <= p < len(tokens) and tokens[p] == \"\":\n",
    "                tokens[p] = token\n",
    "    # Fill blanks with nothing (some positions can be empty)\n",
    "    text = \" \".join([t for t in tokens if t])\n",
    "    return text if text.strip() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_country_codes(work: dict) -> list[str]:\n",
    "    \"\"\"\n",
    "    Pull country codes from institutions associated with the work's authorships.\n",
    "    We allow multiple institutions -> multiple country codes.\n",
    "    \"\"\"\n",
    "    codes = []\n",
    "    for auth in work.get(\"authorships\", []) or []:\n",
    "        for inst in auth.get(\"institutions\", []) or []:\n",
    "            cc = inst.get(\"country_code\")\n",
    "            if cc:\n",
    "                codes.append(cc.upper())\n",
    "    return sorted(set(codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_openalex_sample():\n",
    "    rows = []\n",
    "    cursor = \"*\"\n",
    "    fetched = 0\n",
    "\n",
    "    while fetched < CFG.max_works:\n",
    "        url, params = build_openalex_url(cursor=cursor)\n",
    "        r = requests.get(url, params=params, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        payload = r.json()\n",
    "\n",
    "        for work in payload.get(\"results\", []) or []:\n",
    "            if fetched >= CFG.max_works:\n",
    "                break\n",
    "\n",
    "            inv = work.get(\"abstract_inverted_index\")\n",
    "            abstract = inverted_index_to_text(inv)\n",
    "            if not abstract:\n",
    "                continue\n",
    "\n",
    "            country_codes = extract_country_codes(work)\n",
    "\n",
    "            loc = work.get(\"primary_location\") or {}\n",
    "            src = loc.get(\"source\") or {}\n",
    "\n",
    "            rows.append({\n",
    "                \"openalex_id\": work.get(\"id\"),\n",
    "                \"doi\": work.get(\"doi\"),\n",
    "                \"title\": work.get(\"title\"),\n",
    "                \"publication_date\": work.get(\"publication_date\"),\n",
    "                \"primary_location\": src.get(\"display_name\"),\n",
    "                \"country_codes\": \"|\".join(country_codes) if country_codes else \"\",\n",
    "                \"n_country_codes\": len(country_codes),\n",
    "                \"abstract\": abstract,\n",
    "                \"type\": work.get(\"type\"),\n",
    "                \"cited_by_count\": work.get(\"cited_by_count\"),\n",
    "            })\n",
    "            fetched += 1\n",
    "\n",
    "        cursor = payload.get(\"meta\", {}).get(\"next_cursor\")\n",
    "        if not cursor:\n",
    "            break\n",
    "\n",
    "        # be a good API citizen\n",
    "        time.sleep(0.15)\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94664fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(RAW_CACHE_PATH):\n",
    "    df = pd.read_csv(RAW_CACHE_PATH)\n",
    "    print(f\"Loaded cached sample: {len(df):,} rows from {RAW_CACHE_PATH}\")\n",
    "else:\n",
    "    df = fetch_openalex_sample()\n",
    "    print(f\"Fetched sample: {len(df):,} rows\")\n",
    "    df.to_csv(RAW_CACHE_PATH, index=False)\n",
    "    print(f\"Saved cache to {RAW_CACHE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3746c9b7",
   "metadata": {},
   "source": [
    "## 3) Quick data sanity checks\n",
    "\n",
    "Before modeling: inspect what we actually have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows:\", len(df))\n",
    "print(\"Missing abstracts:\", df[\"abstract\"].isna().sum())\n",
    "print(\"Avg abstract length (chars):\", int(df[\"abstract\"].str.len().mean()))\n",
    "print(\"Median # country codes:\", int(df[\"n_country_codes\"].median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e63c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a few titles + first ~250 chars of abstracts\n",
    "for i in range(3):\n",
    "    print(\"\\n—\" * 40)\n",
    "    print(df.loc[i, \"title\"])\n",
    "    print(textwrap.shorten(df.loc[i, \"abstract\"], width=250, placeholder=\"…\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a034374",
   "metadata": {},
   "source": [
    "## 4) Region mapping (coarse, imperfect, still useful)\n",
    "\n",
    "We'll map *country codes* to broad regions:\n",
    "- **US** (United States)\n",
    "- **China**\n",
    "- **Europe** (broadly: EU + UK + EFTA + nearby; we keep it simple. I know I know Brexit... but come on.)\n",
    "- **Other**\n",
    "- **Unknown**\n",
    "\n",
    "**Note:** a paper can have multiple country codes. We'll assign:\n",
    "- If **US** appears anywhere -> label \"United States\"\n",
    "- Else if **CN** appears -> \"China\"\n",
    "- Else if any European code appears -> \"Europe\"\n",
    "- Else if any codes exist -> \"Other\"\n",
    "- Else -> \"Unknown\"\n",
    "\n",
    "This priority rule is arbitrary on purpose: it gives us a stable grouping for visualization, not truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "EUROPE_CODES = {\n",
    "    # EU members + UK + EFTA + common European countries\n",
    "    \"AT\",\"BE\",\"BG\",\"HR\",\"CY\",\"CZ\",\"DK\",\"EE\",\"FI\",\"FR\",\"DE\",\"GR\",\"HU\",\"IE\",\"IT\",\"LV\",\"LT\",\"LU\",\n",
    "    \"MT\",\"NL\",\"PL\",\"PT\",\"RO\",\"SK\",\"SI\",\"ES\",\"SE\",\n",
    "    \"GB\",\"UK\",  # UK sometimes appears as GB; UK included defensively. Brits are indecisive. \n",
    "    \"NO\",\"CH\",\"IS\",\"LI\",\n",
    "    \"UA\",\"TR\",\"RS\",\"BA\",\"ME\",\"MK\",\"AL\",\"MD\",\"BY\",\"GE\",\"AM\",\"AZ\",\n",
    "}\n",
    "\n",
    "def map_region(country_codes_str: str) -> str:\n",
    "    if not isinstance(country_codes_str, str) or country_codes_str.strip() == \"\":\n",
    "        return \"Unknown\"\n",
    "    codes = {c.strip().upper() for c in country_codes_str.split(\"|\") if c.strip()}\n",
    "    if \"US\" in codes:\n",
    "        return \"United States\"\n",
    "    if \"CN\" in codes:\n",
    "        return \"China\"\n",
    "    if len(codes & EUROPE_CODES) > 0:\n",
    "        return \"Europe\"\n",
    "    return \"Other\"\n",
    "\n",
    "df[\"region\"] = df[\"country_codes\"].apply(map_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0944e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"region\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c9b26",
   "metadata": {},
   "source": [
    "## 5) Minimal text cleaning (decisions, not perfection)\n",
    "\n",
    "We'll keep cleaning light and transparent:\n",
    "- normalize whitespace\n",
    "- lowercase (for TF–IDF)\n",
    "- remove obvious URLs\n",
    "- remove non-letter characters *selectively* (keep hyphens and spaces)\n",
    "\n",
    "Key principle:\n",
    "> Cleaning is an argument about what information you consider irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea4eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pat = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "multi_space_pat = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    s = url_pat.sub(\" \", s)\n",
    "    s = s.lower()\n",
    "    # keep letters, spaces, and hyphens; convert everything else to space\n",
    "    s = re.sub(r\"[^a-z\\s\\-]\", \" \", s)\n",
    "    s = multi_space_pat.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"text\"] = df[\"abstract\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88533eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a before/after example\n",
    "idx = 0\n",
    "print(\"TITLE:\", df.loc[idx, \"title\"])\n",
    "print(\"\\nRAW:\\n\", textwrap.shorten(df.loc[idx, \"abstract\"], width=400, placeholder=\"…\"))\n",
    "print(\"\\nCLEAN:\\n\", textwrap.shorten(df.loc[idx, \"text\"], width=400, placeholder=\"…\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b949070",
   "metadata": {},
   "source": [
    "## 6) Representation 1: TF–IDF (sparse geometry)\n",
    "\n",
    "TF–IDF is a classic baseline that is still conceptually powerful:\n",
    "- You get a high-dimensional sparse vector for each document.\n",
    "- Distance/similarity becomes a geometric question.\n",
    "\n",
    "We will keep choices explicit:\n",
    "- `min_df` / `max_df` control vocabulary inclusion.\n",
    "- `ngram_range` decides whether phrases matter.\n",
    "- stop words are a value judgment: we start with English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1657e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    ngram_range=(1, 2),\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"text\"])\n",
    "print(\"TF–IDF matrix shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few top-weighted terms for one document\n",
    "doc_i = 0\n",
    "row = X[doc_i]\n",
    "if row.nnz > 0:\n",
    "    topk = 12\n",
    "    inds = row.indices[np.argsort(row.data)[-topk:][::-1]]\n",
    "    terms = [vectorizer.get_feature_names_out()[j] for j in inds]\n",
    "    weights = np.sort(row.data)[-topk:][::-1]\n",
    "    print(df.loc[doc_i, \"title\"])\n",
    "    for t, w in zip(terms, weights):\n",
    "        print(f\"{t:<28} {w:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f267c8",
   "metadata": {},
   "source": [
    "## 7) A first \"agenda lens\": similarity search\n",
    "\n",
    "We'll pick one abstract and retrieve its nearest neighbors (cosine similarity in TF–IDF space).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = cosine_similarity(X[0], X).ravel()\n",
    "nn = np.argsort(sim)[::-1][:10]\n",
    "\n",
    "print(\"Query document:\")\n",
    "print(\" -\", df.loc[0, \"title\"])\n",
    "print(\" - region:\", df.loc[0, \"region\"])\n",
    "print()\n",
    "\n",
    "print(\"Nearest neighbors:\")\n",
    "for j in nn[1:]:\n",
    "    print(f\"sim={sim[j]:.3f} | {df.loc[j,'region']:<13} | {textwrap.shorten(str(df.loc[j,'title']), width=80, placeholder='…')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4273a4",
   "metadata": {},
   "source": [
    "## 8) Visualizing structure: reduce dimensionality (TruncatedSVD)\n",
    "\n",
    "TF–IDF lives in a huge space. To visualize structure, we project into 2D.\n",
    "\n",
    "We'll use **TruncatedSVD** (works directly on sparse matrices).\n",
    "This is not a \"true map\" — it's a view. Views can mislead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303fecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=2, random_state=7)\n",
    "Z = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for region, sub in df.assign(x=Z[:,0], y=Z[:,1]).groupby(\"region\"):\n",
    "    plt.scatter(sub[\"x\"], sub[\"y\"], s=12, alpha=0.6, label=region)\n",
    "plt.title(\"TF–IDF → 2D projection (TruncatedSVD)\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.legend(markerscale=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c658cc",
   "metadata": {},
   "source": [
    "## 9) How stable is the \"agenda perception\"?\n",
    "\n",
    "We'll do two quick stress tests:\n",
    "\n",
    "1) Change the representation slightly (unigrams only vs unigrams+bigrams)\n",
    "2) Change vocabulary thresholds (`min_df`)\n",
    "\n",
    "When you do that, your not optimizing, the \"story\" that may emerge depends on the choices you make. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f588492",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_uni = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    ngram_range=(1, 1),\n",
    ")\n",
    "X_uni = vectorizer_uni.fit_transform(df[\"text\"])\n",
    "\n",
    "Z_uni = TruncatedSVD(n_components=2, random_state=7).fit_transform(X_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62db4e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for region, sub in df.assign(x=Z_uni[:,0], y=Z_uni[:,1]).groupby(\"region\"):\n",
    "    plt.scatter(sub[\"x\"], sub[\"y\"], s=12, alpha=0.6, label=region)\n",
    "plt.title(\"TF–IDF (unigrams only) → 2D projection\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.legend(markerscale=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b2aa0",
   "metadata": {},
   "source": [
    "### Reflection (live discussion)\n",
    "\n",
    "- Do the apparent separations persist?\n",
    "- Do the clouds rotate / smear / overlap?\n",
    "- If your *interpretation* changes when you tweak `ngram_range`, what does that imply?\n",
    "\n",
    "A useful conclusion is not \"regions differ.\"\n",
    "A useful conclusion is:\n",
    "> The perception of difference is sensitive to representational choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3de0990",
   "metadata": {},
   "source": [
    "## 10) Optional: a lightweight \"topic lens\" (top terms by cluster)\n",
    "\n",
    "We'll do a simple KMeans clustering on the TF–IDF vectors, then interpret clusters by top terms.\n",
    "This is *not* state-of-the-art topic modeling; it's a transparent, geometry-driven baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32195853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 8\n",
    "kmeans = KMeans(n_clusters=k, random_state=7, n_init=\"auto\")\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "df[\"cluster\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb873bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cluster\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b93859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top terms per cluster (centroid weights)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "topn = 12\n",
    "for c in range(k):\n",
    "    top_idx = np.argsort(centroids[c])[-topn:][::-1]\n",
    "    top_terms = [terms[i] for i in top_idx]\n",
    "    print(f\"\\nCluster {c} — top terms:\")\n",
    "    print(\", \".join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e1a0e",
   "metadata": {},
   "source": [
    "### Region composition by cluster (a first aggregation)\n",
    "\n",
    "This is where \"perceived agendas\" can appear.\n",
    "Notice how quickly the table invites narrative — and how dependent it is on upstream decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab(df[\"cluster\"], df[\"region\"], normalize=\"index\")\n",
    "ct.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "ct.plot(kind=\"bar\", stacked=True, figsize=(10, 5))\n",
    "plt.title(\"Cluster composition by region (row-normalized)\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Share within cluster\")\n",
    "plt.legend(title=\"Region\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37941999",
   "metadata": {},
   "source": [
    "\n",
    "Region labels mostly reflect coverage and metadata, not content.\n",
    "\n",
    "Aggregation alone does not create meaning\n",
    "\n",
    "## Interlude: Why This Result Is So Very Unsatisfying (and Still Useful)\n",
    "\n",
    "- The absence of clear national differentiation is not a failure of NLP.\n",
    "- It reflects a deeper issue: we are asking a question that is too broad.\n",
    "- Scientific agendas are not global properties of “science”... too big it failed. \n",
    "\n",
    "\n",
    "### With no evidence (Hand waving assertion)\n",
    "\n",
    "Topics with genuine international interaction will share three properties:\n",
    "- emerge within shared, contested problem spaces.   \n",
    "- heterogeneous regulatory, ethical, and institutional contexts. \n",
    "- enough volume to support substructure. \n",
    "  \n",
    "To see agenda differences, we must condition on topic!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59bed88",
   "metadata": {},
   "source": [
    "## Act II: Conditioning on a Shared Topic — AI Governance\n",
    "\n",
    "In Act I, we asked a deliberately broad question:\n",
    "*If we cluster recent scientific abstracts, do national research agendas emerge?*\n",
    "\n",
    "The answer was largely **no** — and that result is informative.\n",
    "\n",
    "Scientific agendas are not global properties of “science.”\n",
    "They emerge within **shared, contested problem spaces**.\n",
    "\n",
    "To examine whether regional differences in emphasis appear at all, we now **condition the corpus** on a single topic that is:\n",
    "- internationally active\n",
    "- socially and institutionally contested\n",
    "- plausibly shaped by regional context\n",
    "\n",
    "We will focus on **AI governance, ethics, and regulation**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7ed16",
   "metadata": {},
   "source": [
    "### Defining the Topic\n",
    "\n",
    "Rather than relying on metadata categories, we define AI governance using a filter (below) applied directly to the abstract text.\n",
    "\n",
    "This is a choice, not a neutral fact. Some attention and justification is needed when choosing what to include or not. \n",
    "\n",
    "The goal is not to perfectly capture the topic. This is too fast and loose for that. Rather we are working toward a *shared object of inquiry* where differences in emphasis might *plausibly* emerge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d820e046",
   "metadata": {},
   "source": [
    "### Why We Are Changing the Question\n",
    "\n",
    "Up to this point, we have treated **clusters** as the primary object of analysis.\n",
    "\n",
    "That framing is no longer appropriate.\n",
    "\n",
    "When studying research agendas, we are rarely interested in whether documents\n",
    "fall into clean, separable groups.\n",
    "\n",
    "Instead, agendas are better understood as **differences in emphasis within a shared topic**.\n",
    "\n",
    "For the remainder of this notebook, we will treat:\n",
    "- *clusters* as optional diagnostics\n",
    "- *term salience and semantic emphasis* as the primary signals of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c8ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOV_TERMS = [\n",
    "    \"governance\",\n",
    "    \"ethics\",\n",
    "    \"ethical\",\n",
    "    \"fairness\",\n",
    "    \"bias\",\n",
    "    \"accountability\",\n",
    "    \"transparency\",\n",
    "    \"privacy\",\n",
    "    \"regulation\",\n",
    "    \"regulatory\",\n",
    "    \"compliance\",\n",
    "    \"risk\",\n",
    "    \"responsible ai\",\n",
    "]\n",
    "\n",
    "pattern = \"|\".join(GOV_TERMS)\n",
    "\n",
    "df_gov = df[df[\"text\"].str.contains(pattern, regex=True)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original corpus size: {len(df):,}\")\n",
    "print(f\"AI governance subset: {len(df_gov):,}\")\n",
    "df_gov[\"region\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699b6da",
   "metadata": {},
   "source": [
    "Before modeling, we quickly inspect what this conditioning step produced.\n",
    "\n",
    "This helps answer two questions:\n",
    "1. Did we meaningfully narrow the corpus?\n",
    "2. Is the subset still internationally mixed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(\"\\n—\" * 40)\n",
    "    print(df_gov.loc[i, \"title\"])\n",
    "    print(df_gov.loc[i, \"region\"])\n",
    "    print(df_gov.loc[i, \"abstract\"][:400], \"…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7433a6",
   "metadata": {},
   "source": [
    "Importantly, we did not change the modeling pipeline.\n",
    "\n",
    "We reuse:\n",
    "- the same cleaning decisions\n",
    "- the same TF–IDF configuration\n",
    "- the same dimensionality reduction\n",
    "\n",
    "Any differences we observe now come from **conditioning**, not from new machinery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d733d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gov = vectorizer.fit_transform(df_gov[\"text\"])\n",
    "print(\"TF–IDF matrix (governance subset):\", X_gov.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdce261",
   "metadata": {},
   "source": [
    "We again project the TF–IDF space into two dimensions.\n",
    "\n",
    "Remember this is not some sort of projection of \"truth\". It's a lens, and here we are zooming in by restricting the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ee3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_gov = TruncatedSVD(n_components=2, random_state=1955).fit_transform(X_gov)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for region, sub in df_gov.assign(x=Z_gov[:,0], y=Z_gov[:,1]).groupby(\"region\"):\n",
    "    plt.scatter(sub[\"x\"], sub[\"y\"], s=14, alpha=0.7, label=region)\n",
    "\n",
    "plt.title(\"AI Governance Abstracts — TF–IDF Projection\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.legend(markerscale=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6f4ea",
   "metadata": {},
   "source": [
    "At this point, the question changes.\n",
    "\n",
    "We are no longer asking:\n",
    "“Do clusters correspond to countries?”\n",
    "\n",
    "Instead, we ask:\n",
    "**Within a shared topic, how does emphasis differ by region?**\n",
    "\n",
    "One simple way to examine emphasis is to compare\n",
    "*which terms are most characteristic* of documents from each region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def top_terms_for_region(region, top_n=12):\n",
    "    sub = df_gov[df_gov[\"region\"] == region]\n",
    "    if len(sub) == 0:\n",
    "        return []\n",
    "    X_sub = vectorizer.transform(sub[\"text\"])\n",
    "    mean_tfidf = X_sub.mean(axis=0).A1\n",
    "    top_idx = mean_tfidf.argsort()[-top_n:][::-1]\n",
    "    return [feature_names[i] for i in top_idx]\n",
    "\n",
    "for r in [\"United States\", \"Europe\", \"China\", \"Other\"]:\n",
    "    print(f\"\\nTop terms — {r}\")\n",
    "    print(top_terms_for_region(r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26d458",
   "metadata": {},
   "source": [
    "### What Changed  (and a little of why it Matters)\n",
    "\n",
    "When we analyzed all scientific abstracts, national agendas did not emerge. \n",
    "\n",
    "When we conditioned on a **shared, contested topic**, did differences in emphasis appear? Is this the same as national priorities? (spoiler-no)\n",
    "\n",
    "It means that:\n",
    "- conditioning choices shape what structure becomes visible\n",
    "- representation choices shape how that structure appears\n",
    "- aggregation turns emphasis into narrative\n",
    "\n",
    "This tension  between insight and over interpretation \n",
    "is central to how NLP is used in policy, strategy, and research analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
